{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ”ï¸ CÃ¡lculo para IA: Escalando a Montanha do Conhecimento\n\n## MÃ³dulo 1: IntroduÃ§Ã£o ao CÃ¡lculo - Por que se importar com isso?\n\n**Por Pedro Nunes Guth**\n\nEaÃ­, galera! Bem-vindos ao primeiro mÃ³dulo do nosso curso \"CÃ¡lculo para IA\"! ğŸš€\n\nTÃ¡, mas antes de comeÃ§ar, uma pergunta honesta: quantas vezes vocÃª jÃ¡ ouviu falar que \"nÃ£o precisa saber matemÃ¡tica pra trabalhar com IA\"? \n\nBom... tecnicamente Ã© verdade. VocÃª pode usar bibliotecas prontas, frameworks e APIs sem entender uma vÃ­rgula do que tÃ¡ rolando por baixo dos panos. Mas aÃ­ eu te pergunto: vocÃª quer ser sÃ³ um usuÃ¡rio ou quer ser um **mestre da IA**?\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cÃ¡lculo-para-ia-modulo-01_img_01.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Bora preparar nosso ambiente!\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "from IPython.display import HTML, display\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurando o matplotlib para ficar bonitinho\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"ğŸ¯ Ambiente configurado! Bora comeÃ§ar nossa jornada!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤” TÃ¡, mas o que diabos Ã© CÃ¡lculo mesmo?\n\nOlha, vou ser direto com vocÃªs: **CÃ¡lculo Ã© a matemÃ¡tica do movimento e da mudanÃ§a**.\n\nSabe quando vocÃª tÃ¡ dirigindo e quer saber:\n- Qual sua velocidade neste exato momento?\n- Quanto combustÃ­vel vocÃª vai gastar na viagem toda?\n- Qual a melhor rota para chegar mais rÃ¡pido?\n\nO CÃ¡lculo responde essas perguntas! E adivinha? IA faz exatamente a mesma coisa:\n\n- **Velocidade** â†’ QuÃ£o rÃ¡pido o modelo estÃ¡ aprendendo?\n- **CombustÃ­vel** â†’ Quanto \"erro\" total temos?\n- **Melhor rota** â†’ Qual direÃ§Ã£o seguir para melhorar?\n\n### Os Dois Pilares do CÃ¡lculo:\n\n1. **CÃ¡lculo Diferencial** (Derivadas): *\"QuÃ£o rÃ¡pido as coisas mudam?\"*\n2. **CÃ¡lculo Integral**: *\"Quanto acumula no total?\"*\n\n**Dica do Pedro**: Pense assim - derivada Ã© como um velocÃ­metro (mudanÃ§a instantÃ¢nea), integral Ã© como o hodÃ´metro (acumulado total)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma analogia visual simples\n",
        "# Imagine que vocÃª estÃ¡ dirigindo e medindo sua posiÃ§Ã£o ao longo do tempo\n",
        "\n",
        "# Dados da viagem\n",
        "tempo = np.linspace(0, 10, 100)  # 10 horas de viagem\n",
        "posicao = 2*tempo**2 + 3*tempo + 1  # FunÃ§Ã£o da posiÃ§Ã£o (quadrÃ¡tica)\n",
        "\n",
        "# Calculando a velocidade (derivada da posiÃ§Ã£o)\n",
        "velocidade = 4*tempo + 3  # Derivada de 2tÂ² + 3t + 1 = 4t + 3\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# GrÃ¡fico da posiÃ§Ã£o\n",
        "ax1.plot(tempo, posicao, 'b-', linewidth=3, label='PosiÃ§Ã£o (km)')\n",
        "ax1.set_xlabel('Tempo (horas)')\n",
        "ax1.set_ylabel('PosiÃ§Ã£o (km)')\n",
        "ax1.set_title('ğŸš— PosiÃ§Ã£o do Carro ao Longo do Tempo')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "# GrÃ¡fico da velocidade (derivada da posiÃ§Ã£o)\n",
        "ax2.plot(tempo, velocidade, 'r-', linewidth=3, label='Velocidade (km/h)')\n",
        "ax2.set_xlabel('Tempo (horas)')\n",
        "ax2.set_ylabel('Velocidade (km/h)')\n",
        "ax2.set_title('âš¡ Velocidade do Carro (Derivada da PosiÃ§Ã£o)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ“Š Olha que lindo! A velocidade Ã© a 'taxa de mudanÃ§a' da posiÃ§Ã£o!\")\n",
        "print(\"ğŸ”¥ Esse Ã© o conceito de DERIVADA - a base de tudo na IA!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Por que IA precisa de CÃ¡lculo? A ConexÃ£o Vital\n\nBora ser bem direto: **Todo algoritmo de Machine Learning Ã©, no fundo, um problema de otimizaÃ§Ã£o**.\n\n### O que significa isso?\n\nSignifica que queremos:\n1. **Minimizar** algo ruim (erro, perda, custo)\n2. **Maximizar** algo bom (acurÃ¡cia, lucro, eficiÃªncia)\n\nE como fazemos isso matematicamente? Com **CÃ¡lculo**!\n\n### A MatemÃ¡tica por trÃ¡s:\n\nSuponha que temos uma funÃ§Ã£o de erro $E(w)$, onde $w$ sÃ£o os pesos do modelo.\n\nPara minimizar o erro, precisamos encontrar onde:\n\n$$\\frac{dE}{dw} = 0$$\n\nIsso significa: *\"onde a taxa de mudanÃ§a do erro Ã© zero\"* - ou seja, o ponto mÃ­nimo!\n\n**Dica do Pedro**: Ã‰ como achar o fundo do vale numa montanha. No ponto mais baixo, se vocÃª der um passinho para qualquer lado, sÃ³ sobe!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos visualizar uma funÃ§Ã£o de erro simples\n",
        "# Imagine que temos um modelo com apenas 1 parÃ¢metro (w)\n",
        "\n",
        "# Definindo nossa funÃ§Ã£o de erro\n",
        "w = np.linspace(-3, 5, 1000)\n",
        "erro = (w - 2)**2 + 0.5  # FunÃ§Ã£o quadrÃ¡tica com mÃ­nimo em w=2\n",
        "\n",
        "# Calculando a derivada (taxa de mudanÃ§a)\n",
        "derivada_erro = 2 * (w - 2)  # Derivada de (w-2)Â² + 0.5\n",
        "\n",
        "# Encontrando o ponto onde a derivada Ã© zero\n",
        "w_otimo = 2  # Onde derivada = 0\n",
        "erro_minimo = 0.5\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# GrÃ¡fico da funÃ§Ã£o de erro\n",
        "ax1.plot(w, erro, 'b-', linewidth=3, label='FunÃ§Ã£o de Erro E(w)')\n",
        "ax1.plot(w_otimo, erro_minimo, 'ro', markersize=12, label='MÃ­nimo Global')\n",
        "ax1.axvline(x=w_otimo, color='red', linestyle='--', alpha=0.7)\n",
        "ax1.set_xlabel('ParÃ¢metro w')\n",
        "ax1.set_ylabel('Erro E(w)')\n",
        "ax1.set_title('ğŸ¯ FunÃ§Ã£o de Erro - Queremos Minimizar Isso!')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "ax1.text(w_otimo+0.3, erro_minimo+0.1, 'Objetivo!\\n(Menor Erro)', \n",
        "         fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='yellow', alpha=0.7))\n",
        "\n",
        "# GrÃ¡fico da derivada\n",
        "ax2.plot(w, derivada_erro, 'g-', linewidth=3, label='Derivada dE/dw')\n",
        "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "ax2.axvline(x=w_otimo, color='red', linestyle='--', alpha=0.7)\n",
        "ax2.plot(w_otimo, 0, 'ro', markersize=12, label='Derivada = 0')\n",
        "ax2.set_xlabel('ParÃ¢metro w')\n",
        "ax2.set_ylabel('dE/dw')\n",
        "ax2.set_title('ğŸ“ˆ Derivada da FunÃ§Ã£o de Erro - Nossa BÃºssola!')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "ax2.text(w_otimo+0.3, 0.2, 'Aqui a derivada\\nÃ© zero!', \n",
        "         fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='lightgreen', alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸš€ Sacou? A derivada nos mostra ONDE estÃ¡ o mÃ­nimo!\")\n",
        "print(\"ğŸ’¡ Ã‰ exatamente isso que o Gradiente Descendente faz!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”ï¸ A Analogia da Montanha do Erro - O CoraÃ§Ã£o da IA\n\nAgora vem a analogia mais importante do curso inteiro! Prepara que vai ser **LIIINDO**!\n\n### ğŸ­ A HistÃ³ria:\n\nImagina que vocÃª estÃ¡ perdido numa montanha **gigante**, completamente no escuro, sÃ³ com uma lanterna pequena que ilumina alguns metros ao redor.\n\nSeu objetivo? **Chegar no ponto mais baixo possÃ­vel** (o vale).\n\n### ğŸ—ºï¸ Traduzindo para IA:\n\n- **A montanha**: Ã‰ nossa funÃ§Ã£o de erro/custo\n- **Sua posiÃ§Ã£o**: SÃ£o os parÃ¢metros atuais do modelo\n- **A altura**: Ã‰ o valor do erro naquele ponto\n- **O vale**: Ã‰ o conjunto Ã³timo de parÃ¢metros (menor erro)\n- **A lanterna**: Ã‰ o gradiente (derivada)\n- **A descida**: Ã‰ o algoritmo de otimizaÃ§Ã£o\n\n### ğŸ§­ Como usar a \"lanterna\" (gradiente)?\n\nA lanterna te mostra:\n1. **Para onde Ã© mais Ã­ngreme** (direÃ§Ã£o do gradiente)\n2. **QuÃ£o Ã­ngreme Ã©** (magnitude do gradiente)\n\nE a estratÃ©gia? **Sempre descer na direÃ§Ã£o mais Ã­ngreme!**\n\n**Dica do Pedro**: O gradiente sempre aponta para CIMA (mÃ¡ximo). Por isso descemos na direÃ§Ã£o OPOSTA (-gradiente) para achar o mÃ­nimo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar nossa \"Montanha do Erro\" em 3D!\n",
        "# Imagine um modelo com 2 parÃ¢metros: w1 e w2\n",
        "\n",
        "# Criando a grade de parÃ¢metros\n",
        "w1 = np.linspace(-3, 3, 50)\n",
        "w2 = np.linspace(-3, 3, 50)\n",
        "W1, W2 = np.meshgrid(w1, w2)\n",
        "\n",
        "# FunÃ§Ã£o de erro (uma paraboloide - formato de tigela)\n",
        "# MÃ­nimo em (1, 0.5)\n",
        "Z = (W1 - 1)**2 + 2*(W2 - 0.5)**2 + 0.1\n",
        "\n",
        "# Criando a visualizaÃ§Ã£o 3D\n",
        "fig = plt.figure(figsize=(15, 12))\n",
        "\n",
        "# Subplot 1: Vista 3D da montanha\n",
        "ax1 = fig.add_subplot(221, projection='3d')\n",
        "surf = ax1.plot_surface(W1, W2, Z, cmap='terrain', alpha=0.8)\n",
        "ax1.set_xlabel('ParÃ¢metro w1')\n",
        "ax1.set_ylabel('ParÃ¢metro w2')\n",
        "ax1.set_zlabel('Erro E(w1,w2)')\n",
        "ax1.set_title('ğŸ”ï¸ A Montanha do Erro (3D)')\n",
        "\n",
        "# Marcando o ponto Ã³timo\n",
        "ax1.scatter([1], [0.5], [0.1], color='red', s=100, label='Vale Ã“timo')\n",
        "\n",
        "# Subplot 2: Vista de cima (curvas de nÃ­vel)\n",
        "ax2 = fig.add_subplot(222)\n",
        "contour = ax2.contour(W1, W2, Z, levels=20, cmap='terrain')\n",
        "ax2.clabel(contour, inline=True, fontsize=8)\n",
        "ax2.plot(1, 0.5, 'ro', markersize=12, label='Vale Ã“timo')\n",
        "ax2.set_xlabel('ParÃ¢metro w1')\n",
        "ax2.set_ylabel('ParÃ¢metro w2')\n",
        "ax2.set_title('ğŸ—ºï¸ Mapa TopogrÃ¡fico (Vista de Cima)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: Simulando uma descida\n",
        "ax3 = fig.add_subplot(223)\n",
        "ax3.contour(W1, W2, Z, levels=20, cmap='terrain', alpha=0.6)\n",
        "\n",
        "# Simulando passos de descida do gradiente\n",
        "# ComeÃ§ando de um ponto aleatÃ³rio\n",
        "w1_path = [-2.0]\n",
        "w2_path = [2.0]\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Fazendo alguns passos de descida\n",
        "for i in range(15):\n",
        "    # Calculando o gradiente no ponto atual\n",
        "    w1_atual, w2_atual = w1_path[-1], w2_path[-1]\n",
        "    \n",
        "    # Gradiente da funÃ§Ã£o Z = (w1-1)Â² + 2(w2-0.5)Â² + 0.1\n",
        "    grad_w1 = 2 * (w1_atual - 1)  # âˆ‚Z/âˆ‚w1\n",
        "    grad_w2 = 4 * (w2_atual - 0.5)  # âˆ‚Z/âˆ‚w2\n",
        "    \n",
        "    # Dando um passo na direÃ§Ã£o oposta ao gradiente\n",
        "    w1_novo = w1_atual - learning_rate * grad_w1\n",
        "    w2_novo = w2_atual - learning_rate * grad_w2\n",
        "    \n",
        "    w1_path.append(w1_novo)\n",
        "    w2_path.append(w2_novo)\n",
        "\n",
        "# Plotando o caminho\n",
        "ax3.plot(w1_path, w2_path, 'ro-', linewidth=3, markersize=8, label='Caminho da Descida')\n",
        "ax3.plot(w1_path[0], w2_path[0], 'go', markersize=12, label='InÃ­cio')\n",
        "ax3.plot(w1_path[-1], w2_path[-1], 'bo', markersize=12, label='Final')\n",
        "ax3.plot(1, 0.5, 'r*', markersize=20, label='Vale Verdadeiro')\n",
        "ax3.set_xlabel('ParÃ¢metro w1')\n",
        "ax3.set_ylabel('ParÃ¢metro w2')\n",
        "ax3.set_title('ğŸš¶â€â™‚ï¸ Descida do Gradiente em AÃ§Ã£o!')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 4: Erro ao longo do tempo\n",
        "ax4 = fig.add_subplot(224)\n",
        "erros = [(w1 - 1)**2 + 2*(w2 - 0.5)**2 + 0.1 for w1, w2 in zip(w1_path, w2_path)]\n",
        "ax4.plot(range(len(erros)), erros, 'b-o', linewidth=3, markersize=6)\n",
        "ax4.set_xlabel('IteraÃ§Ã£o')\n",
        "ax4.set_ylabel('Erro')\n",
        "ax4.set_title('ğŸ“‰ Erro Diminuindo ao Longo do Tempo')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"ğŸ¯ ComeÃ§amos com erro {erros[0]:.3f} e terminamos com {erros[-1]:.3f}\")\n",
        "print(f\"ğŸ“ PosiÃ§Ã£o final: w1={w1_path[-1]:.3f}, w2={w2_path[-1]:.3f}\")\n",
        "print(f\"â­ PosiÃ§Ã£o Ã³tima: w1=1.000, w2=0.500\")\n",
        "print(\"\\nğŸ”¥ Ã‰ EXATAMENTE assim que funciona o treinamento de IA!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§  Mermaid: O Fluxo do Aprendizado de MÃ¡quina\n\nBora visualizar como o cÃ¡lculo se encaixa no processo de aprendizado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um diagrama explicativo do processo\n",
        "from IPython.display import HTML\n",
        "\n",
        "mermaid_code = \"\"\"\n",
        "%%html\n",
        "<div class=\"mermaid\">\n",
        "graph TD\n",
        "    A[Dados de Entrada] --> B[Modelo com ParÃ¢metros w]\n",
        "    B --> C[PrediÃ§Ãµes Å·]\n",
        "    C --> D[FunÃ§Ã£o de Erro E(w)]\n",
        "    D --> E[Calcular Gradiente âˆ‡E]\n",
        "    E --> F[Atualizar ParÃ¢metros w = w - Î±âˆ‡E]\n",
        "    F --> G{Erro Pequeno?}\n",
        "    G -->|NÃ£o| B\n",
        "    G -->|Sim| H[Modelo Treinado! ğŸ‰]\n",
        "    \n",
        "    style D fill:#ffcccc\n",
        "    style E fill:#ccffcc\n",
        "    style F fill:#ccccff\n",
        "    style H fill:#ffffcc\n",
        "</div>\n",
        "\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
        "<script>mermaid.initialize({startOnLoad:true});</script>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<div style=\"text-align: center; font-size: 16px; margin: 20px;\">\n",
        "    <h3>ğŸ”„ O Ciclo do Aprendizado de MÃ¡quina</h3>\n",
        "    <div style=\"background-color: #f0f8ff; padding: 20px; border-radius: 10px; margin: 20px;\">\n",
        "        <p><strong>ğŸ”´ Vermelho:</strong> FunÃ§Ã£o de Erro (onde mora o CÃ¡lculo Integral)</p>\n",
        "        <p><strong>ğŸŸ¢ Verde:</strong> CÃ¡lculo do Gradiente (CÃ¡lculo Diferencial)</p>\n",
        "        <p><strong>ğŸ”µ Azul:</strong> AtualizaÃ§Ã£o dos ParÃ¢metros (Gradiente Descendente)</p>\n",
        "        <p><strong>ğŸŸ¡ Amarelo:</strong> Sucesso! Modelo Otimizado!</p>\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\"))\n",
        "\n",
        "print(\"ğŸ¯ Esse Ã© o ciclo que TODO algoritmo de ML segue!\")\n",
        "print(\"ğŸ“š E o CÃ¡lculo estÃ¡ presente em CADA etapa colorida!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š Exemplo PrÃ¡tico: RegressÃ£o Linear na Unha\n\nTÃ¡ na hora de colocar a mÃ£o na massa! Vamos implementar uma regressÃ£o linear simples **do zero**, usando apenas os conceitos de cÃ¡lculo.\n\n### O Problema:\nQueremos ajustar uma reta $y = wx + b$ aos nossos dados.\n\n### A FunÃ§Ã£o de Erro (MSE):\n$$E(w,b) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - (wx_i + b))^2$$\n\n### Os Gradientes:\n$$\\frac{\\partial E}{\\partial w} = \\frac{2}{n}\\sum_{i=1}^{n}(\\hat{y}_i - y_i) \\cdot x_i$$\n\n$$\\frac{\\partial E}{\\partial b} = \\frac{2}{n}\\sum_{i=1}^{n}(\\hat{y}_i - y_i)$$\n\n**Dica do Pedro**: Essas fÃ³rmulas vÃªm diretamente da regra da cadeia! Vamos estudar isso nos prÃ³ximos mÃ³dulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gerando dados sintÃ©ticos para nossa regressÃ£o\n",
        "np.random.seed(42)  # Para resultados reproduzÃ­veis\n",
        "\n",
        "# Dados verdadeiros: y = 2.5x + 1.3 + ruÃ­do\n",
        "n_pontos = 100\n",
        "x_dados = np.random.uniform(-2, 2, n_pontos)\n",
        "w_real, b_real = 2.5, 1.3\n",
        "ruido = np.random.normal(0, 0.5, n_pontos)\n",
        "y_dados = w_real * x_dados + b_real + ruido\n",
        "\n",
        "# Visualizando os dados\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x_dados, y_dados, alpha=0.6, s=50, c='blue', label='Dados Observados')\n",
        "plt.plot(x_dados, w_real * x_dados + b_real, 'r--', linewidth=2, label=f'Linha Verdadeira: y = {w_real}x + {b_real}')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('ğŸ¯ Nossos Dados para RegressÃ£o Linear')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"ğŸ“Š Gerados {n_pontos} pontos de dados\")\n",
        "print(f\"ğŸ“ ParÃ¢metros reais: w = {w_real}, b = {b_real}\")\n",
        "print(\"ğŸ² Agora vamos tentar 'descobrir' esses parÃ¢metros usando cÃ¡lculo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ImplementaÃ§Ã£o do Gradiente Descendente do ZERO!\n",
        "# Esse Ã© o coraÃ§Ã£o da IA, galera!\n",
        "\n",
        "class RegressaoLinearNaUnha:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.w = np.random.normal(0, 0.1)  # Peso inicializado aleatoriamente\n",
        "        self.b = np.random.normal(0, 0.1)  # Bias inicializado aleatoriamente\n",
        "        \n",
        "        # Para guardar o histÃ³rico\n",
        "        self.historico_w = []\n",
        "        self.historico_b = []\n",
        "        self.historico_erro = []\n",
        "    \n",
        "    def predizer(self, x):\n",
        "        \"\"\"Faz prediÃ§Ãµes: Å· = wx + b\"\"\"\n",
        "        return self.w * x + self.b\n",
        "    \n",
        "    def calcular_erro(self, x, y):\n",
        "        \"\"\"Calcula o MSE (Mean Squared Error)\"\"\"\n",
        "        y_pred = self.predizer(x)\n",
        "        mse = np.mean((y - y_pred)**2)\n",
        "        return mse\n",
        "    \n",
        "    def calcular_gradientes(self, x, y):\n",
        "        \"\"\"Calcula os gradientes usando CÃLCULO! ğŸ”¥\"\"\"\n",
        "        n = len(x)\n",
        "        y_pred = self.predizer(x)\n",
        "        \n",
        "        # Gradiente em relaÃ§Ã£o ao peso w\n",
        "        # âˆ‚E/âˆ‚w = (2/n) * Î£(Å· - y) * x\n",
        "        grad_w = (2/n) * np.sum((y_pred - y) * x)\n",
        "        \n",
        "        # Gradiente em relaÃ§Ã£o ao bias b\n",
        "        # âˆ‚E/âˆ‚b = (2/n) * Î£(Å· - y)\n",
        "        grad_b = (2/n) * np.sum(y_pred - y)\n",
        "        \n",
        "        return grad_w, grad_b\n",
        "    \n",
        "    def treinar_um_passo(self, x, y):\n",
        "        \"\"\"Um passo do gradiente descendente\"\"\"\n",
        "        # 1. Calcular gradientes\n",
        "        grad_w, grad_b = self.calcular_gradientes(x, y)\n",
        "        \n",
        "        # 2. Atualizar parÃ¢metros (descer na montanha!)\n",
        "        self.w -= self.learning_rate * grad_w\n",
        "        self.b -= self.learning_rate * grad_b\n",
        "        \n",
        "        # 3. Guardar histÃ³rico\n",
        "        self.historico_w.append(self.w)\n",
        "        self.historico_b.append(self.b)\n",
        "        self.historico_erro.append(self.calcular_erro(x, y))\n",
        "    \n",
        "    def treinar(self, x, y, epochs=1000, verbose=True):\n",
        "        \"\"\"Treinamento completo\"\"\"\n",
        "        if verbose:\n",
        "            print(f\"ğŸš€ Iniciando treinamento...\")\n",
        "            print(f\"ğŸ“Š ParÃ¢metros iniciais: w = {self.w:.3f}, b = {self.b:.3f}\")\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            self.treinar_um_passo(x, y)\n",
        "            \n",
        "            if verbose and (epoch + 1) % 200 == 0:\n",
        "                erro_atual = self.historico_erro[-1]\n",
        "                print(f\"Ã‰poca {epoch+1:4d}: Erro = {erro_atual:.4f}, w = {self.w:.3f}, b = {self.b:.3f}\")\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nâœ… Treinamento concluÃ­do!\")\n",
        "            print(f\"ğŸ¯ ParÃ¢metros finais: w = {self.w:.3f}, b = {self.b:.3f}\")\n",
        "            print(f\"ğŸ“ ParÃ¢metros reais:  w = {w_real:.3f}, b = {b_real:.3f}\")\n",
        "\n",
        "# Criando e treinando nosso modelo\n",
        "modelo = RegressaoLinearNaUnha(learning_rate=0.01)\n",
        "modelo.treinar(x_dados, y_dados, epochs=1000)\n",
        "\n",
        "print(\"\\nğŸ”¥ Liiiindo! Implementamos ML do zero usando sÃ³ CÃ¡lculo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos visualizar os resultados do nosso treinamento\n",
        "\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Dados e linha ajustada\n",
        "ax1.scatter(x_dados, y_dados, alpha=0.6, s=30, c='blue', label='Dados')\n",
        "ax1.plot(x_dados, w_real * x_dados + b_real, 'r--', linewidth=2, label=f'Real: y = {w_real}x + {b_real}')\n",
        "ax1.plot(x_dados, modelo.predizer(x_dados), 'g-', linewidth=2, \n",
        "         label=f'Modelo: y = {modelo.w:.2f}x + {modelo.b:.2f}')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_title('ğŸ¯ Resultado Final: Dados vs Modelo Treinado')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. EvoluÃ§Ã£o do erro\n",
        "ax2.plot(modelo.historico_erro, 'b-', linewidth=2)\n",
        "ax2.set_xlabel('Ã‰poca')\n",
        "ax2.set_ylabel('Erro (MSE)')\n",
        "ax2.set_title('ğŸ“‰ Erro Diminuindo Durante o Treinamento')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_yscale('log')  # Escala logarÃ­tmica para ver melhor\n",
        "\n",
        "# 3. EvoluÃ§Ã£o do parÃ¢metro w\n",
        "ax3.plot(modelo.historico_w, 'g-', linewidth=2, label='w aprendido')\n",
        "ax3.axhline(y=w_real, color='red', linestyle='--', linewidth=2, label=f'w real = {w_real}')\n",
        "ax3.set_xlabel('Ã‰poca')\n",
        "ax3.set_ylabel('Valor de w')\n",
        "ax3.set_title('ğŸ¯ ConvergÃªncia do ParÃ¢metro w')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. EvoluÃ§Ã£o do parÃ¢metro b\n",
        "ax4.plot(modelo.historico_b, 'orange', linewidth=2, label='b aprendido')\n",
        "ax4.axhline(y=b_real, color='red', linestyle='--', linewidth=2, label=f'b real = {b_real}')\n",
        "ax4.set_xlabel('Ã‰poca')\n",
        "ax4.set_ylabel('Valor de b')\n",
        "ax4.set_title('ğŸ¯ ConvergÃªncia do ParÃ¢metro b')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculando mÃ©tricas finais\n",
        "erro_final = modelo.historico_erro[-1]\n",
        "erro_w = abs(modelo.w - w_real)\n",
        "erro_b = abs(modelo.b - b_real)\n",
        "\n",
        "print(f\"ğŸ“Š RESULTADOS FINAIS:\")\n",
        "print(f\"   Erro MSE final: {erro_final:.4f}\")\n",
        "print(f\"   Erro em w: {erro_w:.4f}\")\n",
        "print(f\"   Erro em b: {erro_b:.4f}\")\n",
        "print(f\"\\nğŸš€ Nosso modelo aprendeu os parÃ¢metros usando apenas CÃLCULO!\")\n",
        "print(f\"ğŸ”¥ Ã‰ exatamente assim que funciona o ChatGPT, sÃ³ que com bilhÃµes de parÃ¢metros!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸŒŸ O Que Vem Por AÃ­: Preview dos PrÃ³ximos MÃ³dulos\n\nAgora que vocÃª entendeu **por que** o cÃ¡lculo Ã© importante, vamos nos aprofundar nos prÃ³ximos mÃ³dulos:\n\n### ğŸ“š Roadmap do Curso:\n\n**MÃ³dulo 2 - FunÃ§Ãµes e GrÃ¡ficos**: Vamos aprender a \"ler\" funÃ§Ãµes e entender por que visualizar Ã© crucial para IA.\n\n**MÃ³dulo 3 - Limites**: O conceito que permite definir derivadas rigorosamente.\n\n**MÃ³dulos 4-5 - Derivadas**: As ferramentas fundamentais para otimizaÃ§Ã£o.\n\n**MÃ³dulo 6 - Regra da Cadeia**: O **coraÃ§Ã£o** do backpropagation!\n\n**MÃ³dulos 7-8 - Integrais**: Para probabilidade e mÃ©tricas como AUC.\n\n**MÃ³dulos 9-12 - MultivariÃ¡vel e Gradiente Descendente**: O arsenal completo para IA moderna!\n\n### ğŸ¯ Conceitos-Chave Que JÃ¡ Vimos:\n\n1. âœ… **CÃ¡lculo = MatemÃ¡tica da mudanÃ§a**\n2. âœ… **IA = Problema de otimizaÃ§Ã£o**  \n3. âœ… **Derivada = Taxa de mudanÃ§a instantÃ¢nea**\n4. âœ… **Gradiente = DireÃ§Ã£o de maior crescimento**\n5. âœ… **Gradiente Descendente = Algoritmo de otimizaÃ§Ã£o**\n\n**Dica do Pedro**: Guarde bem esses 5 pontos. Eles sÃ£o a base de tudo que vem pela frente!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um mapa mental dos prÃ³ximos mÃ³dulos\n",
        "from IPython.display import HTML\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
        "            padding: 30px; border-radius: 15px; color: white; text-align: center;\">\n",
        "    <h2>ğŸ—ºï¸ Jornada do CÃ¡lculo para IA</h2>\n",
        "    <div style=\"display: flex; justify-content: space-around; flex-wrap: wrap; margin-top: 20px;\">\n",
        "        \n",
        "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; margin: 10px; \n",
        "                    border-radius: 10px; min-width: 200px;\">\n",
        "            <h4>ğŸ“Š Fundamentos</h4>\n",
        "            <p>MÃ³dulos 2-3<br>\n",
        "            FunÃ§Ãµes & Limites</p>\n",
        "        </div>\n",
        "        \n",
        "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; margin: 10px; \n",
        "                    border-radius: 10px; min-width: 200px;\">\n",
        "            <h4>âš¡ Derivadas</h4>\n",
        "            <p>MÃ³dulos 4-6<br>\n",
        "            O Core da IA</p>\n",
        "        </div>\n",
        "        \n",
        "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; margin: 10px; \n",
        "                    border-radius: 10px; min-width: 200px;\">\n",
        "            <h4>ğŸ“ˆ Integrais</h4>\n",
        "            <p>MÃ³dulos 7-8<br>\n",
        "            Probabilidade & AUC</p>\n",
        "        </div>\n",
        "        \n",
        "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; margin: 10px; \n",
        "                    border-radius: 10px; min-width: 200px;\">\n",
        "            <h4>ğŸš€ IA Real</h4>\n",
        "            <p>MÃ³dulos 9-12<br>\n",
        "            MultivariÃ¡vel & GD</p>\n",
        "        </div>\n",
        "    </div>\n",
        "    \n",
        "    <div style=\"margin-top: 30px; font-size: 18px;\">\n",
        "        <strong>ğŸ¯ Objetivo Final:</strong> Entender completamente como funciona o treinamento de IA!\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\"))\n",
        "\n",
        "print(\"\\nğŸ“š Cada mÃ³dulo vai construir sobre o anterior\")\n",
        "print(\"ğŸ”¥ No final, vocÃª vai entender IA de uma forma que 99% das pessoas nÃ£o entende!\")\n",
        "print(\"ğŸ’ª Preparado para a jornada? Bora para o MÃ³dulo 2!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ’ª ExercÃ­cio PrÃ¡tico 1: Implementando seu Primeiro Otimizador\n\nAgora Ã© sua vez! Vamos implementar um problema de otimizaÃ§Ã£o simples usando os conceitos que aprendemos.\n\n### ğŸ¯ Desafio:\nVocÃª tem uma funÃ§Ã£o $f(x) = x^4 - 4x^3 + 6x^2 - 4x + 5$ e quer encontrar seu mÃ­nimo usando gradiente descendente.\n\n### ğŸ“ Sua MissÃ£o:\n1. Calcule a derivada $f'(x)$ na mÃ£o\n2. Implemente o gradiente descendente\n3. Encontre o mÃ­nimo da funÃ§Ã£o\n4. Visualize o processo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÃCIO 1: Complete o cÃ³digo abaixo!\n",
        "\n",
        "def f(x):\n",
        "    \"\"\"FunÃ§Ã£o que queremos minimizar: f(x) = xâ´ - 4xÂ³ + 6xÂ² - 4x + 5\"\"\"\n",
        "    return x**4 - 4*x**3 + 6*x**2 - 4*x + 5\n",
        "\n",
        "def f_derivada(x):\n",
        "    \"\"\"Derivada de f(x). CALCULE VOCÃŠ!\n",
        "    \n",
        "    Dica: Use as regras de derivaÃ§Ã£o:\n",
        "    - Derivada de x^n = n*x^(n-1)\n",
        "    - Derivada de constante = 0\n",
        "    \"\"\"\n",
        "    # SEU CÃ“DIGO AQUI!\n",
        "    # return ???\n",
        "    pass\n",
        "\n",
        "def gradiente_descendente_1d(x_inicial, learning_rate=0.01, max_iter=1000):\n",
        "    \"\"\"Implementa gradiente descendente para 1 variÃ¡vel\n",
        "    \n",
        "    Args:\n",
        "        x_inicial: ponto inicial\n",
        "        learning_rate: taxa de aprendizado\n",
        "        max_iter: nÃºmero mÃ¡ximo de iteraÃ§Ãµes\n",
        "    \n",
        "    Returns:\n",
        "        historico_x: lista com valores de x em cada iteraÃ§Ã£o\n",
        "        historico_f: lista com valores de f(x) em cada iteraÃ§Ã£o\n",
        "    \"\"\"\n",
        "    x = x_inicial\n",
        "    historico_x = [x]\n",
        "    historico_f = [f(x)]\n",
        "    \n",
        "    for i in range(max_iter):\n",
        "        # COMPLETE AQUI:\n",
        "        # 1. Calcule o gradiente no ponto atual\n",
        "        # 2. Atualize x usando a regra: x_novo = x_atual - learning_rate * gradiente\n",
        "        # 3. Adicione aos histÃ³ricos\n",
        "        \n",
        "        # SEU CÃ“DIGO AQUI!\n",
        "        pass\n",
        "    \n",
        "    return historico_x, historico_f\n",
        "\n",
        "# Teste seu cÃ³digo (descomente quando implementar)\n",
        "# x_otimo, f_otimo = gradiente_descendente_1d(x_inicial=3.0)\n",
        "# print(f\"MÃ­nimo encontrado em x = {x_otimo[-1]:.4f}\")\n",
        "# print(f\"Valor mÃ­nimo f(x) = {f_otimo[-1]:.4f}\")\n",
        "\n",
        "print(\"ğŸ¯ Implemente as funÃ§Ãµes acima e teste!\")\n",
        "print(\"ğŸ’¡ Dica: A derivada de xâ´ - 4xÂ³ + 6xÂ² - 4x + 5 Ã©...?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§© ExercÃ­cio PrÃ¡tico 2: AnÃ¡lise da Montanha do Erro\n\n### ğŸ¯ Desafio Conceitual:\n\nResponda Ã s perguntas abaixo baseado no que vocÃª aprendeu:\n\n1. **Por que usamos o negativo do gradiente na atualizaÃ§Ã£o dos parÃ¢metros?**\n\n2. **O que acontece se a taxa de aprendizado (learning rate) for muito alta?**\n\n3. **O que acontece se a taxa de aprendizado for muito baixa?**\n\n4. **Na analogia da montanha, o que representa:**\n   - A altura?\n   - Sua posiÃ§Ã£o?\n   - A bÃºssola?\n   - O tamanho dos passos?\n\n5. **Por que Ã© importante visualizar funÃ§Ãµes em IA?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÃCIO 2: Responda escrevendo suas respostas como strings\n",
        "\n",
        "respostas = {\n",
        "    \"pergunta_1\": \"\"\"\n",
        "    Sua resposta aqui...\n",
        "    \"\"\",\n",
        "    \n",
        "    \"pergunta_2\": \"\"\"\n",
        "    Sua resposta aqui...\n",
        "    \"\"\",\n",
        "    \n",
        "    \"pergunta_3\": \"\"\"\n",
        "    Sua resposta aqui...\n",
        "    \"\"\",\n",
        "    \n",
        "    \"pergunta_4\": {\n",
        "        \"altura\": \"Sua resposta...\",\n",
        "        \"posiÃ§Ã£o\": \"Sua resposta...\",\n",
        "        \"bÃºssola\": \"Sua resposta...\",\n",
        "        \"passos\": \"Sua resposta...\"\n",
        "    },\n",
        "    \n",
        "    \"pergunta_5\": \"\"\"\n",
        "    Sua resposta aqui...\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "# FunÃ§Ã£o para verificar se vocÃª respondeu\n",
        "def verificar_respostas(respostas):\n",
        "    total = 0\n",
        "    respondidas = 0\n",
        "    \n",
        "    for key, value in respostas.items():\n",
        "        if key == \"pergunta_4\":\n",
        "            for subkey, subvalue in value.items():\n",
        "                total += 1\n",
        "                if len(subvalue.strip()) > 20:  # Resposta com pelo menos 20 chars\n",
        "                    respondidas += 1\n",
        "        else:\n",
        "            total += 1\n",
        "            if len(value.strip()) > 20:\n",
        "                respondidas += 1\n",
        "    \n",
        "    print(f\"ğŸ“Š VocÃª respondeu {respondidas}/{total} perguntas\")\n",
        "    if respondidas == total:\n",
        "        print(\"ğŸ‰ ParabÃ©ns! Todas as perguntas respondidas!\")\n",
        "    else:\n",
        "        print(\"ğŸ’ª Continue respondendo para fixar o conteÃºdo!\")\n",
        "\n",
        "verificar_respostas(respostas)\n",
        "print(\"\\nğŸ¯ Este exercÃ­cio Ã© crucial para consolidar os conceitos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Resumo do MÃ³dulo 1: O Que Aprendemos\n\n### ğŸ† Conceitos Fundamentais Dominados:\n\n1. **ğŸ§® O que Ã© CÃ¡lculo:**\n   - MatemÃ¡tica do movimento e da mudanÃ§a\n   - Duas partes: Diferencial (derivadas) e Integral\n   - Base matemÃ¡tica de toda IA moderna\n\n2. **ğŸ”ï¸ A Analogia da Montanha do Erro:**\n   - FunÃ§Ã£o de erro como paisagem montanhosa\n   - Gradiente como bÃºssola indicando direÃ§Ã£o\n   - Gradiente descendente como estratÃ©gia de descida\n\n3. **âš¡ Por que IA precisa de CÃ¡lculo:**\n   - Todo ML Ã© problema de otimizaÃ§Ã£o\n   - Precisamos minimizar funÃ§Ãµes de erro\n   - Derivadas nos mostram a direÃ§Ã£o Ã³tima\n\n4. **ğŸ› ï¸ ImplementaÃ§Ã£o PrÃ¡tica:**\n   - RegressÃ£o linear do zero\n   - Gradiente descendente manual\n   - VisualizaÃ§Ã£o do processo de aprendizado\n\n### ğŸ“Š FÃ³rmulas-Chave Aprendidas:\n\n- **FunÃ§Ã£o de Erro MSE:** $E = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n- **AtualizaÃ§Ã£o de ParÃ¢metros:** $w_{new} = w_{old} - \\alpha \\frac{\\partial E}{\\partial w}$\n- **CondiÃ§Ã£o de Ã“timo:** $\\frac{dE}{dw} = 0$\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cÃ¡lculo-para-ia-modulo-01_img_02.png)\n\n**Dica Final do Pedro**: VocÃª acabou de aprender os fundamentos que movem TODA a inteligÃªncia artificial moderna. ChatGPT, GPT-4, sistemas de recomendaÃ§Ã£o, carros autÃ´nomos... todos usam exatamente esses conceitos, sÃ³ que em escala gigantesca!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CÃ³digo de encerramento com estatÃ­sticas do mÃ³dulo\n",
        "import datetime\n",
        "\n",
        "def gerar_certificado_modulo1():\n",
        "    \"\"\"Gera um certificado simbÃ³lico de conclusÃ£o do MÃ³dulo 1\"\"\"\n",
        "    \n",
        "    data_atual = datetime.datetime.now().strftime(\"%d/%m/%Y\")\n",
        "    \n",
        "    certificado = f\"\"\"\n",
        "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "    â•‘                    ğŸ“ CERTIFICADO DE CONCLUSÃƒO               â•‘\n",
        "    â•‘                                                              â•‘\n",
        "    â•‘                      CÃLCULO PARA IA                         â•‘\n",
        "    â•‘                       MÃ“DULO 1                               â•‘\n",
        "    â•‘                                                              â•‘\n",
        "    â•‘              \"IntroduÃ§Ã£o ao CÃ¡lculo e IA\"                   â•‘\n",
        "    â•‘                                                              â•‘\n",
        "    â•‘   ğŸ”ï¸  Dominou a analogia da Montanha do Erro               â•‘\n",
        "    â•‘   âš¡  Entendeu o papel do CÃ¡lculo na IA                     â•‘\n",
        "    â•‘   ğŸ› ï¸  Implementou Gradiente Descendente do zero             â•‘\n",
        "    â•‘   ğŸ“Š  Visualizou o processo de otimizaÃ§Ã£o                   â•‘\n",
        "    â•‘                                                              â•‘\n",
        "    â•‘              Por: Pedro Nunes Guth                          â•‘\n",
        "    â•‘              Data: {data_atual}                            â•‘\n",
        "    â•‘                                                              â•‘\n",
        "    â•‘        ğŸš€ Pronto para o MÃ³dulo 2: FunÃ§Ãµes e GrÃ¡ficos! ğŸš€    â•‘\n",
        "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    \"\"\"\n",
        "    \n",
        "    return certificado\n",
        "\n",
        "# EstatÃ­sticas do mÃ³dulo\n",
        "estatisticas = {\n",
        "    \"conceitos_aprendidos\": 8,\n",
        "    \"formulas_matematicas\": 5,\n",
        "    \"graficos_criados\": 6,\n",
        "    \"linhas_de_codigo\": 200,\n",
        "    \"exercicios_propostos\": 2,\n",
        "    \"analogias_usadas\": 3\n",
        "}\n",
        "\n",
        "print(gerar_certificado_modulo1())\n",
        "\n",
        "print(\"\\nğŸ“Š ESTATÃSTICAS DO MÃ“DULO 1:\")\n",
        "for key, value in estatisticas.items():\n",
        "    nome = key.replace('_', ' ').title()\n",
        "    print(f\"   {nome}: {value}\")\n",
        "\n",
        "print(\"\\nğŸ¯ PRÃ“XIMOS PASSOS:\")\n",
        "print(\"   1. Revise os conceitos principais\")\n",
        "print(\"   2. Complete os exercÃ­cios prÃ¡ticos\")\n",
        "print(\"   3. Parta para o MÃ³dulo 2: FunÃ§Ãµes e GrÃ¡ficos\")\n",
        "\n",
        "print(\"\\nğŸ”¥ LEMBRA: VocÃª agora entende IA de um jeito que pouquÃ­ssimas pessoas entendem!\")\n",
        "print(\"ğŸ’ª Continue nessa jornada que vai valer MUITO a pena!\")\n",
        "\n",
        "# MotivaÃ§Ã£o final\n",
        "frases_motivacionais = [\n",
        "    \"ğŸš€ Cada linha de cÃ³digo te aproxima da maestria!\",\n",
        "    \"ğŸ§  Conhecimento Ã© o Ãºnico investimento que sempre dÃ¡ retorno!\",\n",
        "    \"âš¡ VocÃª estÃ¡ construindo superpoderes matemÃ¡ticos!\",\n",
        "    \"ğŸ¯ O futuro pertence a quem entende os fundamentos!\",\n",
        "    \"ğŸ† PersistÃªncia + Conhecimento = Sucesso garantido!\"\n",
        "]\n",
        "\n",
        "import random\n",
        "frase_do_dia = random.choice(frases_motivacionais)\n",
        "print(f\"\\nğŸ’ FRASE DO DIA: {frase_do_dia}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ‰ MÃ“DULO 1 CONCLUÃDO COM SUCESSO! ğŸ‰\")\n",
        "print(\"=\"*60)"
      ]
    }
  ]
}
