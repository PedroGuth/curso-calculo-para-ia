# C√°lculo para IA

## Descri√ß√£o
Curso completo e pr√°tico com 11 m√≥dulos sobre C√°lculo aplicado √† Intelig√™ncia Artificial. Desenvolvido por Pedro Nunes Guth, este curso apresenta os fundamentos do c√°lculo diferencial e integral essenciais para compreender e implementar algoritmos de machine learning, desde conceitos b√°sicos at√© otimiza√ß√£o avan√ßada.

## Objetivos de Aprendizagem

Ao final deste curso, voc√™ ser√° capaz de:
- ‚úÖ Compreender a import√¢ncia do c√°lculo para machine learning
- ‚úÖ Visualizar e interpretar fun√ß√µes matem√°ticas
- ‚úÖ Calcular derivadas e entender sua aplica√ß√£o em IA
- ‚úÖ Aplicar a regra da cadeia no backpropagation
- ‚úÖ Entender integrais e suas aplica√ß√µes em probabilidade
- ‚úÖ Trabalhar com fun√ß√µes de m√∫ltiplas vari√°veis
- ‚úÖ Calcular gradientes e derivadas parciais
- ‚úÖ Implementar algoritmos de gradiente descendente
- ‚úÖ Aplicar varia√ß√µes avan√ßadas como SGD e Adam
- ‚úÖ Conectar teoria matem√°tica com implementa√ß√µes pr√°ticas

## Estrutura do Curso

### M√≥dulo 1: Introdu√ß√£o ao C√°lculo - Por que se Importar com Isso?
**Conceitos abordados:**
- Defini√ß√£o e import√¢ncia do c√°lculo para IA
- Analogia da montanha do erro
- C√°lculo diferencial vs integral
- Conex√£o entre c√°lculo e machine learning

**Aplica√ß√µes pr√°ticas:**
- Compreens√£o do funcionamento de algoritmos de ML
- Visualiza√ß√£o de fun√ß√µes de custo
- Fundamentos para otimiza√ß√£o de modelos

---

### M√≥dulo 2: Fun√ß√µes e Gr√°ficos - Visualizando o Problema
**Conceitos abordados:**
- Defini√ß√£o de fun√ß√µes matem√°ticas
- Import√¢ncia da visualiza√ß√£o em IA
- Fun√ß√µes lineares, quadr√°ticas e exponenciais
- Fun√ß√µes de custo simples

**Aplica√ß√µes pr√°ticas:**
- An√°lise de fun√ß√µes de perda
- Visualiza√ß√£o de dados e modelos
- Interpreta√ß√£o de resultados de ML

---

### M√≥dulo 3: Limites - Chegando Perto sem Tocar
**Conceitos abordados:**
- Defini√ß√£o formal de limite
- Continuidade de fun√ß√µes
- Suavidade e diferenciabilidade
- Limites laterais e infinitos

**Aplica√ß√µes pr√°ticas:**
- Valida√ß√£o de fun√ß√µes de ativa√ß√£o
- An√°lise de converg√™ncia de algoritmos
- Fundamentos para c√°lculo de derivadas

---

### M√≥dulo 4: Derivadas 1 - A Taxa de Varia√ß√£o Imediata
**Conceitos abordados:**
- Defini√ß√£o de derivada como limite
- Interpreta√ß√£o geom√©trica (inclina√ß√£o da tangente)
- Derivada como taxa de varia√ß√£o
- C√°lculo de derivadas b√°sicas

**Aplica√ß√µes pr√°ticas:**
- An√°lise de fun√ß√µes de custo
- Otimiza√ß√£o de par√¢metros
- Fundamentos para gradiente descendente

---

### M√≥dulo 5: Derivadas 2 - As Regras do Jogo
**Conceitos abordados:**
- Regra da soma e diferen√ßa
- Regra do produto
- Regra do quociente
- Regra da pot√™ncia
- Aplica√ß√µes em fun√ß√µes de perda

**Aplica√ß√µes pr√°ticas:**
- Deriva√ß√£o de fun√ß√µes complexas
- C√°lculo de gradientes em ML
- Otimiza√ß√£o de fun√ß√µes de custo

---

### M√≥dulo 6: Regra da Cadeia - A Regra de Ouro do Backpropagation
**Conceitos abordados:**
- Fun√ß√µes compostas
- Regra da cadeia para deriva√ß√£o
- Aplica√ß√£o em redes neurais
- Backpropagation step-by-step

**Aplica√ß√µes pr√°ticas:**
- Implementa√ß√£o de backpropagation
- Treinamento de redes neurais
- C√°lculo de gradientes em deep learning

---

### M√≥dulo 7: Integrais 1 - O que a Integral Mede Afinal?
**Conceitos abordados:**
- Integral definida e indefinida
- √Årea sob a curva
- Teorema Fundamental do C√°lculo
- M√©todos de integra√ß√£o b√°sicos

**Aplica√ß√µes pr√°ticas:**
- C√°lculo de probabilidades
- An√°lise de distribui√ß√µes
- Fundamentos para m√©tricas de avalia√ß√£o

---

### M√≥dulo 8: Integrais 2 - Probabilidade e Aplica√ß√£o na IA
**Conceitos abordados:**
- Integrais em fun√ß√µes de densidade de probabilidade
- √Årea Sob a Curva (AUC)
- M√©tricas de classifica√ß√£o
- Integra√ß√£o num√©rica

**Aplica√ß√µes pr√°ticas:**
- Avalia√ß√£o de modelos de classifica√ß√£o
- C√°lculo de m√©tricas ROC-AUC
- An√°lise de performance de algoritmos

---

### M√≥dulo 9: Fun√ß√µes de M√∫ltiplas Vari√°veis - A Paisagem do Erro
**Conceitos abordados:**
- Fun√ß√µes de duas ou mais vari√°veis
- Visualiza√ß√£o em 3D
- Superf√≠cies e contornos
- Aplica√ß√£o em modelos multivariados

**Aplica√ß√µes pr√°ticas:**
- Modelagem de problemas complexos
- Visualiza√ß√£o de espa√ßos de par√¢metros
- An√°lise de fun√ß√µes de custo multidimensionais

---

### M√≥dulo 10: Derivadas Parciais e Gradientes - A B√∫ssola do Aprendizado
**Conceitos abordados:**
- Derivadas parciais
- Vetor gradiente
- Dire√ß√£o de maior inclina√ß√£o
- Gradiente como b√∫ssola de otimiza√ß√£o

**Aplica√ß√µes pr√°ticas:**
- C√°lculo de gradientes em ML
- Otimiza√ß√£o de par√¢metros
- Fundamentos para algoritmos de descida

---

### M√≥dulo 11: Gradiente Descendente 1 - A Descida na Dire√ß√£o Certa
**Conceitos abordados:**
- Algoritmo do gradiente descendente
- Taxa de aprendizado
- Converg√™ncia e otimiza√ß√£o
- Implementa√ß√£o pr√°tica

**Aplica√ß√µes pr√°ticas:**
- Treinamento de modelos de ML
- Otimiza√ß√£o de fun√ß√µes de custo
- Implementa√ß√£o de algoritmos de otimiza√ß√£o

---

### M√≥dulo 12: Gradiente Descendente 2 - Varia√ß√µes e Truques
**Conceitos abordados:**
- Stochastic Gradient Descent (SGD)
- Mini-batch gradient descent
- Algoritmo Adam
- T√©cnicas de otimiza√ß√£o avan√ßada

**Aplica√ß√µes pr√°ticas:**
- Otimiza√ß√£o eficiente de modelos grandes
- Treinamento de deep learning
- T√©cnicas de acelera√ß√£o de converg√™ncia

---

## Compet√™ncias Desenvolvidas

### üßÆ Fundamentos Matem√°ticos
- Compreens√£o profunda de c√°lculo diferencial e integral
- Dom√≠nio de derivadas e suas aplica√ß√µes
- Interpreta√ß√£o geom√©trica de conceitos matem√°ticos
- Trabalho com fun√ß√µes de m√∫ltiplas vari√°veis

### üî¨ An√°lise e Otimiza√ß√£o
- C√°lculo de gradientes e derivadas parciais
- Implementa√ß√£o de algoritmos de otimiza√ß√£o
- An√°lise de converg√™ncia e performance
- Aplica√ß√£o da regra da cadeia em backpropagation

### üíª Implementa√ß√£o Pr√°tica
- Implementa√ß√£o de gradiente descendente
- Uso de varia√ß√µes avan√ßadas (SGD, Adam)
- Visualiza√ß√£o de fun√ß√µes e otimiza√ß√£o
- Conex√£o entre teoria e pr√°tica em ML

### üéØ Aplica√ß√£o em IA
- Compreens√£o do funcionamento de redes neurais
- Implementa√ß√£o de backpropagation
- Otimiza√ß√£o de modelos de machine learning
- An√°lise de fun√ß√µes de custo e perda

## Pr√©-requisitos

- Conhecimento b√°sico de Python
- No√ß√µes elementares de matem√°tica (√°lgebra b√°sica)
- Familiaridade com conceitos b√°sicos de fun√ß√µes
- Interesse em machine learning e otimiza√ß√£o

## Metodologia

O curso combina:
- **Teoria fundamentada** com explica√ß√µes claras e analogias pr√°ticas
- **Visualiza√ß√µes interativas** para compreens√£o de conceitos abstratos
- **Implementa√ß√£o hands-on** com c√≥digo Python e bibliotecas especializadas
- **Exemplos do mundo real** conectando c√°lculo com aplica√ß√µes de IA
- **Progress√£o gradual** dos conceitos b√°sicos at√© aplica√ß√µes avan√ßadas

## Notebooks Gerados

Total de notebooks: 11

1. [modulo-01-introdu√ß√£o-ao-c√°lculo:-por-que-se-importar-com-isso?---o-que-√©-o-c√°lculo-e-como-ele-√©-a-base-do-aprendizado-de-m√°quina.-a-analogia-da-montanha-do-erro.-notebook.ipynb](modulo-01-introdu√ß√£o-ao-c√°lculo:-por-que-se-importar-com-isso?---o-que-√©-o-c√°lculo-e-como-ele-√©-a-base-do-aprendizado-de-m√°quina.-a-analogia-da-montanha-do-erro.-notebook.ipynb)
2. [modulo-02-fun√ß√µes-e-gr√°ficos:-visualizando-o-problema---notebook-focado-em-fun√ß√µes-matem√°ticas-e-a-import√¢ncia-de-visualiz√°-las.-exemplo-de-fun√ß√µes-de-custo-simples.-notebook.ipynb](modulo-02-fun√ß√µes-e-gr√°ficos:-visualizando-o-problema---notebook-focado-em-fun√ß√µes-matem√°ticas-e-a-import√¢ncia-de-visualiz√°-las.-exemplo-de-fun√ß√µes-de-custo-simples.-notebook.ipynb)
3. [modulo-03-limites:-chegando-perto-sem-tocar---o-conceito-de-limite-e-sua-import√¢ncia-para-definir-a-continuidade-e-a-suavidade-das-fun√ß√µes.-notebook.ipynb](modulo-03-limites:-chegando-perto-sem-tocar---o-conceito-de-limite-e-sua-import√¢ncia-para-definir-a-continuidade-e-a-suavidade-das-fun√ß√µes.-notebook.ipynb)
4. [modulo-04-derivadas-1:-a-taxa-de-varia√ß√£o-imediata---o-conceito-da-primeira-derivada.-como-ela-nos-d√°-a-inclina√ß√£o-da-reta-tangente-em-um-ponto-espec√≠fico.-notebook.ipynb](modulo-04-derivadas-1:-a-taxa-de-varia√ß√£o-imediata---o-conceito-da-primeira-derivada.-como-ela-nos-d√°-a-inclina√ß√£o-da-reta-tangente-em-um-ponto-espec√≠fico.-notebook.ipynb)
5. [modulo-05-derivadas-2:-as-regras-do-jogo---regras-de-deriva√ß√£o:-regra-da-soma,-do-produto,-do-quociente.-aplica√ß√µes-em-fun√ß√µes-que-representam-perdas.-notebook.ipynb](modulo-05-derivadas-2:-as-regras-do-jogo---regras-de-deriva√ß√£o:-regra-da-soma,-do-produto,-do-quociente.-aplica√ß√µes-em-fun√ß√µes-que-representam-perdas.-notebook.ipynb)
6. [modulo-06-regra-da-cadeia:-a-regra-de-ouro-do-backpropagation---o-notebook-mais-cr√≠tico.-detalhamento-da-regra-da-cadeia-para-compor-fun√ß√µes-e-sua-aplica√ß√£o-direta-no-c√°lculo-dos-gradientes-em-redes-neurais.-notebook.ipynb](modulo-06-regra-da-cadeia:-a-regra-de-ouro-do-backpropagation---o-notebook-mais-cr√≠tico.-detalhamento-da-regra-da-cadeia-para-compor-fun√ß√µes-e-sua-aplica√ß√£o-direta-no-c√°lculo-dos-gradientes-em-redes-neurais.-notebook.ipynb)
7. [modulo-07-integrais-1:-a-porra-da-integral-e-o-que-ela-mede-afinal?---introdu√ß√£o-ao-conceito-de-integral-definida-e-indefinida.-o-que-significa-a-√°rea-sob-a-curva-e-sua-rela√ß√£o-com-as-derivadas-(teorema-fundamental-do-c√°lculo).-notebook.ipynb](modulo-07-integrais-1:-a-porra-da-integral-e-o-que-ela-mede-afinal?---introdu√ß√£o-ao-conceito-de-integral-definida-e-indefinida.-o-que-significa-a-√°rea-sob-a-curva-e-sua-rela√ß√£o-com-as-derivadas-(teorema-fundamental-do-c√°lculo).-notebook.ipynb)
8. [modulo-09-fun√ß√µes-de-m√∫ltiplas-vari√°veis:-a-paisagem-do-erro---introdu√ß√£o-a-fun√ß√µes-com-m√∫ltiplas-vari√°veis,-que-representam-as-m√∫ltiplas-entradas-de-um-modelo.-notebook.ipynb](modulo-09-fun√ß√µes-de-m√∫ltiplas-vari√°veis:-a-paisagem-do-erro---introdu√ß√£o-a-fun√ß√µes-com-m√∫ltiplas-vari√°veis,-que-representam-as-m√∫ltiplas-entradas-de-um-modelo.-notebook.ipynb)
9. [modulo-10-derivadas-parciais-e-gradientes:-a-b√∫ssola-do-aprendizado---como-encontrar-a-derivada-de-uma-fun√ß√£o-com-v√°rias-vari√°veis.-o-vetor-gradiente-e-como-ele-aponta-na-dire√ß√£o-de-maior-inclina√ß√£o.-notebook.ipynb](modulo-10-derivadas-parciais-e-gradientes:-a-b√∫ssola-do-aprendizado---como-encontrar-a-derivada-de-uma-fun√ß√£o-com-v√°rias-vari√°veis.-o-vetor-gradiente-e-como-ele-aponta-na-dire√ß√£o-de-maior-inclina√ß√£o.-notebook.ipynb)
10. [modulo-11-gradiente-descendente-1:-a-descida-na-dire√ß√£o-certa---notebook-pr√°tico-sobre-o-algoritmo-do-gradiente-descendente.-implementa√ß√£o-do-algoritmo-para-otimizar-uma-fun√ß√£o-de-custo-simples.-notebook.ipynb](modulo-11-gradiente-descendente-1:-a-descida-na-dire√ß√£o-certa---notebook-pr√°tico-sobre-o-algoritmo-do-gradiente-descendente.-implementa√ß√£o-do-algoritmo-para-otimizar-uma-fun√ß√£o-de-custo-simples.-notebook.ipynb)
11. [modulo-12-gradiente-descendente-2:-varia√ß√µes-e-truques---explorando-varia√ß√µes-como-stochastic-gradient-descent-(sgd)-e-adam,-e-a-import√¢ncia-da-taxa-de-aprendizado.-notebook.ipynb](modulo-12-gradiente-descendente-2:-varia√ß√µes-e-truques---explorando-varia√ß√µes-como-stochastic-gradient-descent-(sgd)-e-adam,-e-a-import√¢ncia-da-taxa-de-aprendizado.-notebook.ipynb)
