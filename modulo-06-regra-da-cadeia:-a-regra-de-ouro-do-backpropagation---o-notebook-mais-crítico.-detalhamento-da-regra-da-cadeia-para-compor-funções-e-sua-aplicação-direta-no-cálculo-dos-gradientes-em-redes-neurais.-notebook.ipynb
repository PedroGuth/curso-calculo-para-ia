{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîó Regra da Cadeia: A Chave Secreta do Backpropagation\n\n## M√≥dulo 6: O Notebook Mais Cr√≠tico de Todo o Curso!\n\n### Por Pedro Nunes Guth\n\n---\n\nSalve galera! Chegou a hora do **MOMENTO MAIS IMPORTANTE** do nosso curso de C√°lculo para IA! üöÄ\n\nT√°, mas Pedro... por que esse notebook √© T√ÉO cr√≠tico assim? Bom, imagina que voc√™ √© um detetive tentando descobrir quem √© o culpado por cada erro que sua rede neural comete. A **Regra da Cadeia** √© sua lupa de investiga√ß√£o!\n\n√â ela que permite que a nossa rede neural **aprenda de tr√°s pra frente** (backpropagation), descobrindo exatamente quanto cada neur√¥nio contribuiu pro erro final.\n\n**üéØ O que vamos ver hoje:**\n- Como fun√ß√µes compostas funcionam na pr√°tica\n- A matem√°tica da regra da cadeia (sem drama!)\n- Aplica√ß√£o direta no backpropagation\n- Implementa√ß√£o passo a passo\n- Porque isso √© A ALMA de toda IA moderna\n\nBora mergulhar nessa! üèä‚Äç‚ôÇÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - As ferramentas do Pedro\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√£o dos gr√°ficos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"üîß Ferramentas carregadas!\")\n",
        "print(\"üìä Gr√°ficos configurados!\")\n",
        "print(\"üöÄ Bora come√ßar a brincadeira!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Parte 1: O Que Diabos √© uma Fun√ß√£o Composta?\n\nT√°, antes de partir pro backpropagation, vamos entender o conceito b√°sico. Lembra quando voc√™ era crian√ßa e brincava daqueles bonequinhos russos (matryoshka)? Uma boneca dentro da outra?\n\n**Fun√ß√£o composta √© exatamente isso!** Uma fun√ß√£o dentro da outra.\n\n## üé≠ A Analogia da F√°brica de Brigadeiros\n\nImagina uma f√°brica de brigadeiros com 3 etapas:\n1. **Esta√ß√£o 1**: Mistura os ingredientes ‚Üí $u = g(x)$\n2. **Esta√ß√£o 2**: Aquece a mistura ‚Üí $v = h(u)$ \n3. **Esta√ß√£o 3**: Enrola o brigadeiro ‚Üí $y = f(v)$\n\nO resultado final √©: $y = f(h(g(x)))$ - uma fun√ß√£o composta!\n\n## üìê Matematicamente Falando\n\nSe temos:\n- $y = f(u)$ onde $u = g(x)$\n- Ent√£o: $y = f(g(x))$ √© uma **fun√ß√£o composta**\n\nA **Regra da Cadeia** nos diz como derivar isso:\n\n$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n\n**üéØ Dica do Pedro:** Pensa como uma corrente - cada elo multiplica com o pr√≥ximo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos ver a regra da cadeia na pr√°tica!\n",
        "# Exemplo: y = (2x + 1)¬≥\n",
        "\n",
        "def funcao_composta_exemplo(x):\n",
        "    \"\"\"Fun√ß√£o composta: y = (2x + 1)¬≥\"\"\"\n",
        "    u = 2*x + 1  # Fun√ß√£o interna: g(x) = 2x + 1\n",
        "    y = u**3     # Fun√ß√£o externa: f(u) = u¬≥\n",
        "    return y, u\n",
        "\n",
        "def derivada_pela_regra_da_cadeia(x):\n",
        "    \"\"\"Derivada usando regra da cadeia\"\"\"\n",
        "    u = 2*x + 1\n",
        "    \n",
        "    # dy/du = 3u¬≤\n",
        "    dy_du = 3 * u**2\n",
        "    \n",
        "    # du/dx = 2\n",
        "    du_dx = 2\n",
        "    \n",
        "    # Regra da cadeia: dy/dx = (dy/du) √ó (du/dx)\n",
        "    dy_dx = dy_du * du_dx\n",
        "    \n",
        "    return dy_dx, dy_du, du_dx\n",
        "\n",
        "# Testando com x = 3\n",
        "x_teste = 3\n",
        "y, u = funcao_composta_exemplo(x_teste)\n",
        "derivada, dy_du, du_dx = derivada_pela_regra_da_cadeia(x_teste)\n",
        "\n",
        "print(f\"üìä Para x = {x_teste}:\")\n",
        "print(f\"   u = g(x) = 2({x_teste}) + 1 = {u}\")\n",
        "print(f\"   y = f(u) = ({u})¬≥ = {y}\")\n",
        "print(f\"\")\n",
        "print(f\"üîó Regra da Cadeia:\")\n",
        "print(f\"   dy/du = 3u¬≤ = 3({u})¬≤ = {dy_du}\")\n",
        "print(f\"   du/dx = 2\")\n",
        "print(f\"   dy/dx = {dy_du} √ó {du_dx} = {derivada}\")\n",
        "print(f\"\")\n",
        "print(f\"‚úÖ Resultado: A taxa de varia√ß√£o √© {derivada}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìà Visualizando a Regra da Cadeia\n\nVamos ver graficamente como isso funciona! √â sempre mais f√°cil entender vendo, n√©?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o da fun√ß√£o composta e sua derivada\n",
        "x = np.linspace(-2, 2, 400)\n",
        "\n",
        "# Fun√ß√£o composta: y = (2x + 1)¬≥\n",
        "y_composta = (2*x + 1)**3\n",
        "\n",
        "# Derivada pela regra da cadeia: dy/dx = 6(2x + 1)¬≤\n",
        "derivada = 6 * (2*x + 1)**2\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico da fun√ß√£o\n",
        "ax1.plot(x, y_composta, 'b-', linewidth=3, label='y = (2x + 1)¬≥')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_title('Fun√ß√£o Composta', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "\n",
        "# Destacar um ponto espec√≠fico\n",
        "x_ponto = 0.5\n",
        "y_ponto = (2*x_ponto + 1)**3\n",
        "ax1.plot(x_ponto, y_ponto, 'ro', markersize=8)\n",
        "ax1.annotate(f'({x_ponto}, {y_ponto:.1f})', \n",
        "             xy=(x_ponto, y_ponto), \n",
        "             xytext=(x_ponto+0.2, y_ponto+2),\n",
        "             arrowprops=dict(arrowstyle='->', color='red'))\n",
        "\n",
        "# Gr√°fico da derivada\n",
        "ax2.plot(x, derivada, 'r-', linewidth=3, label=\"dy/dx = 6(2x + 1)¬≤\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('Derivada')\n",
        "ax2.set_title('Derivada (Inclina√ß√£o)', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "\n",
        "# Destacar o mesmo ponto na derivada\n",
        "derivada_ponto = 6 * (2*x_ponto + 1)**2\n",
        "ax2.plot(x_ponto, derivada_ponto, 'ro', markersize=8)\n",
        "ax2.annotate(f'Inclina√ß√£o = {derivada_ponto:.1f}', \n",
        "             xy=(x_ponto, derivada_ponto), \n",
        "             xytext=(x_ponto+0.2, derivada_ponto+5),\n",
        "             arrowprops=dict(arrowstyle='->', color='red'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Observe que:\")\n",
        "print(f\"   ‚Ä¢ No ponto x = {x_ponto}, a fun√ß√£o vale {y_ponto:.1f}\")\n",
        "print(f\"   ‚Ä¢ A inclina√ß√£o (derivada) nesse ponto √© {derivada_ponto:.1f}\")\n",
        "print(f\"   ‚Ä¢ Isso significa que a fun√ß√£o est√° crescendo rapidamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîó Parte 2: A Matem√°tica Descomplicada da Regra da Cadeia\n\nAgora que voc√™ j√° viu na pr√°tica, vamos entender a matem√°tica por tr√°s. Relaxa, vai ser moleza!\n\n## üéØ O Teorema da Regra da Cadeia\n\n**Vers√£o Formal:** Se $y = f(u)$ e $u = g(x)$, ent√£o:\n\n$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n\n**Vers√£o Pedro:** \"A derivada da fun√ß√£o de fora vezes a derivada da fun√ß√£o de dentro!\"\n\n## üèóÔ∏è Constru√ß√£o Intuitiva\n\nImagina que voc√™ quer saber como $y$ muda quando $x$ muda um pouquinho:\n\n1. **Passo 1**: $x$ muda ‚Üí isso afeta $u$ na taxa $\\frac{du}{dx}$\n2. **Passo 2**: $u$ muda ‚Üí isso afeta $y$ na taxa $\\frac{dy}{du}$\n3. **Resultado**: O efeito total √© o produto das duas taxas!\n\n## üîÑ Para M√∫ltiplas Camadas\n\nSe temos $y = f(v)$, $v = h(u)$, $u = g(x)$:\n\n$$\\frac{dy}{dx} = \\frac{dy}{dv} \\cdot \\frac{dv}{du} \\cdot \\frac{du}{dx}$$\n\n**üéØ Dica do Pedro:** √â como uma corrente - cada elo multiplica com o pr√≥ximo, n√£o importa quantos elos temos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementa√ß√£o da regra da cadeia para m√∫ltiplas camadas\n",
        "class CadeiaDerivativas:\n",
        "    \"\"\"Classe para demonstrar a regra da cadeia com m√∫ltiplas fun√ß√µes\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.historico = []\n",
        "    \n",
        "    def funcao_composta_complexa(self, x):\n",
        "        \"\"\"Exemplo: y = sin(e^(x¬≤))\"\"\"\n",
        "        \n",
        "        # Camada 1: u = x¬≤\n",
        "        u = x**2\n",
        "        \n",
        "        # Camada 2: v = e^u\n",
        "        v = np.exp(u)\n",
        "        \n",
        "        # Camada 3: y = sin(v)\n",
        "        y = np.sin(v)\n",
        "        \n",
        "        return y, v, u\n",
        "    \n",
        "    def calcular_derivadas(self, x):\n",
        "        \"\"\"Calcula as derivadas usando regra da cadeia\"\"\"\n",
        "        \n",
        "        y, v, u = self.funcao_composta_complexa(x)\n",
        "        \n",
        "        # Derivadas parciais\n",
        "        du_dx = 2*x              # d/dx(x¬≤) = 2x\n",
        "        dv_du = np.exp(u)        # d/du(e^u) = e^u\n",
        "        dy_dv = np.cos(v)        # d/dv(sin(v)) = cos(v)\n",
        "        \n",
        "        # Regra da cadeia: dy/dx = (dy/dv) √ó (dv/du) √ó (du/dx)\n",
        "        dy_dx = dy_dv * dv_du * du_dx\n",
        "        \n",
        "        # Guardar para visualiza√ß√£o\n",
        "        resultado = {\n",
        "            'x': x, 'u': u, 'v': v, 'y': y,\n",
        "            'du_dx': du_dx, 'dv_du': dv_du, 'dy_dv': dy_dv,\n",
        "            'dy_dx': dy_dx\n",
        "        }\n",
        "        \n",
        "        return resultado\n",
        "    \n",
        "    def mostrar_calculo(self, x):\n",
        "        \"\"\"Mostra o c√°lculo passo a passo\"\"\"\n",
        "        \n",
        "        resultado = self.calcular_derivadas(x)\n",
        "        \n",
        "        print(f\"üî¢ Calculando para x = {x}\")\n",
        "        print(f\"\" + \"=\"*50)\n",
        "        print(f\"üìç Valores das fun√ß√µes:\")\n",
        "        print(f\"   u = x¬≤ = ({x})¬≤ = {resultado['u']:.4f}\")\n",
        "        print(f\"   v = e^u = e^{resultado['u']:.4f} = {resultado['v']:.4f}\")\n",
        "        print(f\"   y = sin(v) = sin({resultado['v']:.4f}) = {resultado['y']:.4f}\")\n",
        "        print(f\"\")\n",
        "        print(f\"üîó Derivadas individuais:\")\n",
        "        print(f\"   du/dx = 2x = 2({x}) = {resultado['du_dx']:.4f}\")\n",
        "        print(f\"   dv/du = e^u = {resultado['dv_du']:.4f}\")\n",
        "        print(f\"   dy/dv = cos(v) = {resultado['dy_dv']:.4f}\")\n",
        "        print(f\"\")\n",
        "        print(f\"‚ö° REGRA DA CADEIA:\")\n",
        "        print(f\"   dy/dx = (dy/dv) √ó (dv/du) √ó (du/dx)\")\n",
        "        print(f\"   dy/dx = {resultado['dy_dv']:.4f} √ó {resultado['dv_du']:.4f} √ó {resultado['du_dx']:.4f}\")\n",
        "        print(f\"   dy/dx = {resultado['dy_dx']:.4f}\")\n",
        "        \n",
        "        return resultado\n",
        "\n",
        "# Testando nossa classe\n",
        "cadeia = CadeiaDerivativas()\n",
        "resultado = cadeia.mostrar_calculo(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Parte 3: Entrando no Mundo das Redes Neurais\n\nAgora vem a parte **MAIS IMPORTANTE** do curso todo! Como a regra da cadeia funciona nas redes neurais?\n\n## üèóÔ∏è Anatomia de um Neur√¥nio\n\nUm neur√¥nio faz basicamente isto:\n1. **Soma ponderada**: $z = w_1x_1 + w_2x_2 + ... + w_nx_n + b$\n2. **Fun√ß√£o de ativa√ß√£o**: $a = \\sigma(z)$ (sigmoid, ReLU, etc.)\n\nIsso √© uma **fun√ß√£o composta**! $a = \\sigma(w \\cdot x + b)$\n\n## üéØ O Problema do Aprendizado\n\nNossa rede comete erros. Queremos saber:\n- \"**Quanto cada peso $w$ contribuiu pro erro?**\"\n- \"**Em que dire√ß√£o devo ajustar cada peso?**\"\n\nA resposta est√° na derivada: $\\frac{\\partial \\text{Erro}}{\\partial w}$\n\n## üîÑ Backpropagation = Regra da Cadeia\n\nO backpropagation √© **literalmente** a regra da cadeia aplicada de tr√°s pra frente!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-06_img_01.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um neur√¥nio simples para ver a regra da cadeia em a√ß√£o\n",
        "class NeuronioSimples:\n",
        "    \"\"\"Um neur√¥nio para demonstrar backpropagation\"\"\"\n",
        "    \n",
        "    def __init__(self, pesos, bias):\n",
        "        self.w = np.array(pesos)  # Pesos\n",
        "        self.b = bias             # Bias\n",
        "        \n",
        "        # Guardar valores para backprop\n",
        "        self.x = None\n",
        "        self.z = None\n",
        "        self.a = None\n",
        "    \n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Fun√ß√£o sigmoid: œÉ(z) = 1/(1 + e^(-z))\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "    \n",
        "    def sigmoid_derivada(self, z):\n",
        "        \"\"\"Derivada da sigmoid: œÉ'(z) = œÉ(z)(1 - œÉ(z))\"\"\"\n",
        "        s = self.sigmoid(z)\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass - calcular sa√≠da\"\"\"\n",
        "        self.x = np.array(x)\n",
        "        \n",
        "        # Soma ponderada: z = w¬∑x + b\n",
        "        self.z = np.dot(self.w, self.x) + self.b\n",
        "        \n",
        "        # Ativa√ß√£o: a = œÉ(z)\n",
        "        self.a = self.sigmoid(self.z)\n",
        "        \n",
        "        return self.a\n",
        "    \n",
        "    def backward(self, erro_da_saida):\n",
        "        \"\"\"Backward pass - calcular gradientes usando REGRA DA CADEIA!\"\"\"\n",
        "        \n",
        "        # REGRA DA CADEIA em a√ß√£o!\n",
        "        # dL/dw = (dL/da) √ó (da/dz) √ó (dz/dw)\n",
        "        \n",
        "        # 1. dL/da j√° temos (erro_da_saida)\n",
        "        dL_da = erro_da_saida\n",
        "        \n",
        "        # 2. da/dz = derivada da sigmoid\n",
        "        da_dz = self.sigmoid_derivada(self.z)\n",
        "        \n",
        "        # 3. dz/dw = x (derivada de w¬∑x + b em rela√ß√£o a w)\n",
        "        dz_dw = self.x\n",
        "        \n",
        "        # 4. dz/db = 1 (derivada de w¬∑x + b em rela√ß√£o a b)\n",
        "        dz_db = 1\n",
        "        \n",
        "        # APLICANDO A REGRA DA CADEIA!\n",
        "        dL_dw = dL_da * da_dz * dz_dw  # Gradiente dos pesos\n",
        "        dL_db = dL_da * da_dz * dz_db  # Gradiente do bias\n",
        "        \n",
        "        return dL_dw, dL_db, da_dz\n",
        "    \n",
        "    def mostrar_calculo(self, x, erro):\n",
        "        \"\"\"Mostra o c√°lculo completo\"\"\"\n",
        "        \n",
        "        print(\"üß† NEUR√îNIO EM A√á√ÉO!\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Forward pass\n",
        "        saida = self.forward(x)\n",
        "        print(f\"üìä FORWARD PASS:\")\n",
        "        print(f\"   Entrada: x = {self.x}\")\n",
        "        print(f\"   Pesos: w = {self.w}\")\n",
        "        print(f\"   Bias: b = {self.b}\")\n",
        "        print(f\"   Soma: z = w¬∑x + b = {self.z:.4f}\")\n",
        "        print(f\"   Sa√≠da: a = œÉ(z) = {self.a:.4f}\")\n",
        "        print()\n",
        "        \n",
        "        # Backward pass\n",
        "        dL_dw, dL_db, da_dz = self.backward(erro)\n",
        "        print(f\"üîÑ BACKWARD PASS (Regra da Cadeia):\")\n",
        "        print(f\"   Erro da sa√≠da: dL/da = {erro:.4f}\")\n",
        "        print(f\"   Derivada sigmoid: da/dz = {da_dz:.4f}\")\n",
        "        print(f\"   Derivada z vs w: dz/dw = x = {self.x}\")\n",
        "        print(f\"   \")\n",
        "        print(f\"   üîó REGRA DA CADEIA:\")\n",
        "        print(f\"   dL/dw = (dL/da) √ó (da/dz) √ó (dz/dw)\")\n",
        "        print(f\"   dL/dw = {erro:.4f} √ó {da_dz:.4f} √ó {self.x} = {dL_dw}\")\n",
        "        print(f\"   dL/db = {erro:.4f} √ó {da_dz:.4f} √ó 1 = {dL_db:.4f}\")\n",
        "        \n",
        "        return dL_dw, dL_db\n",
        "\n",
        "# Testando nosso neur√¥nio\n",
        "neuronio = NeuronioSimples(pesos=[0.5, -0.3], bias=0.1)\n",
        "x_entrada = [1.0, 2.0]\n",
        "erro_exemplo = 0.2  # Digamos que o erro seja 0.2\n",
        "\n",
        "gradientes_w, gradiente_b = neuronio.mostrar_calculo(x_entrada, erro_exemplo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Visualizando o Backpropagation\n\nVamos ver como os gradientes fluem pela rede!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o do fluxo de gradientes\n",
        "def visualizar_gradientes():\n",
        "    \"\"\"Cria uma visualiza√ß√£o do fluxo de gradientes\"\"\"\n",
        "    \n",
        "    # Simula√ß√£o de uma rede com 3 camadas\n",
        "    camadas = ['Entrada', 'Oculta 1', 'Oculta 2', 'Sa√≠da']\n",
        "    posicoes_x = [0, 1, 2, 3]\n",
        "    \n",
        "    # Simula√ß√£o de gradientes (quanto maior, mais importante)\n",
        "    gradientes = [1.0, 0.8, 0.5, 0.2]  # Diminuem conforme voltamos\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "    \n",
        "    # Gr√°fico 1: Forward Pass\n",
        "    ax1.plot(posicoes_x, [1, 0.8, 0.6, 0.4], 'bo-', linewidth=3, markersize=10)\n",
        "    for i, (pos, camada) in enumerate(zip(posicoes_x, camadas)):\n",
        "        ax1.annotate(camada, (pos, [1, 0.8, 0.6, 0.4][i]), \n",
        "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "    \n",
        "    ax1.set_title('Forward Pass: Informa√ß√£o Indo Pra Frente', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Ativa√ß√£o')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_xlim(-0.5, 3.5)\n",
        "    \n",
        "    # Setas mostrando dire√ß√£o\n",
        "    for i in range(len(posicoes_x)-1):\n",
        "        ax1.annotate('', xy=(posicoes_x[i+1]-0.1, 0.2), xytext=(posicoes_x[i]+0.1, 0.2),\n",
        "                    arrowprops=dict(arrowstyle='->', lw=2, color='blue'))\n",
        "    \n",
        "    # Gr√°fico 2: Backward Pass\n",
        "    cores = plt.cm.Reds([0.3, 0.5, 0.7, 0.9])  # Cores baseadas na magnitude do gradiente\n",
        "    barras = ax2.bar(posicoes_x, gradientes, color=cores, alpha=0.7, width=0.6)\n",
        "    \n",
        "    # Adicionar valores nas barras\n",
        "    for i, (pos, grad, camada) in enumerate(zip(posicoes_x, gradientes, camadas)):\n",
        "        ax2.text(pos, grad + 0.05, f'{grad:.1f}', ha='center', fontweight='bold')\n",
        "        ax2.text(pos, -0.15, camada, ha='center', rotation=45)\n",
        "    \n",
        "    ax2.set_title('Backward Pass: Gradientes Voltando (Regra da Cadeia!)', fontsize=14, fontweight='bold')\n",
        "    ax2.set_ylabel('Magnitude do Gradiente')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_xlim(-0.5, 3.5)\n",
        "    ax2.set_ylim(-0.3, 1.2)\n",
        "    \n",
        "    # Setas mostrando dire√ß√£o reversa\n",
        "    for i in range(len(posicoes_x)-1, 0, -1):\n",
        "        ax2.annotate('', xy=(posicoes_x[i-1]+0.1, 0.05), xytext=(posicoes_x[i]-0.1, 0.05),\n",
        "                    arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"üìä Observe que:\")\n",
        "    print(\"   üîµ Forward: Informa√ß√£o flui da entrada para a sa√≠da\")\n",
        "    print(\"   üî¥ Backward: Gradientes fluem da sa√≠da para a entrada\")\n",
        "    print(\"   üìâ Gradientes diminuem conforme voltamos (problema do vanishing gradient!)\")\n",
        "    print(\"   üîó Cada gradiente √© calculado pela REGRA DA CADEIA!\")\n",
        "\n",
        "visualizar_gradientes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üåä Parte 4: Backpropagation em uma Rede Completa\n\nAgora vamos implementar o backpropagation completo em uma rede neural simples!\n\n## üèóÔ∏è Arquitetura da Nossa Rede\n\nVamos criar uma rede com:\n- **Entrada**: 2 neur√¥nios\n- **Camada oculta**: 3 neur√¥nios (sigmoid)\n- **Sa√≠da**: 1 neur√¥nio (sigmoid)\n\n## üéØ O Fluxo Completo\n\n1. **Forward**: $x \\rightarrow h \\rightarrow y$\n2. **Erro**: $L = \\frac{1}{2}(y - y_{target})^2$\n3. **Backward**: Regra da cadeia para cada peso!\n\n**üéØ Dica do Pedro:** √â aqui que a m√°gica acontece! Cada peso recebe exatamente o gradiente que merece!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementa√ß√£o completa de uma rede neural com backpropagation\n",
        "class RedeNeuralSimples:\n",
        "    \"\"\"Rede neural simples para demonstrar backpropagation\"\"\"\n",
        "    \n",
        "    def __init__(self, entrada=2, oculta=3, saida=1):\n",
        "        # Inicializa√ß√£o aleat√≥ria dos pesos (pequenos valores)\n",
        "        self.W1 = np.random.randn(entrada, oculta) * 0.5    # Pesos entrada -> oculta\n",
        "        self.b1 = np.zeros((1, oculta))                     # Bias camada oculta\n",
        "        self.W2 = np.random.randn(oculta, saida) * 0.5      # Pesos oculta -> sa√≠da\n",
        "        self.b2 = np.zeros((1, saida))                      # Bias sa√≠da\n",
        "        \n",
        "        # Para guardar valores durante forward pass\n",
        "        self.z1 = None\n",
        "        self.a1 = None\n",
        "        self.z2 = None\n",
        "        self.a2 = None\n",
        "        self.X = None\n",
        "        \n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Fun√ß√£o sigmoid\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "    \n",
        "    def sigmoid_derivada(self, z):\n",
        "        \"\"\"Derivada da sigmoid\"\"\"\n",
        "        s = self.sigmoid(z)\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward propagation\"\"\"\n",
        "        self.X = X\n",
        "        \n",
        "        # Camada oculta: z1 = X¬∑W1 + b1, a1 = œÉ(z1)\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "        \n",
        "        # Camada de sa√≠da: z2 = a1¬∑W2 + b2, a2 = œÉ(z2)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        \n",
        "        return self.a2\n",
        "    \n",
        "    def calcular_erro(self, y_true, y_pred):\n",
        "        \"\"\"Fun√ß√£o de custo: Mean Squared Error\"\"\"\n",
        "        return 0.5 * np.mean((y_true - y_pred)**2)\n",
        "    \n",
        "    def backward(self, X, y_true):\n",
        "        \"\"\"Backward propagation usando REGRA DA CADEIA!\"\"\"\n",
        "        m = X.shape[0]  # N√∫mero de exemplos\n",
        "        \n",
        "        # PASSO 1: Gradiente da fun√ß√£o de custo\n",
        "        # dL/da2 = (a2 - y_true)\n",
        "        dL_da2 = (self.a2 - y_true)\n",
        "        \n",
        "        # PASSO 2: Gradientes da camada de sa√≠da (REGRA DA CADEIA!)\n",
        "        # dL/dz2 = (dL/da2) √ó (da2/dz2)\n",
        "        da2_dz2 = self.sigmoid_derivada(self.z2)\n",
        "        dL_dz2 = dL_da2 * da2_dz2\n",
        "        \n",
        "        # dL/dW2 = (dL/dz2) √ó (dz2/dW2) = (dL/dz2) √ó a1\n",
        "        dL_dW2 = np.dot(self.a1.T, dL_dz2) / m\n",
        "        \n",
        "        # dL/db2 = (dL/dz2) √ó (dz2/db2) = (dL/dz2) √ó 1\n",
        "        dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True) / m\n",
        "        \n",
        "        # PASSO 3: Gradientes da camada oculta (REGRA DA CADEIA NOVAMENTE!)\n",
        "        # dL/da1 = (dL/dz2) √ó (dz2/da1) = (dL/dz2) √ó W2\n",
        "        dL_da1 = np.dot(dL_dz2, self.W2.T)\n",
        "        \n",
        "        # dL/dz1 = (dL/da1) √ó (da1/dz1)\n",
        "        da1_dz1 = self.sigmoid_derivada(self.z1)\n",
        "        dL_dz1 = dL_da1 * da1_dz1\n",
        "        \n",
        "        # dL/dW1 = (dL/dz1) √ó (dz1/dW1) = (dL/dz1) √ó X\n",
        "        dL_dW1 = np.dot(X.T, dL_dz1) / m\n",
        "        \n",
        "        # dL/db1 = (dL/dz1) √ó (dz1/db1) = (dL/dz1) √ó 1\n",
        "        dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True) / m\n",
        "        \n",
        "        gradientes = {\n",
        "            'dL_dW2': dL_dW2, 'dL_db2': dL_db2,\n",
        "            'dL_dW1': dL_dW1, 'dL_db1': dL_db1,\n",
        "            'dL_dz2': dL_dz2, 'dL_dz1': dL_dz1\n",
        "        }\n",
        "        \n",
        "        return gradientes\n",
        "    \n",
        "    def treinar_um_passo(self, X, y, taxa_aprendizado=0.1):\n",
        "        \"\"\"Um passo de treinamento\"\"\"\n",
        "        \n",
        "        # Forward pass\n",
        "        y_pred = self.forward(X)\n",
        "        \n",
        "        # Calcular erro\n",
        "        erro = self.calcular_erro(y, y_pred)\n",
        "        \n",
        "        # Backward pass\n",
        "        gradientes = self.backward(X, y)\n",
        "        \n",
        "        # Atualizar pesos (Gradient Descent)\n",
        "        self.W2 -= taxa_aprendizado * gradientes['dL_dW2']\n",
        "        self.b2 -= taxa_aprendizado * gradientes['dL_db2']\n",
        "        self.W1 -= taxa_aprendizado * gradientes['dL_dW1']\n",
        "        self.b1 -= taxa_aprendizado * gradientes['dL_db1']\n",
        "        \n",
        "        return erro, gradientes\n",
        "\n",
        "# Criando nossa rede\n",
        "rede = RedeNeuralSimples()\n",
        "\n",
        "# Dados de exemplo (problema XOR)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])  # XOR\n",
        "\n",
        "print(\"üß† REDE NEURAL CRIADA!\")\n",
        "print(f\"üìä Dados de entrada: {X.shape}\")\n",
        "print(f\"üìä Dados de sa√≠da: {y.shape}\")\n",
        "print(f\"üîß Pesos W1: {rede.W1.shape}\")\n",
        "print(f\"üîß Pesos W2: {rede.W2.shape}\")\n",
        "print(f\"\")\n",
        "print(f\"üéØ Vamos treinar para resolver o problema XOR!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos treinar nossa rede e ver a regra da cadeia em a√ß√£o!\n",
        "def treinar_e_visualizar(rede, X, y, epocas=1000):\n",
        "    \"\"\"Treina a rede e mostra a evolu√ß√£o\"\"\"\n",
        "    \n",
        "    historico_erro = []\n",
        "    historico_gradientes = []\n",
        "    \n",
        "    print(\"üöÄ COME√áANDO O TREINAMENTO!\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for epoca in range(epocas):\n",
        "        erro, gradientes = rede.treinar_um_passo(X, y, taxa_aprendizado=1.0)\n",
        "        \n",
        "        historico_erro.append(erro)\n",
        "        \n",
        "        # Guardar magnitude dos gradientes\n",
        "        mag_grad = np.mean(np.abs(gradientes['dL_dW2'])) + np.mean(np.abs(gradientes['dL_dW1']))\n",
        "        historico_gradientes.append(mag_grad)\n",
        "        \n",
        "        # Mostrar progresso\n",
        "        if epoca % 200 == 0:\n",
        "            y_pred = rede.forward(X)\n",
        "            print(f\"√âpoca {epoca:4d}: Erro = {erro:.6f}, |Gradientes| = {mag_grad:.6f}\")\n",
        "            print(f\"             Predi√ß√µes: {y_pred.flatten()}\")\n",
        "    \n",
        "    return historico_erro, historico_gradientes\n",
        "\n",
        "# Treinar nossa rede\n",
        "historico_erro, historico_grad = treinar_e_visualizar(rede, X, y)\n",
        "\n",
        "print(\"\\n‚úÖ TREINAMENTO CONCLU√çDO!\")\n",
        "print(\"\\nüìä RESULTADO FINAL:\")\n",
        "y_final = rede.forward(X)\n",
        "for i in range(len(X)):\n",
        "    print(f\"   Input: {X[i]} ‚Üí Esperado: {y[i][0]} ‚Üí Predi√ß√£o: {y_final[i][0]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o da evolu√ß√£o do treinamento\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico do erro\n",
        "ax1.plot(historico_erro, 'b-', linewidth=2)\n",
        "ax1.set_title('Evolu√ß√£o do Erro Durante o Treinamento', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('√âpoca')\n",
        "ax1.set_ylabel('Erro (MSE)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_yscale('log')\n",
        "\n",
        "# Adicionar anota√ß√µes\n",
        "ax1.annotate('Erro diminuindo\\ngra√ßas aos gradientes!', \n",
        "            xy=(500, historico_erro[500]), xytext=(300, historico_erro[100]),\n",
        "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
        "            fontsize=12, ha='center',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='yellow', alpha=0.7))\n",
        "\n",
        "# Gr√°fico dos gradientes\n",
        "ax2.plot(historico_grad, 'r-', linewidth=2, label='Magnitude dos Gradientes')\n",
        "ax2.set_title('Evolu√ß√£o dos Gradientes (Regra da Cadeia)', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('√âpoca')\n",
        "ax2.set_ylabel('Magnitude M√©dia dos Gradientes')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "# Adicionar anota√ß√µes\n",
        "ax2.annotate('Gradientes ficam menores\\nconforme convergimos', \n",
        "            xy=(800, historico_grad[800]), xytext=(400, max(historico_grad)*0.7),\n",
        "            arrowprops=dict(arrowstyle='->', color='blue', lw=2),\n",
        "            fontsize=12, ha='center',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue', alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä AN√ÅLISE DOS GR√ÅFICOS:\")\n",
        "print(f\"   üîµ Erro final: {historico_erro[-1]:.6f}\")\n",
        "print(f\"   üî¥ Gradiente final: {historico_grad[-1]:.6f}\")\n",
        "print(f\"   ‚úÖ A rede aprendeu o XOR usando a REGRA DA CADEIA!\")\n",
        "print(f\"   üîó Cada peso foi ajustado na dire√ß√£o correta gra√ßas ao backpropagation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Parte 5: Exemplo Pr√°tico - Demonstra√ß√£o Detalhada\n\nVamos fazer um exemplo **SUPER DETALHADO** mostrando cada passo da regra da cadeia em uma itera√ß√£o!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo super detalhado - Passo a passo da regra da cadeia\n",
        "def exemplo_detalhado_regra_cadeia():\n",
        "    \"\"\"Mostra EXATAMENTE como a regra da cadeia funciona\"\"\"\n",
        "    \n",
        "    print(\"üîç EXEMPLO SUPER DETALHADO DA REGRA DA CADEIA\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    \n",
        "    # Rede simples: 2 entradas ‚Üí 1 sa√≠da\n",
        "    # Fun√ß√£o: y = œÉ(w1*x1 + w2*x2 + b)\n",
        "    \n",
        "    # Valores de exemplo\n",
        "    x1, x2 = 0.5, 1.0\n",
        "    w1, w2 = 0.3, -0.4\n",
        "    b = 0.1\n",
        "    y_target = 0.8  # Valor que queremos\n",
        "    \n",
        "    print(f\"üìä CONFIGURA√á√ÉO:\")\n",
        "    print(f\"   Entradas: x1 = {x1}, x2 = {x2}\")\n",
        "    print(f\"   Pesos: w1 = {w1}, w2 = {w2}\")\n",
        "    print(f\"   Bias: b = {b}\")\n",
        "    print(f\"   Alvo: y_target = {y_target}\")\n",
        "    print()\n",
        "    \n",
        "    # PASSO 1: Forward Pass\n",
        "    print(f\"üîÑ PASSO 1: FORWARD PASS\")\n",
        "    z = w1*x1 + w2*x2 + b\n",
        "    y = 1 / (1 + np.exp(-z))  # sigmoid\n",
        "    \n",
        "    print(f\"   z = w1*x1 + w2*x2 + b\")\n",
        "    print(f\"   z = {w1}*{x1} + {w2}*{x2} + {b} = {z}\")\n",
        "    print(f\"   y = œÉ(z) = œÉ({z}) = {y:.4f}\")\n",
        "    print()\n",
        "    \n",
        "    # PASSO 2: Calcular Erro\n",
        "    print(f\"üìä PASSO 2: CALCULAR ERRO\")\n",
        "    erro = 0.5 * (y - y_target)**2\n",
        "    print(f\"   L = 0.5 * (y - y_target)¬≤\")\n",
        "    print(f\"   L = 0.5 * ({y:.4f} - {y_target})¬≤ = {erro:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # PASSO 3: Backward Pass - REGRA DA CADEIA!\n",
        "    print(f\"üîó PASSO 3: BACKWARD PASS (REGRA DA CADEIA!)\")\n",
        "    print(f\"\" + \"=\"*50)\n",
        "    \n",
        "    # Gradiente da fun√ß√£o de custo\n",
        "    dL_dy = y - y_target\n",
        "    print(f\"1Ô∏è‚É£ Derivada do erro:\")\n",
        "    print(f\"   dL/dy = y - y_target = {y:.4f} - {y_target} = {dL_dy:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # Derivada da sigmoid\n",
        "    dy_dz = y * (1 - y)\n",
        "    print(f\"2Ô∏è‚É£ Derivada da sigmoid:\")\n",
        "    print(f\"   dy/dz = y(1-y) = {y:.4f} * (1-{y:.4f}) = {dy_dz:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # Derivadas das somas ponderadas\n",
        "    dz_dw1 = x1\n",
        "    dz_dw2 = x2\n",
        "    dz_db = 1\n",
        "    \n",
        "    print(f\"3Ô∏è‚É£ Derivadas da soma ponderada:\")\n",
        "    print(f\"   dz/dw1 = x1 = {dz_dw1}\")\n",
        "    print(f\"   dz/dw2 = x2 = {dz_dw2}\")\n",
        "    print(f\"   dz/db = 1 = {dz_db}\")\n",
        "    print()\n",
        "    \n",
        "    # APLICANDO A REGRA DA CADEIA!\n",
        "    print(f\"üéØ APLICANDO A REGRA DA CADEIA:\")\n",
        "    print(f\"\" + \"-\"*40)\n",
        "    \n",
        "    dL_dw1 = dL_dy * dy_dz * dz_dw1\n",
        "    dL_dw2 = dL_dy * dy_dz * dz_dw2\n",
        "    dL_db = dL_dy * dy_dz * dz_db\n",
        "    \n",
        "    print(f\"dL/dw1 = (dL/dy) √ó (dy/dz) √ó (dz/dw1)\")\n",
        "    print(f\"dL/dw1 = {dL_dy:.6f} √ó {dy_dz:.6f} √ó {dz_dw1} = {dL_dw1:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"dL/dw2 = (dL/dy) √ó (dy/dz) √ó (dz/dw2)\")\n",
        "    print(f\"dL/dw2 = {dL_dy:.6f} √ó {dy_dz:.6f} √ó {dz_dw2} = {dL_dw2:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"dL/db = (dL/dy) √ó (dy/dz) √ó (dz/db)\")\n",
        "    print(f\"dL/db = {dL_dy:.6f} √ó {dy_dz:.6f} √ó {dz_db} = {dL_db:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # INTERPRETA√á√ÉO\n",
        "    print(f\"üß† INTERPRETA√á√ÉO:\")\n",
        "    print(f\"\" + \"=\"*30)\n",
        "    print(f\"   ‚Ä¢ dL/dw1 = {dL_dw1:.6f} {'(positivo ‚Üí aumentar w1 aumenta erro)' if dL_dw1 > 0 else '(negativo ‚Üí aumentar w1 diminui erro)'}\")\n",
        "    print(f\"   ‚Ä¢ dL/dw2 = {dL_dw2:.6f} {'(positivo ‚Üí aumentar w2 aumenta erro)' if dL_dw2 > 0 else '(negativo ‚Üí aumentar w2 diminui erro)'}\")\n",
        "    print(f\"   ‚Ä¢ dL/db = {dL_db:.6f} {'(positivo ‚Üí aumentar b aumenta erro)' if dL_db > 0 else '(negativo ‚Üí aumentar b diminui erro)'}\")\n",
        "    print()\n",
        "    \n",
        "    # Atualiza√ß√£o dos pesos\n",
        "    taxa = 0.1\n",
        "    w1_novo = w1 - taxa * dL_dw1\n",
        "    w2_novo = w2 - taxa * dL_dw2\n",
        "    b_novo = b - taxa * dL_db\n",
        "    \n",
        "    print(f\"üîß ATUALIZA√á√ÉO DOS PESOS (taxa = {taxa}):\")\n",
        "    print(f\"   w1_novo = {w1} - {taxa} * {dL_dw1:.6f} = {w1_novo:.6f}\")\n",
        "    print(f\"   w2_novo = {w2} - {taxa} * {dL_dw2:.6f} = {w2_novo:.6f}\")\n",
        "    print(f\"   b_novo = {b} - {taxa} * {dL_db:.6f} = {b_novo:.6f}\")\n",
        "    \n",
        "    return {\n",
        "        'gradientes': [dL_dw1, dL_dw2, dL_db],\n",
        "        'pesos_novos': [w1_novo, w2_novo, b_novo],\n",
        "        'erro': erro\n",
        "    }\n",
        "\n",
        "resultado = exemplo_detalhado_regra_cadeia()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üí° Exerc√≠cio Pr√°tico 1: Implementa√ß√£o Manual\n\nAgora √© sua vez! Vamos implementar a regra da cadeia passo a passo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 1: Implemente a regra da cadeia para esta fun√ß√£o\n",
        "# Fun√ß√£o: y = (3x¬≤ + 2x + 1)‚Å¥\n",
        "\n",
        "def exercicio_1():\n",
        "    \"\"\"DESAFIO: Calcule dy/dx usando regra da cadeia\"\"\"\n",
        "    \n",
        "    print(\"üéØ EXERC√çCIO 1: Regra da Cadeia Manual\")\n",
        "    print(\"=\" * 45)\n",
        "    print(\"Fun√ß√£o: y = (3x¬≤ + 2x + 1)‚Å¥\")\n",
        "    print()\n",
        "    \n",
        "    def funcao_composta(x):\n",
        "        \"\"\"y = (3x¬≤ + 2x + 1)‚Å¥\"\"\"\n",
        "        u = 3*x**2 + 2*x + 1  # Fun√ß√£o interna\n",
        "        y = u**4               # Fun√ß√£o externa\n",
        "        return y, u\n",
        "    \n",
        "    def derivada_manual(x):\n",
        "        \"\"\"COMPLETE ESTA FUN√á√ÉO!\"\"\"\n",
        "        \n",
        "        # Passo 1: Calcular u\n",
        "        u = 3*x**2 + 2*x + 1\n",
        "        \n",
        "        # Passo 2: Derivada da fun√ß√£o externa (dy/du)\n",
        "        # DICA: se y = u‚Å¥, ent√£o dy/du = ?\n",
        "        dy_du = 4 * u**3  # COMPLETE AQUI!\n",
        "        \n",
        "        # Passo 3: Derivada da fun√ß√£o interna (du/dx)\n",
        "        # DICA: se u = 3x¬≤ + 2x + 1, ent√£o du/dx = ?\n",
        "        du_dx = 6*x + 2   # COMPLETE AQUI!\n",
        "        \n",
        "        # Passo 4: Regra da cadeia\n",
        "        dy_dx = dy_du * du_dx  # COMPLETE AQUI!\n",
        "        \n",
        "        return dy_dx, dy_du, du_dx, u\n",
        "    \n",
        "    # Teste com x = 2\n",
        "    x_teste = 2\n",
        "    y, u = funcao_composta(x_teste)\n",
        "    derivada, dy_du, du_dx, u_calc = derivada_manual(x_teste)\n",
        "    \n",
        "    print(f\"Para x = {x_teste}:\")\n",
        "    print(f\"   u = 3({x_teste})¬≤ + 2({x_teste}) + 1 = {u}\")\n",
        "    print(f\"   y = ({u})‚Å¥ = {y}\")\n",
        "    print()\n",
        "    print(f\"Derivadas:\")\n",
        "    print(f\"   dy/du = 4u¬≥ = 4({u})¬≥ = {dy_du}\")\n",
        "    print(f\"   du/dx = 6x + 2 = 6({x_teste}) + 2 = {du_dx}\")\n",
        "    print(f\"   dy/dx = {dy_du} √ó {du_dx} = {derivada}\")\n",
        "    \n",
        "    # Verifica√ß√£o num√©rica\n",
        "    epsilon = 1e-7\n",
        "    y1, _ = funcao_composta(x_teste + epsilon)\n",
        "    y2, _ = funcao_composta(x_teste - epsilon)\n",
        "    derivada_numerica = (y1 - y2) / (2 * epsilon)\n",
        "    \n",
        "    print()\n",
        "    print(f\"‚úÖ Verifica√ß√£o:\")\n",
        "    print(f\"   Derivada anal√≠tica: {derivada}\")\n",
        "    print(f\"   Derivada num√©rica:  {derivada_numerica:.0f}\")\n",
        "    print(f\"   Diferen√ßa: {abs(derivada - derivada_numerica):.2e}\")\n",
        "    \n",
        "    return derivada == int(derivada_numerica)\n",
        "\n",
        "sucesso = exercicio_1()\n",
        "print(f\"\\n{'üéâ PARAB√âNS! Voc√™ acertou!' if sucesso else '‚ùå Ops! Verifique seus c√°lculos.'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ Exerc√≠cio Pr√°tico 2: Backprop em Mini-Rede\n\nAgora vamos implementar o backpropagation em uma rede neural min√∫scula!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 2: Implemente o backpropagation nesta mini-rede\n",
        "\n",
        "class MiniRede:\n",
        "    \"\"\"Rede neural min√∫scula: 1 entrada ‚Üí 1 neur√¥nio oculto ‚Üí 1 sa√≠da\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Pesos fixos para o exerc√≠cio\n",
        "        self.w1 = 0.5   # Peso entrada ‚Üí oculto\n",
        "        self.b1 = 0.2   # Bias oculto\n",
        "        self.w2 = 0.8   # Peso oculto ‚Üí sa√≠da\n",
        "        self.b2 = 0.1   # Bias sa√≠da\n",
        "        \n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        # Camada oculta\n",
        "        self.z1 = self.w1 * x + self.b1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "        \n",
        "        # Camada de sa√≠da\n",
        "        self.z2 = self.w2 * self.a1 + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        \n",
        "        # Guardar entrada para backward\n",
        "        self.x = x\n",
        "        \n",
        "        return self.a2\n",
        "    \n",
        "    def backward_exercicio(self, y_true):\n",
        "        \"\"\"COMPLETE ESTA FUN√á√ÉO! Calcule os gradientes usando regra da cadeia\"\"\"\n",
        "        \n",
        "        # PASSO 1: Gradiente da fun√ß√£o de custo\n",
        "        # L = 0.5 * (a2 - y_true)¬≤\n",
        "        # dL/da2 = ?\n",
        "        dL_da2 = self.a2 - y_true  # COMPLETE!\n",
        "        \n",
        "        # PASSO 2: Gradientes da camada de sa√≠da\n",
        "        # da2/dz2 = a2 * (1 - a2)  [derivada sigmoid]\n",
        "        da2_dz2 = self.a2 * (1 - self.a2)  # COMPLETE!\n",
        "        \n",
        "        # dL/dz2 = (dL/da2) √ó (da2/dz2)  [regra da cadeia!]\n",
        "        dL_dz2 = dL_da2 * da2_dz2  # COMPLETE!\n",
        "        \n",
        "        # dL/dw2 = (dL/dz2) √ó (dz2/dw2) = (dL/dz2) √ó a1\n",
        "        dL_dw2 = dL_dz2 * self.a1  # COMPLETE!\n",
        "        \n",
        "        # dL/db2 = (dL/dz2) √ó (dz2/db2) = (dL/dz2) √ó 1\n",
        "        dL_db2 = dL_dz2 * 1  # COMPLETE!\n",
        "        \n",
        "        # PASSO 3: Gradientes da camada oculta\n",
        "        # dL/da1 = (dL/dz2) √ó (dz2/da1) = (dL/dz2) √ó w2\n",
        "        dL_da1 = dL_dz2 * self.w2  # COMPLETE!\n",
        "        \n",
        "        # da1/dz1 = a1 * (1 - a1)  [derivada sigmoid]\n",
        "        da1_dz1 = self.a1 * (1 - self.a1)  # COMPLETE!\n",
        "        \n",
        "        # dL/dz1 = (dL/da1) √ó (da1/dz1)  [regra da cadeia!]\n",
        "        dL_dz1 = dL_da1 * da1_dz1  # COMPLETE!\n",
        "        \n",
        "        # dL/dw1 = (dL/dz1) √ó (dz1/dw1) = (dL/dz1) √ó x\n",
        "        dL_dw1 = dL_dz1 * self.x  # COMPLETE!\n",
        "        \n",
        "        # dL/db1 = (dL/dz1) √ó (dz1/db1) = (dL/dz1) √ó 1\n",
        "        dL_db1 = dL_dz1 * 1  # COMPLETE!\n",
        "        \n",
        "        return {\n",
        "            'dL_dw1': dL_dw1, 'dL_db1': dL_db1,\n",
        "            'dL_dw2': dL_dw2, 'dL_db2': dL_db2\n",
        "        }\n",
        "\n",
        "# Teste do exerc√≠cio\n",
        "def testar_exercicio_2():\n",
        "    rede = MiniRede()\n",
        "    \n",
        "    # Dados de teste\n",
        "    x = 1.5\n",
        "    y_true = 0.9\n",
        "    \n",
        "    print(\"üéØ EXERC√çCIO 2: Backpropagation Manual\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Entrada: x = {x}\")\n",
        "    print(f\"Alvo: y_true = {y_true}\")\n",
        "    print()\n",
        "    \n",
        "    # Forward pass\n",
        "    y_pred = rede.forward(x)\n",
        "    erro = 0.5 * (y_pred - y_true)**2\n",
        "    \n",
        "    print(f\"üìä Forward Pass:\")\n",
        "    print(f\"   z1 = {rede.w1} * {x} + {rede.b1} = {rede.z1:.4f}\")\n",
        "    print(f\"   a1 = œÉ({rede.z1:.4f}) = {rede.a1:.4f}\")\n",
        "    print(f\"   z2 = {rede.w2} * {rede.a1:.4f} + {rede.b2} = {rede.z2:.4f}\")\n",
        "    print(f\"   a2 = œÉ({rede.z2:.4f}) = {rede.a2:.4f}\")\n",
        "    print(f\"   Erro = {erro:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # Backward pass\n",
        "    gradientes = rede.backward_exercicio(y_true)\n",
        "    \n",
        "    print(f\"üîÑ Backward Pass (seus resultados):\")\n",
        "    print(f\"   dL/dw1 = {gradientes['dL_dw1']:.6f}\")\n",
        "    print(f\"   dL/db1 = {gradientes['dL_db1']:.6f}\")\n",
        "    print(f\"   dL/dw2 = {gradientes['dL_dw2']:.6f}\")\n",
        "    print(f\"   dL/db2 = {gradientes['dL_db2']:.6f}\")\n",
        "    \n",
        "    return gradientes\n",
        "\n",
        "gradientes_exercicio = testar_exercicio_2()\n",
        "print(f\"\\n‚úÖ Exerc√≠cio conclu√≠do! Verifique se os gradientes fazem sentido.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üåü Parte 6: Por Que Isso √â T√ÉO Importante?\n\nAgora que voc√™ domina a regra da cadeia, vamos entender por que ela √© **FUNDAMENTAL** para toda a IA moderna!\n\n## üèóÔ∏è Sem Regra da Cadeia, N√£o Existe:\n\n- **ü§ñ Redes Neurais Profundas** (Deep Learning)\n- **üé® GANs** (Redes Generativas)\n- **üó£Ô∏è Transformers** (ChatGPT, BERT, etc.)\n- **üëÅÔ∏è Vis√£o Computacional** (CNNs)\n- **üìù Processamento de Linguagem Natural**\n\n## üîó A Conex√£o com os Pr√≥ximos M√≥dulos\n\nNos pr√≥ximos m√≥dulos, vamos ver como:\n- **Gradientes** (M√≥dulo 10) s√£o calculados usando regra da cadeia\n- **Gradient Descent** (M√≥dulos 11-12) usa esses gradientes para otimizar\n- **Fun√ß√µes multivari√°veis** (M√≥dulo 9) estendem essa ideia\n\n## üéØ Problemas Reais que a Regra da Cadeia Resolve\n\n1. **Vanishing Gradients**: Gradientes que \"desaparecem\" em redes profundas\n2. **Exploding Gradients**: Gradientes que \"explodem\" e desestabilizam o treinamento\n3. **Otimiza√ß√£o**: Como chegar no m√≠nimo global da fun√ß√£o de custo\n\n**üéØ Dica do Pedro:** A regra da cadeia √© como o DNA da IA - est√° presente em TUDO!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstra√ß√£o do problema do vanishing gradient\n",
        "def demonstrar_vanishing_gradient():\n",
        "    \"\"\"Mostra como gradientes podem \"desaparecer\" em redes profundas\"\"\"\n",
        "    \n",
        "    print(\"‚ö†Ô∏è  DEMONSTRA√á√ÉO: Vanishing Gradient Problem\")\n",
        "    print(\"=\" * 55)\n",
        "    print()\n",
        "    \n",
        "    # Simular uma rede com muitas camadas\n",
        "    num_camadas = [2, 5, 10, 15, 20]\n",
        "    gradientes_finais = []\n",
        "    \n",
        "    for n in num_camadas:\n",
        "        # Simular gradiente passando por n camadas\n",
        "        # Cada camada multiplica por ~0.25 (derivada sigmoid t√≠pica)\n",
        "        gradiente_inicial = 1.0\n",
        "        gradiente_final = gradiente_inicial\n",
        "        \n",
        "        for camada in range(n):\n",
        "            # Derivada sigmoid m√°xima √© 0.25\n",
        "            derivada_sigmoid = 0.25\n",
        "            gradiente_final *= derivada_sigmoid\n",
        "        \n",
        "        gradientes_finais.append(gradiente_final)\n",
        "        \n",
        "        print(f\"üî¢ {n:2d} camadas: gradiente = {gradiente_inicial:.1f} √ó (0.25)^{n} = {gradiente_final:.2e}\")\n",
        "    \n",
        "    print()\n",
        "    print(\"üìä CONCLUS√ÉO:\")\n",
        "    print(\"   ‚Ä¢ Com 2 camadas: gradiente ainda utiliz√°vel\")\n",
        "    print(\"   ‚Ä¢ Com 10+ camadas: gradiente praticamente ZERO!\")\n",
        "    print(\"   ‚Ä¢ As primeiras camadas param de aprender üò±\")\n",
        "    print()\n",
        "    print(\"üí° SOLU√á√ïES MODERNAS:\")\n",
        "    print(\"   ‚Ä¢ ReLU (em vez de sigmoid)\")\n",
        "    print(\"   ‚Ä¢ Batch Normalization\")\n",
        "    print(\"   ‚Ä¢ Residual Connections (ResNet)\")\n",
        "    print(\"   ‚Ä¢ LSTM/GRU para sequ√™ncias\")\n",
        "    \n",
        "    return num_camadas, gradientes_finais\n",
        "\n",
        "camadas, grads = demonstrar_vanishing_gradient()\n",
        "\n",
        "# Visualiza√ß√£o\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogy(camadas, grads, 'ro-', linewidth=3, markersize=8)\n",
        "plt.title('Vanishing Gradient Problem', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('N√∫mero de Camadas')\n",
        "plt.ylabel('Magnitude do Gradiente (escala log)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Adicionar zona de perigo\n",
        "plt.axhline(y=1e-10, color='red', linestyle='--', alpha=0.7)\n",
        "plt.text(12, 1e-9, 'Zona de Perigo\\n(gradiente muito pequeno)', \n",
        "         ha='center', va='center',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='red', alpha=0.2))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéØ Esta √© a import√¢ncia da regra da cadeia bem implementada!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìö Resumo: O Que Aprendemos Hoje\n\n## üéØ Conceitos Fundamentais\n\n1. **Fun√ß√£o Composta**: Uma fun√ß√£o dentro da outra ($y = f(g(x))$)\n\n2. **Regra da Cadeia**: $\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$\n\n3. **Backpropagation**: Aplica√ß√£o da regra da cadeia de tr√°s pra frente\n\n## üîó A F√≥rmula de Ouro\n\nPara uma rede neural:\n$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$$\n\nOnde:\n- $L$ = fun√ß√£o de custo (erro)\n- $a$ = ativa√ß√£o do neur√¥nio  \n- $z$ = soma ponderada\n- $w$ = peso da conex√£o\n\n## üöÄ Por Que Isso Importa\n\n- **üß† Permite que redes neurais aprendam**\n- **üìä Cada peso √© ajustado na dire√ß√£o correta**\n- **‚ö° Torna poss√≠vel treinar redes com milh√µes de par√¢metros**\n- **üéØ √â a base de TODA IA moderna**\n\n## üîÆ Conex√£o com os Pr√≥ximos M√≥dulos\n\n- **M√≥dulo 7-8**: Integrais (para entender probabilidades)\n- **M√≥dulo 9**: Fun√ß√µes multivari√°veis (m√∫ltiplas entradas)\n- **M√≥dulo 10**: Gradientes (vetores de derivadas parciais)\n- **M√≥dulo 11-12**: Gradient Descent (usando os gradientes para otimizar)\n\n## üéØ Dica Final do Pedro\n\n**A regra da cadeia √© como aprender a dirigir** - depois que voc√™ entende, fica autom√°tico. E igual dirigir, ela te leva a lugares incr√≠veis! üöóüí®\n\nAgora voc√™ tem a **chave secreta** do deep learning. Use com sabedoria! üóùÔ∏è‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéâ Parab√©ns! Voc√™ Dominou a Regra da Cadeia!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-06_img_02.png)\n\n---\n\n## üèÜ Certificado de Conclus√£o\n\n**Voc√™ agora possui o conhecimento mais importante do Deep Learning!**\n\n### ‚úÖ Habilidades Adquiridas:\n- ‚úÖ Compreens√£o profunda da regra da cadeia\n- ‚úÖ Implementa√ß√£o manual do backpropagation\n- ‚úÖ Visualiza√ß√£o do fluxo de gradientes\n- ‚úÖ Solu√ß√£o de problemas de vanishing gradients\n- ‚úÖ Base s√≥lida para redes neurais profundas\n\n### üöÄ Pr√≥ximos Passos:\n1. **M√≥dulo 7**: Integrais e probabilidade\n2. **M√≥dulo 8**: AUC e m√©tricas baseadas em integral\n3. **M√≥dulo 9**: Fun√ß√µes multivari√°veis\n4. **M√≥dulo 10**: Gradientes em m√∫ltiplas dimens√µes\n\n---\n\n**üéØ Lembre-se**: A regra da cadeia √© o cora√ß√£o pulsante de toda IA moderna. Voc√™ acabou de desbloquear o segredo do universe do machine learning!\n\n**Continue firme na jornada! O Pedro acredita em voc√™! üí™üî•**"
      ]
    }
  ]
}