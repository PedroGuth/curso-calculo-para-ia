{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔗 Regra da Cadeia: A Chave Secreta do Backpropagation\n\n## Módulo 6: O Notebook Mais Crítico de Todo o Curso!\n\n### Por Pedro Nunes Guth\n\n---\n\nSalve galera! Chegou a hora do **MOMENTO MAIS IMPORTANTE** do nosso curso de Cálculo para IA! 🚀\n\nTá, mas Pedro... por que esse notebook é TÃO crítico assim? Bom, imagina que você é um detetive tentando descobrir quem é o culpado por cada erro que sua rede neural comete. A **Regra da Cadeia** é sua lupa de investigação!\n\nÉ ela que permite que a nossa rede neural **aprenda de trás pra frente** (backpropagation), descobrindo exatamente quanto cada neurônio contribuiu pro erro final.\n\n**🎯 O que vamos ver hoje:**\n- Como funções compostas funcionam na prática\n- A matemática da regra da cadeia (sem drama!)\n- Aplicação direta no backpropagation\n- Implementação passo a passo\n- Porque isso é A ALMA de toda IA moderna\n\nBora mergulhar nessa! 🏊‍♂️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - As ferramentas do Pedro\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuração dos gráficos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"🔧 Ferramentas carregadas!\")\n",
        "print(\"📊 Gráficos configurados!\")\n",
        "print(\"🚀 Bora começar a brincadeira!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🧠 Parte 1: O Que Diabos é uma Função Composta?\n\nTá, antes de partir pro backpropagation, vamos entender o conceito básico. Lembra quando você era criança e brincava daqueles bonequinhos russos (matryoshka)? Uma boneca dentro da outra?\n\n**Função composta é exatamente isso!** Uma função dentro da outra.\n\n## 🎭 A Analogia da Fábrica de Brigadeiros\n\nImagina uma fábrica de brigadeiros com 3 etapas:\n1. **Estação 1**: Mistura os ingredientes → $u = g(x)$\n2. **Estação 2**: Aquece a mistura → $v = h(u)$ \n3. **Estação 3**: Enrola o brigadeiro → $y = f(v)$\n\nO resultado final é: $y = f(h(g(x)))$ - uma função composta!\n\n## 📐 Matematicamente Falando\n\nSe temos:\n- $y = f(u)$ onde $u = g(x)$\n- Então: $y = f(g(x))$ é uma **função composta**\n\nA **Regra da Cadeia** nos diz como derivar isso:\n\n$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n\n**🎯 Dica do Pedro:** Pensa como uma corrente - cada elo multiplica com o próximo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos ver a regra da cadeia na prática!\n",
        "# Exemplo: y = (2x + 1)³\n",
        "\n",
        "def funcao_composta_exemplo(x):\n",
        "    \"\"\"Função composta: y = (2x + 1)³\"\"\"\n",
        "    u = 2*x + 1  # Função interna: g(x) = 2x + 1\n",
        "    y = u**3     # Função externa: f(u) = u³\n",
        "    return y, u\n",
        "\n",
        "def derivada_pela_regra_da_cadeia(x):\n",
        "    \"\"\"Derivada usando regra da cadeia\"\"\"\n",
        "    u = 2*x + 1\n",
        "    \n",
        "    # dy/du = 3u²\n",
        "    dy_du = 3 * u**2\n",
        "    \n",
        "    # du/dx = 2\n",
        "    du_dx = 2\n",
        "    \n",
        "    # Regra da cadeia: dy/dx = (dy/du) × (du/dx)\n",
        "    dy_dx = dy_du * du_dx\n",
        "    \n",
        "    return dy_dx, dy_du, du_dx\n",
        "\n",
        "# Testando com x = 3\n",
        "x_teste = 3\n",
        "y, u = funcao_composta_exemplo(x_teste)\n",
        "derivada, dy_du, du_dx = derivada_pela_regra_da_cadeia(x_teste)\n",
        "\n",
        "print(f\"📊 Para x = {x_teste}:\")\n",
        "print(f\"   u = g(x) = 2({x_teste}) + 1 = {u}\")\n",
        "print(f\"   y = f(u) = ({u})³ = {y}\")\n",
        "print(f\"\")\n",
        "print(f\"🔗 Regra da Cadeia:\")\n",
        "print(f\"   dy/du = 3u² = 3({u})² = {dy_du}\")\n",
        "print(f\"   du/dx = 2\")\n",
        "print(f\"   dy/dx = {dy_du} × {du_dx} = {derivada}\")\n",
        "print(f\"\")\n",
        "print(f\"✅ Resultado: A taxa de variação é {derivada}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📈 Visualizando a Regra da Cadeia\n\nVamos ver graficamente como isso funciona! É sempre mais fácil entender vendo, né?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualização da função composta e sua derivada\n",
        "x = np.linspace(-2, 2, 400)\n",
        "\n",
        "# Função composta: y = (2x + 1)³\n",
        "y_composta = (2*x + 1)**3\n",
        "\n",
        "# Derivada pela regra da cadeia: dy/dx = 6(2x + 1)²\n",
        "derivada = 6 * (2*x + 1)**2\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gráfico da função\n",
        "ax1.plot(x, y_composta, 'b-', linewidth=3, label='y = (2x + 1)³')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_title('Função Composta', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "\n",
        "# Destacar um ponto específico\n",
        "x_ponto = 0.5\n",
        "y_ponto = (2*x_ponto + 1)**3\n",
        "ax1.plot(x_ponto, y_ponto, 'ro', markersize=8)\n",
        "ax1.annotate(f'({x_ponto}, {y_ponto:.1f})', \n",
        "             xy=(x_ponto, y_ponto), \n",
        "             xytext=(x_ponto+0.2, y_ponto+2),\n",
        "             arrowprops=dict(arrowstyle='->', color='red'))\n",
        "\n",
        "# Gráfico da derivada\n",
        "ax2.plot(x, derivada, 'r-', linewidth=3, label=\"dy/dx = 6(2x + 1)²\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('Derivada')\n",
        "ax2.set_title('Derivada (Inclinação)', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "\n",
        "# Destacar o mesmo ponto na derivada\n",
        "derivada_ponto = 6 * (2*x_ponto + 1)**2\n",
        "ax2.plot(x_ponto, derivada_ponto, 'ro', markersize=8)\n",
        "ax2.annotate(f'Inclinação = {derivada_ponto:.1f}', \n",
        "             xy=(x_ponto, derivada_ponto), \n",
        "             xytext=(x_ponto+0.2, derivada_ponto+5),\n",
        "             arrowprops=dict(arrowstyle='->', color='red'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"📊 Observe que:\")\n",
        "print(f\"   • No ponto x = {x_ponto}, a função vale {y_ponto:.1f}\")\n",
        "print(f\"   • A inclinação (derivada) nesse ponto é {derivada_ponto:.1f}\")\n",
        "print(f\"   • Isso significa que a função está crescendo rapidamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔗 Parte 2: A Matemática Descomplicada da Regra da Cadeia\n\nAgora que você já viu na prática, vamos entender a matemática por trás. Relaxa, vai ser moleza!\n\n## 🎯 O Teorema da Regra da Cadeia\n\n**Versão Formal:** Se $y = f(u)$ e $u = g(x)$, então:\n\n$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n\n**Versão Pedro:** \"A derivada da função de fora vezes a derivada da função de dentro!\"\n\n## 🏗️ Construção Intuitiva\n\nImagina que você quer saber como $y$ muda quando $x$ muda um pouquinho:\n\n1. **Passo 1**: $x$ muda → isso afeta $u$ na taxa $\\frac{du}{dx}$\n2. **Passo 2**: $u$ muda → isso afeta $y$ na taxa $\\frac{dy}{du}$\n3. **Resultado**: O efeito total é o produto das duas taxas!\n\n## 🔄 Para Múltiplas Camadas\n\nSe temos $y = f(v)$, $v = h(u)$, $u = g(x)$:\n\n$$\\frac{dy}{dx} = \\frac{dy}{dv} \\cdot \\frac{dv}{du} \\cdot \\frac{du}{dx}$$\n\n**🎯 Dica do Pedro:** É como uma corrente - cada elo multiplica com o próximo, não importa quantos elos temos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementação da regra da cadeia para múltiplas camadas\n",
        "class CadeiaDerivativas:\n",
        "    \"\"\"Classe para demonstrar a regra da cadeia com múltiplas funções\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.historico = []\n",
        "    \n",
        "    def funcao_composta_complexa(self, x):\n",
        "        \"\"\"Exemplo: y = sin(e^(x²))\"\"\"\n",
        "        \n",
        "        # Camada 1: u = x²\n",
        "        u = x**2\n",
        "        \n",
        "        # Camada 2: v = e^u\n",
        "        v = np.exp(u)\n",
        "        \n",
        "        # Camada 3: y = sin(v)\n",
        "        y = np.sin(v)\n",
        "        \n",
        "        return y, v, u\n",
        "    \n",
        "    def calcular_derivadas(self, x):\n",
        "        \"\"\"Calcula as derivadas usando regra da cadeia\"\"\"\n",
        "        \n",
        "        y, v, u = self.funcao_composta_complexa(x)\n",
        "        \n",
        "        # Derivadas parciais\n",
        "        du_dx = 2*x              # d/dx(x²) = 2x\n",
        "        dv_du = np.exp(u)        # d/du(e^u) = e^u\n",
        "        dy_dv = np.cos(v)        # d/dv(sin(v)) = cos(v)\n",
        "        \n",
        "        # Regra da cadeia: dy/dx = (dy/dv) × (dv/du) × (du/dx)\n",
        "        dy_dx = dy_dv * dv_du * du_dx\n",
        "        \n",
        "        # Guardar para visualização\n",
        "        resultado = {\n",
        "            'x': x, 'u': u, 'v': v, 'y': y,\n",
        "            'du_dx': du_dx, 'dv_du': dv_du, 'dy_dv': dy_dv,\n",
        "            'dy_dx': dy_dx\n",
        "        }\n",
        "        \n",
        "        return resultado\n",
        "    \n",
        "    def mostrar_calculo(self, x):\n",
        "        \"\"\"Mostra o cálculo passo a passo\"\"\"\n",
        "        \n",
        "        resultado = self.calcular_derivadas(x)\n",
        "        \n",
        "        print(f\"🔢 Calculando para x = {x}\")\n",
        "        print(f\"\" + \"=\"*50)\n",
        "        print(f\"📍 Valores das funções:\")\n",
        "        print(f\"   u = x² = ({x})² = {resultado['u']:.4f}\")\n",
        "        print(f\"   v = e^u = e^{resultado['u']:.4f} = {resultado['v']:.4f}\")\n",
        "        print(f\"   y = sin(v) = sin({resultado['v']:.4f}) = {resultado['y']:.4f}\")\n",
        "        print(f\"\")\n",
        "        print(f\"🔗 Derivadas individuais:\")\n",
        "        print(f\"   du/dx = 2x = 2({x}) = {resultado['du_dx']:.4f}\")\n",
        "        print(f\"   dv/du = e^u = {resultado['dv_du']:.4f}\")\n",
        "        print(f\"   dy/dv = cos(v) = {resultado['dy_dv']:.4f}\")\n",
        "        print(f\"\")\n",
        "        print(f\"⚡ REGRA DA CADEIA:\")\n",
        "        print(f\"   dy/dx = (dy/dv) × (dv/du) × (du/dx)\")\n",
        "        print(f\"   dy/dx = {resultado['dy_dv']:.4f} × {resultado['dv_du']:.4f} × {resultado['du_dx']:.4f}\")\n",
        "        print(f\"   dy/dx = {resultado['dy_dx']:.4f}\")\n",
        "        \n",
        "        return resultado\n",
        "\n",
        "# Testando nossa classe\n",
        "cadeia = CadeiaDerivativas()\n",
        "resultado = cadeia.mostrar_calculo(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🧠 Parte 3: Entrando no Mundo das Redes Neurais\n\nAgora vem a parte **MAIS IMPORTANTE** do curso todo! Como a regra da cadeia funciona nas redes neurais?\n\n## 🏗️ Anatomia de um Neurônio\n\nUm neurônio faz basicamente isto:\n1. **Soma ponderada**: $z = w_1x_1 + w_2x_2 + ... + w_nx_n + b$\n2. **Função de ativação**: $a = \\sigma(z)$ (sigmoid, ReLU, etc.)\n\nIsso é uma **função composta**! $a = \\sigma(w \\cdot x + b)$\n\n## 🎯 O Problema do Aprendizado\n\nNossa rede comete erros. Queremos saber:\n- \"**Quanto cada peso $w$ contribuiu pro erro?**\"\n- \"**Em que direção devo ajustar cada peso?**\"\n\nA resposta está na derivada: $\\frac{\\partial \\text{Erro}}{\\partial w}$\n\n## 🔄 Backpropagation = Regra da Cadeia\n\nO backpropagation é **literalmente** a regra da cadeia aplicada de trás pra frente!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cálculo-para-ia-modulo-06_img_01.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um neurônio simples para ver a regra da cadeia em ação\n",
        "class NeuronioSimples:\n",
        "    \"\"\"Um neurônio para demonstrar backpropagation\"\"\"\n",
        "    \n",
        "    def __init__(self, pesos, bias):\n",
        "        self.w = np.array(pesos)  # Pesos\n",
        "        self.b = bias             # Bias\n",
        "        \n",
        "        # Guardar valores para backprop\n",
        "        self.x = None\n",
        "        self.z = None\n",
        "        self.a = None\n",
        "    \n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Função sigmoid: σ(z) = 1/(1 + e^(-z))\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "    \n",
        "    def sigmoid_derivada(self, z):\n",
        "        \"\"\"Derivada da sigmoid: σ'(z) = σ(z)(1 - σ(z))\"\"\"\n",
        "        s = self.sigmoid(z)\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass - calcular saída\"\"\"\n",
        "        self.x = np.array(x)\n",
        "        \n",
        "        # Soma ponderada: z = w·x + b\n",
        "        self.z = np.dot(self.w, self.x) + self.b\n",
        "        \n",
        "        # Ativação: a = σ(z)\n",
        "        self.a = self.sigmoid(self.z)\n",
        "        \n",
        "        return self.a\n",
        "    \n",
        "    def backward(self, erro_da_saida):\n",
        "        \"\"\"Backward pass - calcular gradientes usando REGRA DA CADEIA!\"\"\"\n",
        "        \n",
        "        # REGRA DA CADEIA em ação!\n",
        "        # dL/dw = (dL/da) × (da/dz) × (dz/dw)\n",
        "        \n",
        "        # 1. dL/da já temos (erro_da_saida)\n",
        "        dL_da = erro_da_saida\n",
        "        \n",
        "        # 2. da/dz = derivada da sigmoid\n",
        "        da_dz = self.sigmoid_derivada(self.z)\n",
        "        \n",
        "        # 3. dz/dw = x (derivada de w·x + b em relação a w)\n",
        "        dz_dw = self.x\n",
        "        \n",
        "        # 4. dz/db = 1 (derivada de w·x + b em relação a b)\n",
        "        dz_db = 1\n",
        "        \n",
        "        # APLICANDO A REGRA DA CADEIA!\n",
        "        dL_dw = dL_da * da_dz * dz_dw  # Gradiente dos pesos\n",
        "        dL_db = dL_da * da_dz * dz_db  # Gradiente do bias\n",
        "        \n",
        "        return dL_dw, dL_db, da_dz\n",
        "    \n",
        "    def mostrar_calculo(self, x, erro):\n",
        "        \"\"\"Mostra o cálculo completo\"\"\"\n",
        "        \n",
        "        print(\"🧠 NEURÔNIO EM AÇÃO!\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Forward pass\n",
        "        saida = self.forward(x)\n",
        "        print(f\"📊 FORWARD PASS:\")\n",
        "        print(f\"   Entrada: x = {self.x}\")\n",
        "        print(f\"   Pesos: w = {self.w}\")\n",
        "        print(f\"   Bias: b = {self.b}\")\n",
        "        print(f\"   Soma: z = w·x + b = {self.z:.4f}\")\n",
        "        print(f\"   Saída: a = σ(z) = {self.a:.4f}\")\n",
        "        print()\n",
        "        \n",
        "        # Backward pass\n",
        "        dL_dw, dL_db, da_dz = self.backward(erro)\n",
        "        print(f\"🔄 BACKWARD PASS (Regra da Cadeia):\")\n",
        "        print(f\"   Erro da saída: dL/da = {erro:.4f}\")\n",
        "        print(f\"   Derivada sigmoid: da/dz = {da_dz:.4f}\")\n",
        "        print(f\"   Derivada z vs w: dz/dw = x = {self.x}\")\n",
        "        print(f\"   \")\n",
        "        print(f\"   🔗 REGRA DA CADEIA:\")\n",
        "        print(f\"   dL/dw = (dL/da) × (da/dz) × (dz/dw)\")\n",
        "        print(f\"   dL/dw = {erro:.4f} × {da_dz:.4f} × {self.x} = {dL_dw}\")\n",
        "        print(f\"   dL/db = {erro:.4f} × {da_dz:.4f} × 1 = {dL_db:.4f}\")\n",
        "        \n",
        "        return dL_dw, dL_db\n",
        "\n",
        "# Testando nosso neurônio\n",
        "neuronio = NeuronioSimples(pesos=[0.5, -0.3], bias=0.1)\n",
        "x_entrada = [1.0, 2.0]\n",
        "erro_exemplo = 0.2  # Digamos que o erro seja 0.2\n",
        "\n",
        "gradientes_w, gradiente_b = neuronio.mostrar_calculo(x_entrada, erro_exemplo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📊 Visualizando o Backpropagation\n\nVamos ver como os gradientes fluem pela rede!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualização do fluxo de gradientes\n",
        "def visualizar_gradientes():\n",
        "    \"\"\"Cria uma visualização do fluxo de gradientes\"\"\"\n",
        "    \n",
        "    # Simulação de uma rede com 3 camadas\n",
        "    camadas = ['Entrada', 'Oculta 1', 'Oculta 2', 'Saída']\n",
        "    posicoes_x = [0, 1, 2, 3]\n",
        "    \n",
        "    # Simulação de gradientes (quanto maior, mais importante)\n",
        "    gradientes = [1.0, 0.8, 0.5, 0.2]  # Diminuem conforme voltamos\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "    \n",
        "    # Gráfico 1: Forward Pass\n",
        "    ax1.plot(posicoes_x, [1, 0.8, 0.6, 0.4], 'bo-', linewidth=3, markersize=10)\n",
        "    for i, (pos, camada) in enumerate(zip(posicoes_x, camadas)):\n",
        "        ax1.annotate(camada, (pos, [1, 0.8, 0.6, 0.4][i]), \n",
        "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "    \n",
        "    ax1.set_title('Forward Pass: Informação Indo Pra Frente', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Ativação')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_xlim(-0.5, 3.5)\n",
        "    \n",
        "    # Setas mostrando direção\n",
        "    for i in range(len(posicoes_x)-1):\n",
        "        ax1.annotate('', xy=(posicoes_x[i+1]-0.1, 0.2), xytext=(posicoes_x[i]+0.1, 0.2),\n",
        "                    arrowprops=dict(arrowstyle='->', lw=2, color='blue'))\n",
        "    \n",
        "    # Gráfico 2: Backward Pass\n",
        "    cores = plt.cm.Reds([0.3, 0.5, 0.7, 0.9])  # Cores baseadas na magnitude do gradiente\n",
        "    barras = ax2.bar(posicoes_x, gradientes, color=cores, alpha=0.7, width=0.6)\n",
        "    \n",
        "    # Adicionar valores nas barras\n",
        "    for i, (pos, grad, camada) in enumerate(zip(posicoes_x, gradientes, camadas)):\n",
        "        ax2.text(pos, grad + 0.05, f'{grad:.1f}', ha='center', fontweight='bold')\n",
        "        ax2.text(pos, -0.15, camada, ha='center', rotation=45)\n",
        "    \n",
        "    ax2.set_title('Backward Pass: Gradientes Voltando (Regra da Cadeia!)', fontsize=14, fontweight='bold')\n",
        "    ax2.set_ylabel('Magnitude do Gradiente')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_xlim(-0.5, 3.5)\n",
        "    ax2.set_ylim(-0.3, 1.2)\n",
        "    \n",
        "    # Setas mostrando direção reversa\n",
        "    for i in range(len(posicoes_x)-1, 0, -1):\n",
        "        ax2.annotate('', xy=(posicoes_x[i-1]+0.1, 0.05), xytext=(posicoes_x[i]-0.1, 0.05),\n",
        "                    arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"📊 Observe que:\")\n",
        "    print(\"   🔵 Forward: Informação flui da entrada para a saída\")\n",
        "    print(\"   🔴 Backward: Gradientes fluem da saída para a entrada\")\n",
        "    print(\"   📉 Gradientes diminuem conforme voltamos (problema do vanishing gradient!)\")\n",
        "    print(\"   🔗 Cada gradiente é calculado pela REGRA DA CADEIA!\")\n",
        "\n",
        "visualizar_gradientes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🌊 Parte 4: Backpropagation em uma Rede Completa\n\nAgora vamos implementar o backpropagation completo em uma rede neural simples!\n\n## 🏗️ Arquitetura da Nossa Rede\n\nVamos criar uma rede com:\n- **Entrada**: 2 neurônios\n- **Camada oculta**: 3 neurônios (sigmoid)\n- **Saída**: 1 neurônio (sigmoid)\n\n## 🎯 O Fluxo Completo\n\n1. **Forward**: $x \\rightarrow h \\rightarrow y$\n2. **Erro**: $L = \\frac{1}{2}(y - y_{target})^2$\n3. **Backward**: Regra da cadeia para cada peso!\n\n**🎯 Dica do Pedro:** É aqui que a mágica acontece! Cada peso recebe exatamente o gradiente que merece!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementação completa de uma rede neural com backpropagation\n",
        "class RedeNeuralSimples:\n",
        "    \"\"\"Rede neural simples para demonstrar backpropagation\"\"\"\n",
        "    \n",
        "    def __init__(self, entrada=2, oculta=3, saida=1):\n",
        "        # Inicialização aleatória dos pesos (pequenos valores)\n",
        "        self.W1 = np.random.randn(entrada, oculta) * 0.5    # Pesos entrada -> oculta\n",
        "        self.b1 = np.zeros((1, oculta))                     # Bias camada oculta\n",
        "        self.W2 = np.random.randn(oculta, saida) * 0.5      # Pesos oculta -> saída\n",
        "        self.b2 = np.zeros((1, saida))                      # Bias saída\n",
        "        \n",
        "        # Para guardar valores durante forward pass\n",
        "        self.z1 = None\n",
        "        self.a1 = None\n",
        "        self.z2 = None\n",
        "        self.a2 = None\n",
        "        self.X = None\n",
        "        \n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Função sigmoid\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "    \n",
        "    def sigmoid_derivada(self, z):\n",
        "        \"\"\"Derivada da sigmoid\"\"\"\n",
        "        s = self.sigmoid(z)\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward propagation\"\"\"\n",
        "        self.X = X\n",
        "        \n",
        "        # Camada oculta: z1 = X·W1 + b1, a1 = σ(z1)\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "        \n",
        "        # Camada de saída: z2 = a1·W2 + b2, a2 = σ(z2)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        \n",
        "        return self.a2\n",
        "    \n",
        "    def calcular_erro(self, y_true, y_pred):\n",
        "        \"\"\"Função de custo: Mean Squared Error\"\"\"\n",
        "        return 0.5 * np.mean((y_true - y_pred)**2)\n",
        "    \n",
        "    def backward(self, X, y_true):\n",
        "        \"\"\"Backward propagation usando REGRA DA CADEIA!\"\"\"\n",
        "        m = X.shape[0]  # Número de exemplos\n",
        "        \n",
        "        # PASSO 1: Gradiente da função de custo\n",
        "        # dL/da2 = (a2 - y_true)\n",
        "        dL_da2 = (self.a2 - y_true)\n",
        "        \n",
        "        # PASSO 2: Gradientes da camada de saída (REGRA DA CADEIA!)\n",
        "        # dL/dz2 = (dL/da2) × (da2/dz2)\n",
        "        da2_dz2 = self.sigmoid_derivada(self.z2)\n",
        "        dL_dz2 = dL_da2 * da2_dz2\n",
        "        \n",
        "        # dL/dW2 = (dL/dz2) × (dz2/dW2) = (dL/dz2) × a1\n",
        "        dL_dW2 = np.dot(self.a1.T, dL_dz2) / m\n",
        "        \n",
        "        # dL/db2 = (dL/dz2) × (dz2/db2) = (dL/dz2) × 1\n",
        "        dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True) / m\n",
        "        \n",
        "        # PASSO 3: Gradientes da camada oculta (REGRA DA CADEIA NOVAMENTE!)\n",
        "        # dL/da1 = (dL/dz2) × (dz2/da1) = (dL/dz2) × W2\n",
        "        dL_da1 = np.dot(dL_dz2, self.W2.T)\n",
        "        \n",
        "        # dL/dz1 = (dL/da1) × (da1/dz1)\n",
        "        da1_dz1 = self.sigmoid_derivada(self.z1)\n",
        "        dL_dz1 = dL_da1 * da1_dz1\n",
        "        \n",
        "        # dL/dW1 = (dL/dz1) × (dz1/dW1) = (dL/dz1) × X\n",
        "        dL_dW1 = np.dot(X.T, dL_dz1) / m\n",
        "        \n",
        "        # dL/db1 = (dL/dz1) × (dz1/db1) = (dL/dz1) × 1\n",
        "        dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True) / m\n",
        "        \n",
        "        gradientes = {\n",
        "            'dL_dW2': dL_dW2, 'dL_db2': dL_db2,\n",
        "            'dL_dW1': dL_dW1, 'dL_db1': dL_db1,\n",
        "            'dL_dz2': dL_dz2, 'dL_dz1': dL_dz1\n",
        "        }\n",
        "        \n",
        "        return gradientes\n",
        "    \n",
        "    def treinar_um_passo(self, X, y, taxa_aprendizado=0.1):\n",
        "        \"\"\"Um passo de treinamento\"\"\"\n",
        "        \n",
        "        # Forward pass\n",
        "        y_pred = self.forward(X)\n",
        "        \n",
        "        # Calcular erro\n",
        "        erro = self.calcular_erro(y, y_pred)\n",
        "        \n",
        "        # Backward pass\n",
        "        gradientes = self.backward(X, y)\n",
        "        \n",
        "        # Atualizar pesos (Gradient Descent)\n",
        "        self.W2 -= taxa_aprendizado * gradientes['dL_dW2']\n",
        "        self.b2 -= taxa_aprendizado * gradientes['dL_db2']\n",
        "        self.W1 -= taxa_aprendizado * gradientes['dL_dW1']\n",
        "        self.b1 -= taxa_aprendizado * gradientes['dL_db1']\n",
        "        \n",
        "        return erro, gradientes\n",
        "\n",
        "# Criando nossa rede\n",
        "rede = RedeNeuralSimples()\n",
        "\n",
        "# Dados de exemplo (problema XOR)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])  # XOR\n",
        "\n",
        "print(\"🧠 REDE NEURAL CRIADA!\")\n",
        "print(f\"📊 Dados de entrada: {X.shape}\")\n",
        "print(f\"📊 Dados de saída: {y.shape}\")\n",
        "print(f\"🔧 Pesos W1: {rede.W1.shape}\")\n",
        "print(f\"🔧 Pesos W2: {rede.W2.shape}\")\n",
        "print(f\"\")\n",
        "print(f\"🎯 Vamos treinar para resolver o problema XOR!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos treinar nossa rede e ver a regra da cadeia em ação!\n",
        "def treinar_e_visualizar(rede, X, y, epocas=1000):\n",
        "    \"\"\"Treina a rede e mostra a evolução\"\"\"\n",
        "    \n",
        "    historico_erro = []\n",
        "    historico_gradientes = []\n",
        "    \n",
        "    print(\"🚀 COMEÇANDO O TREINAMENTO!\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for epoca in range(epocas):\n",
        "        erro, gradientes = rede.treinar_um_passo(X, y, taxa_aprendizado=1.0)\n",
        "        \n",
        "        historico_erro.append(erro)\n",
        "        \n",
        "        # Guardar magnitude dos gradientes\n",
        "        mag_grad = np.mean(np.abs(gradientes['dL_dW2'])) + np.mean(np.abs(gradientes['dL_dW1']))\n",
        "        historico_gradientes.append(mag_grad)\n",
        "        \n",
        "        # Mostrar progresso\n",
        "        if epoca % 200 == 0:\n",
        "            y_pred = rede.forward(X)\n",
        "            print(f\"Época {epoca:4d}: Erro = {erro:.6f}, |Gradientes| = {mag_grad:.6f}\")\n",
        "            print(f\"             Predições: {y_pred.flatten()}\")\n",
        "    \n",
        "    return historico_erro, historico_gradientes\n",
        "\n",
        "# Treinar nossa rede\n",
        "historico_erro, historico_grad = treinar_e_visualizar(rede, X, y)\n",
        "\n",
        "print(\"\\n✅ TREINAMENTO CONCLUÍDO!\")\n",
        "print(\"\\n📊 RESULTADO FINAL:\")\n",
        "y_final = rede.forward(X)\n",
        "for i in range(len(X)):\n",
        "    print(f\"   Input: {X[i]} → Esperado: {y[i][0]} → Predição: {y_final[i][0]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualização da evolução do treinamento\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gráfico do erro\n",
        "ax1.plot(historico_erro, 'b-', linewidth=2)\n",
        "ax1.set_title('Evolução do Erro Durante o Treinamento', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Época')\n",
        "ax1.set_ylabel('Erro (MSE)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_yscale('log')\n",
        "\n",
        "# Adicionar anotações\n",
        "ax1.annotate('Erro diminuindo\\ngraças aos gradientes!', \n",
        "            xy=(500, historico_erro[500]), xytext=(300, historico_erro[100]),\n",
        "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
        "            fontsize=12, ha='center',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='yellow', alpha=0.7))\n",
        "\n",
        "# Gráfico dos gradientes\n",
        "ax2.plot(historico_grad, 'r-', linewidth=2, label='Magnitude dos Gradientes')\n",
        "ax2.set_title('Evolução dos Gradientes (Regra da Cadeia)', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Época')\n",
        "ax2.set_ylabel('Magnitude Média dos Gradientes')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "# Adicionar anotações\n",
        "ax2.annotate('Gradientes ficam menores\\nconforme convergimos', \n",
        "            xy=(800, historico_grad[800]), xytext=(400, max(historico_grad)*0.7),\n",
        "            arrowprops=dict(arrowstyle='->', color='blue', lw=2),\n",
        "            fontsize=12, ha='center',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue', alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"📊 ANÁLISE DOS GRÁFICOS:\")\n",
        "print(f\"   🔵 Erro final: {historico_erro[-1]:.6f}\")\n",
        "print(f\"   🔴 Gradiente final: {historico_grad[-1]:.6f}\")\n",
        "print(f\"   ✅ A rede aprendeu o XOR usando a REGRA DA CADEIA!\")\n",
        "print(f\"   🔗 Cada peso foi ajustado na direção correta graças ao backpropagation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎯 Parte 5: Exemplo Prático - Demonstração Detalhada\n\nVamos fazer um exemplo **SUPER DETALHADO** mostrando cada passo da regra da cadeia em uma iteração!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo super detalhado - Passo a passo da regra da cadeia\n",
        "def exemplo_detalhado_regra_cadeia():\n",
        "    \"\"\"Mostra EXATAMENTE como a regra da cadeia funciona\"\"\"\n",
        "    \n",
        "    print(\"🔍 EXEMPLO SUPER DETALHADO DA REGRA DA CADEIA\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    \n",
        "    # Rede simples: 2 entradas → 1 saída\n",
        "    # Função: y = σ(w1*x1 + w2*x2 + b)\n",
        "    \n",
        "    # Valores de exemplo\n",
        "    x1, x2 = 0.5, 1.0\n",
        "    w1, w2 = 0.3, -0.4\n",
        "    b = 0.1\n",
        "    y_target = 0.8  # Valor que queremos\n",
        "    \n",
        "    print(f\"📊 CONFIGURAÇÃO:\")\n",
        "    print(f\"   Entradas: x1 = {x1}, x2 = {x2}\")\n",
        "    print(f\"   Pesos: w1 = {w1}, w2 = {w2}\")\n",
        "    print(f\"   Bias: b = {b}\")\n",
        "    print(f\"   Alvo: y_target = {y_target}\")\n",
        "    print()\n",
        "    \n",
        "    # PASSO 1: Forward Pass\n",
        "    print(f\"🔄 PASSO 1: FORWARD PASS\")\n",
        "    z = w1*x1 + w2*x2 + b\n",
        "    y = 1 / (1 + np.exp(-z))  # sigmoid\n",
        "    \n",
        "    print(f\"   z = w1*x1 + w2*x2 + b\")\n",
        "    print(f\"   z = {w1}*{x1} + {w2}*{x2} + {b} = {z}\")\n",
        "    print(f\"   y = σ(z) = σ({z}) = {y:.4f}\")\n",
        "    print()\n",
        "    \n",
        "    # PASSO 2: Calcular Erro\n",
        "    print(f\"📊 PASSO 2: CALCULAR ERRO\")\n",
        "    erro = 0.5 * (y - y_target)**2\n",
        "    print(f\"   L = 0.5 * (y - y_target)²\")\n",
        "    print(f\"   L = 0.5 * ({y:.4f} - {y_target})² = {erro:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # PASSO 3: Backward Pass - REGRA DA CADEIA!\n",
        "    print(f\"🔗 PASSO 3: BACKWARD PASS (REGRA DA CADEIA!)\")\n",
        "    print(f\"\" + \"=\"*50)\n",
        "    \n",
        "    # Gradiente da função de custo\n",
        "    dL_dy = y - y_target\n",
        "    print(f\"1️⃣ Derivada do erro:\")\n",
        "    print(f\"   dL/dy = y - y_target = {y:.4f} - {y_target} = {dL_dy:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # Derivada da sigmoid\n",
        "    dy_dz = y * (1 - y)\n",
        "    print(f\"2️⃣ Derivada da sigmoid:\")\n",
        "    print(f\"   dy/dz = y(1-y) = {y:.4f} * (1-{y:.4f}) = {dy_dz:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # Derivadas das somas ponderadas\n",
        "    dz_dw1 = x1\n",
        "    dz_dw2 = x2\n",
        "    dz_db = 1\n",
        "    \n",
        "    print(f\"3️⃣ Derivadas da soma ponderada:\")\n",
        "    print(f\"   dz/dw1 = x1 = {dz_dw1}\")\n",
        "    print(f\"   dz/dw2 = x2 = {dz_dw2}\")\n",
        "    print(f\"   dz/db = 1 = {dz_db}\")\n",
        "    print()\n",
        "    \n",
        "    # APLICANDO A REGRA DA CADEIA!\n",
        "    print(f\"🎯 APLICANDO A REGRA DA CADEIA:\")\n",
        "    print(f\"\" + \"-\"*40)\n",
        "    \n",
        "    dL_dw1 = dL_dy * dy_dz * dz_dw1\n",
        "    dL_dw2 = dL_dy * dy_dz * dz_dw2\n",
        "    dL_db = dL_dy * dy_dz * dz_db\n",
        "    \n",
        "    print(f\"dL/dw1 = (dL/dy) × (dy/dz) × (dz/dw1)\")\n",
        "    print(f\"dL/dw1 = {dL_dy:.6f} × {dy_dz:.6f} × {dz_dw1} = {dL_dw1:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"dL/dw2 = (dL/dy) × (dy/dz) × (dz/dw2)\")\n",
        "    print(f\"dL/dw2 = {dL_dy:.6f} × {dy_dz:.6f} × {dz_dw2} = {dL_dw2:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"dL/db = (dL/dy) × (dy/dz) × (dz/db)\")\n",
        "    print(f\"dL/db = {dL_dy:.6f} × {dy_dz:.6f} × {dz_db} = {dL_db:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # INTERPRETAÇÃO\n",
        "    print(f\"🧠 INTERPRETAÇÃO:\")\n",
        "    print(f\"\" + \"=\"*30)\n",
        "    print(f\"   • dL/dw1 = {dL_dw1:.6f} {'(positivo → aumentar w1 aumenta erro)' if dL_dw1 > 0 else '(negativo → aumentar w1 diminui erro)'}\")\n",
        "    print(f\"   • dL/dw2 = {dL_dw2:.6f} {'(positivo → aumentar w2 aumenta erro)' if dL_dw2 > 0 else '(negativo → aumentar w2 diminui erro)'}\")\n",
        "    print(f\"   • dL/db = {dL_db:.6f} {'(positivo → aumentar b aumenta erro)' if dL_db > 0 else '(negativo → aumentar b diminui erro)'}\")\n",
        "    print()\n",
        "    \n",
        "    # Atualização dos pesos\n",
        "    taxa = 0.1\n",
        "    w1_novo = w1 - taxa * dL_dw1\n",
        "    w2_novo = w2 - taxa * dL_dw2\n",
        "    b_novo = b - taxa * dL_db\n",
        "    \n",
        "    print(f\"🔧 ATUALIZAÇÃO DOS PESOS (taxa = {taxa}):\")\n",
        "    print(f\"   w1_novo = {w1} - {taxa} * {dL_dw1:.6f} = {w1_novo:.6f}\")\n",
        "    print(f\"   w2_novo = {w2} - {taxa} * {dL_dw2:.6f} = {w2_novo:.6f}\")\n",
        "    print(f\"   b_novo = {b} - {taxa} * {dL_db:.6f} = {b_novo:.6f}\")\n",
        "    \n",
        "    return {\n",
        "        'gradientes': [dL_dw1, dL_dw2, dL_db],\n",
        "        'pesos_novos': [w1_novo, w2_novo, b_novo],\n",
        "        'erro': erro\n",
        "    }\n",
        "\n",
        "resultado = exemplo_detalhado_regra_cadeia()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 💡 Exercício Prático 1: Implementação Manual\n\nAgora é sua vez! Vamos implementar a regra da cadeia passo a passo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÍCIO 1: Implemente a regra da cadeia para esta função\n",
        "# Função: y = (3x² + 2x + 1)⁴\n",
        "\n",
        "def exercicio_1():\n",
        "    \"\"\"DESAFIO: Calcule dy/dx usando regra da cadeia\"\"\"\n",
        "    \n",
        "    print(\"🎯 EXERCÍCIO 1: Regra da Cadeia Manual\")\n",
        "    print(\"=\" * 45)\n",
        "    print(\"Função: y = (3x² + 2x + 1)⁴\")\n",
        "    print()\n",
        "    \n",
        "    def funcao_composta(x):\n",
        "        \"\"\"y = (3x² + 2x + 1)⁴\"\"\"\n",
        "        u = 3*x**2 + 2*x + 1  # Função interna\n",
        "        y = u**4               # Função externa\n",
        "        return y, u\n",
        "    \n",
        "    def derivada_manual(x):\n",
        "        \"\"\"COMPLETE ESTA FUNÇÃO!\"\"\"\n",
        "        \n",
        "        # Passo 1: Calcular u\n",
        "        u = 3*x**2 + 2*x + 1\n",
        "        \n",
        "        # Passo 2: Derivada da função externa (dy/du)\n",
        "        # DICA: se y = u⁴, então dy/du = ?\n",
        "        dy_du = 4 * u**3  # COMPLETE AQUI!\n",
        "        \n",
        "        # Passo 3: Derivada da função interna (du/dx)\n",
        "        # DICA: se u = 3x² + 2x + 1, então du/dx = ?\n",
        "        du_dx = 6*x + 2   # COMPLETE AQUI!\n",
        "        \n",
        "        # Passo 4: Regra da cadeia\n",
        "        dy_dx = dy_du * du_dx  # COMPLETE AQUI!\n",
        "        \n",
        "        return dy_dx, dy_du, du_dx, u\n",
        "    \n",
        "    # Teste com x = 2\n",
        "    x_teste = 2\n",
        "    y, u = funcao_composta(x_teste)\n",
        "    derivada, dy_du, du_dx, u_calc = derivada_manual(x_teste)\n",
        "    \n",
        "    print(f\"Para x = {x_teste}:\")\n",
        "    print(f\"   u = 3({x_teste})² + 2({x_teste}) + 1 = {u}\")\n",
        "    print(f\"   y = ({u})⁴ = {y}\")\n",
        "    print()\n",
        "    print(f\"Derivadas:\")\n",
        "    print(f\"   dy/du = 4u³ = 4({u})³ = {dy_du}\")\n",
        "    print(f\"   du/dx = 6x + 2 = 6({x_teste}) + 2 = {du_dx}\")\n",
        "    print(f\"   dy/dx = {dy_du} × {du_dx} = {derivada}\")\n",
        "    \n",
        "    # Verificação numérica\n",
        "    epsilon = 1e-7\n",
        "    y1, _ = funcao_composta(x_teste + epsilon)\n",
        "    y2, _ = funcao_composta(x_teste - epsilon)\n",
        "    derivada_numerica = (y1 - y2) / (2 * epsilon)\n",
        "    \n",
        "    print()\n",
        "    print(f\"✅ Verificação:\")\n",
        "    print(f\"   Derivada analítica: {derivada}\")\n",
        "    print(f\"   Derivada numérica:  {derivada_numerica:.0f}\")\n",
        "    print(f\"   Diferença: {abs(derivada - derivada_numerica):.2e}\")\n",
        "    \n",
        "    return derivada == int(derivada_numerica)\n",
        "\n",
        "sucesso = exercicio_1()\n",
        "print(f\"\\n{'🎉 PARABÉNS! Você acertou!' if sucesso else '❌ Ops! Verifique seus cálculos.'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🧪 Exercício Prático 2: Backprop em Mini-Rede\n\nAgora vamos implementar o backpropagation em uma rede neural minúscula!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÍCIO 2: Implemente o backpropagation nesta mini-rede\n",
        "\n",
        "class MiniRede:\n",
        "    \"\"\"Rede neural minúscula: 1 entrada → 1 neurônio oculto → 1 saída\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Pesos fixos para o exercício\n",
        "        self.w1 = 0.5   # Peso entrada → oculto\n",
        "        self.b1 = 0.2   # Bias oculto\n",
        "        self.w2 = 0.8   # Peso oculto → saída\n",
        "        self.b2 = 0.1   # Bias saída\n",
        "        \n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        # Camada oculta\n",
        "        self.z1 = self.w1 * x + self.b1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "        \n",
        "        # Camada de saída\n",
        "        self.z2 = self.w2 * self.a1 + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        \n",
        "        # Guardar entrada para backward\n",
        "        self.x = x\n",
        "        \n",
        "        return self.a2\n",
        "    \n",
        "    def backward_exercicio(self, y_true):\n",
        "        \"\"\"COMPLETE ESTA FUNÇÃO! Calcule os gradientes usando regra da cadeia\"\"\"\n",
        "        \n",
        "        # PASSO 1: Gradiente da função de custo\n",
        "        # L = 0.5 * (a2 - y_true)²\n",
        "        # dL/da2 = ?\n",
        "        dL_da2 = self.a2 - y_true  # COMPLETE!\n",
        "        \n",
        "        # PASSO 2: Gradientes da camada de saída\n",
        "        # da2/dz2 = a2 * (1 - a2)  [derivada sigmoid]\n",
        "        da2_dz2 = self.a2 * (1 - self.a2)  # COMPLETE!\n",
        "        \n",
        "        # dL/dz2 = (dL/da2) × (da2/dz2)  [regra da cadeia!]\n",
        "        dL_dz2 = dL_da2 * da2_dz2  # COMPLETE!\n",
        "        \n",
        "        # dL/dw2 = (dL/dz2) × (dz2/dw2) = (dL/dz2) × a1\n",
        "        dL_dw2 = dL_dz2 * self.a1  # COMPLETE!\n",
        "        \n",
        "        # dL/db2 = (dL/dz2) × (dz2/db2) = (dL/dz2) × 1\n",
        "        dL_db2 = dL_dz2 * 1  # COMPLETE!\n",
        "        \n",
        "        # PASSO 3: Gradientes da camada oculta\n",
        "        # dL/da1 = (dL/dz2) × (dz2/da1) = (dL/dz2) × w2\n",
        "        dL_da1 = dL_dz2 * self.w2  # COMPLETE!\n",
        "        \n",
        "        # da1/dz1 = a1 * (1 - a1)  [derivada sigmoid]\n",
        "        da1_dz1 = self.a1 * (1 - self.a1)  # COMPLETE!\n",
        "        \n",
        "        # dL/dz1 = (dL/da1) × (da1/dz1)  [regra da cadeia!]\n",
        "        dL_dz1 = dL_da1 * da1_dz1  # COMPLETE!\n",
        "        \n",
        "        # dL/dw1 = (dL/dz1) × (dz1/dw1) = (dL/dz1) × x\n",
        "        dL_dw1 = dL_dz1 * self.x  # COMPLETE!\n",
        "        \n",
        "        # dL/db1 = (dL/dz1) × (dz1/db1) = (dL/dz1) × 1\n",
        "        dL_db1 = dL_dz1 * 1  # COMPLETE!\n",
        "        \n",
        "        return {\n",
        "            'dL_dw1': dL_dw1, 'dL_db1': dL_db1,\n",
        "            'dL_dw2': dL_dw2, 'dL_db2': dL_db2\n",
        "        }\n",
        "\n",
        "# Teste do exercício\n",
        "def testar_exercicio_2():\n",
        "    rede = MiniRede()\n",
        "    \n",
        "    # Dados de teste\n",
        "    x = 1.5\n",
        "    y_true = 0.9\n",
        "    \n",
        "    print(\"🎯 EXERCÍCIO 2: Backpropagation Manual\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Entrada: x = {x}\")\n",
        "    print(f\"Alvo: y_true = {y_true}\")\n",
        "    print()\n",
        "    \n",
        "    # Forward pass\n",
        "    y_pred = rede.forward(x)\n",
        "    erro = 0.5 * (y_pred - y_true)**2\n",
        "    \n",
        "    print(f\"📊 Forward Pass:\")\n",
        "    print(f\"   z1 = {rede.w1} * {x} + {rede.b1} = {rede.z1:.4f}\")\n",
        "    print(f\"   a1 = σ({rede.z1:.4f}) = {rede.a1:.4f}\")\n",
        "    print(f\"   z2 = {rede.w2} * {rede.a1:.4f} + {rede.b2} = {rede.z2:.4f}\")\n",
        "    print(f\"   a2 = σ({rede.z2:.4f}) = {rede.a2:.4f}\")\n",
        "    print(f\"   Erro = {erro:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # Backward pass\n",
        "    gradientes = rede.backward_exercicio(y_true)\n",
        "    \n",
        "    print(f\"🔄 Backward Pass (seus resultados):\")\n",
        "    print(f\"   dL/dw1 = {gradientes['dL_dw1']:.6f}\")\n",
        "    print(f\"   dL/db1 = {gradientes['dL_db1']:.6f}\")\n",
        "    print(f\"   dL/dw2 = {gradientes['dL_dw2']:.6f}\")\n",
        "    print(f\"   dL/db2 = {gradientes['dL_db2']:.6f}\")\n",
        "    \n",
        "    return gradientes\n",
        "\n",
        "gradientes_exercicio = testar_exercicio_2()\n",
        "print(f\"\\n✅ Exercício concluído! Verifique se os gradientes fazem sentido.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🌟 Parte 6: Por Que Isso É TÃO Importante?\n\nAgora que você domina a regra da cadeia, vamos entender por que ela é **FUNDAMENTAL** para toda a IA moderna!\n\n## 🏗️ Sem Regra da Cadeia, Não Existe:\n\n- **🤖 Redes Neurais Profundas** (Deep Learning)\n- **🎨 GANs** (Redes Generativas)\n- **🗣️ Transformers** (ChatGPT, BERT, etc.)\n- **👁️ Visão Computacional** (CNNs)\n- **📝 Processamento de Linguagem Natural**\n\n## 🔗 A Conexão com os Próximos Módulos\n\nNos próximos módulos, vamos ver como:\n- **Gradientes** (Módulo 10) são calculados usando regra da cadeia\n- **Gradient Descent** (Módulos 11-12) usa esses gradientes para otimizar\n- **Funções multivariáveis** (Módulo 9) estendem essa ideia\n\n## 🎯 Problemas Reais que a Regra da Cadeia Resolve\n\n1. **Vanishing Gradients**: Gradientes que \"desaparecem\" em redes profundas\n2. **Exploding Gradients**: Gradientes que \"explodem\" e desestabilizam o treinamento\n3. **Otimização**: Como chegar no mínimo global da função de custo\n\n**🎯 Dica do Pedro:** A regra da cadeia é como o DNA da IA - está presente em TUDO!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstração do problema do vanishing gradient\n",
        "def demonstrar_vanishing_gradient():\n",
        "    \"\"\"Mostra como gradientes podem \"desaparecer\" em redes profundas\"\"\"\n",
        "    \n",
        "    print(\"⚠️  DEMONSTRAÇÃO: Vanishing Gradient Problem\")\n",
        "    print(\"=\" * 55)\n",
        "    print()\n",
        "    \n",
        "    # Simular uma rede com muitas camadas\n",
        "    num_camadas = [2, 5, 10, 15, 20]\n",
        "    gradientes_finais = []\n",
        "    \n",
        "    for n in num_camadas:\n",
        "        # Simular gradiente passando por n camadas\n",
        "        # Cada camada multiplica por ~0.25 (derivada sigmoid típica)\n",
        "        gradiente_inicial = 1.0\n",
        "        gradiente_final = gradiente_inicial\n",
        "        \n",
        "        for camada in range(n):\n",
        "            # Derivada sigmoid máxima é 0.25\n",
        "            derivada_sigmoid = 0.25\n",
        "            gradiente_final *= derivada_sigmoid\n",
        "        \n",
        "        gradientes_finais.append(gradiente_final)\n",
        "        \n",
        "        print(f\"🔢 {n:2d} camadas: gradiente = {gradiente_inicial:.1f} × (0.25)^{n} = {gradiente_final:.2e}\")\n",
        "    \n",
        "    print()\n",
        "    print(\"📊 CONCLUSÃO:\")\n",
        "    print(\"   • Com 2 camadas: gradiente ainda utilizável\")\n",
        "    print(\"   • Com 10+ camadas: gradiente praticamente ZERO!\")\n",
        "    print(\"   • As primeiras camadas param de aprender 😱\")\n",
        "    print()\n",
        "    print(\"💡 SOLUÇÕES MODERNAS:\")\n",
        "    print(\"   • ReLU (em vez de sigmoid)\")\n",
        "    print(\"   • Batch Normalization\")\n",
        "    print(\"   • Residual Connections (ResNet)\")\n",
        "    print(\"   • LSTM/GRU para sequências\")\n",
        "    \n",
        "    return num_camadas, gradientes_finais\n",
        "\n",
        "camadas, grads = demonstrar_vanishing_gradient()\n",
        "\n",
        "# Visualização\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogy(camadas, grads, 'ro-', linewidth=3, markersize=8)\n",
        "plt.title('Vanishing Gradient Problem', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Número de Camadas')\n",
        "plt.ylabel('Magnitude do Gradiente (escala log)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Adicionar zona de perigo\n",
        "plt.axhline(y=1e-10, color='red', linestyle='--', alpha=0.7)\n",
        "plt.text(12, 1e-9, 'Zona de Perigo\\n(gradiente muito pequeno)', \n",
        "         ha='center', va='center',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='red', alpha=0.2))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n🎯 Esta é a importância da regra da cadeia bem implementada!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📚 Resumo: O Que Aprendemos Hoje\n\n## 🎯 Conceitos Fundamentais\n\n1. **Função Composta**: Uma função dentro da outra ($y = f(g(x))$)\n\n2. **Regra da Cadeia**: $\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$\n\n3. **Backpropagation**: Aplicação da regra da cadeia de trás pra frente\n\n## 🔗 A Fórmula de Ouro\n\nPara uma rede neural:\n$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$$\n\nOnde:\n- $L$ = função de custo (erro)\n- $a$ = ativação do neurônio  \n- $z$ = soma ponderada\n- $w$ = peso da conexão\n\n## 🚀 Por Que Isso Importa\n\n- **🧠 Permite que redes neurais aprendam**\n- **📊 Cada peso é ajustado na direção correta**\n- **⚡ Torna possível treinar redes com milhões de parâmetros**\n- **🎯 É a base de TODA IA moderna**\n\n## 🔮 Conexão com os Próximos Módulos\n\n- **Módulo 7-8**: Integrais (para entender probabilidades)\n- **Módulo 9**: Funções multivariáveis (múltiplas entradas)\n- **Módulo 10**: Gradientes (vetores de derivadas parciais)\n- **Módulo 11-12**: Gradient Descent (usando os gradientes para otimizar)\n\n## 🎯 Dica Final do Pedro\n\n**A regra da cadeia é como aprender a dirigir** - depois que você entende, fica automático. E igual dirigir, ela te leva a lugares incríveis! 🚗💨\n\nAgora você tem a **chave secreta** do deep learning. Use com sabedoria! 🗝️✨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎉 Parabéns! Você Dominou a Regra da Cadeia!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cálculo-para-ia-modulo-06_img_02.png)\n\n---\n\n## 🏆 Certificado de Conclusão\n\n**Você agora possui o conhecimento mais importante do Deep Learning!**\n\n### ✅ Habilidades Adquiridas:\n- ✅ Compreensão profunda da regra da cadeia\n- ✅ Implementação manual do backpropagation\n- ✅ Visualização do fluxo de gradientes\n- ✅ Solução de problemas de vanishing gradients\n- ✅ Base sólida para redes neurais profundas\n\n### 🚀 Próximos Passos:\n1. **Módulo 7**: Integrais e probabilidade\n2. **Módulo 8**: AUC e métricas baseadas em integral\n3. **Módulo 9**: Funções multivariáveis\n4. **Módulo 10**: Gradientes em múltiplas dimensões\n\n---\n\n**🎯 Lembre-se**: A regra da cadeia é o coração pulsante de toda IA moderna. Você acabou de desbloquear o segredo do universe do machine learning!\n\n**Continue firme na jornada! O Pedro acredita em você! 💪🔥**"
      ]
    }
  ]
}