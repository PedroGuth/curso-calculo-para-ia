{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "derivadas_2_regras_do_jogo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Derivadas 2: As Regras do Jogo\n## Dominando as Regras de Deriva√ß√£o para Fun√ß√µes de Perda\n\n**Por Pedro Nunes Guth** üìö\n\n---\n\nFala pessoal! Lembram do nosso **M√≥dulo 4** onde descobrimos que a derivada √© como a **inclina√ß√£o da montanha do erro** em cada ponto? Pois √©, agora chegou a hora de aprender as **regras do jogo**! \n\nT√°, mas o que s√£o essas regras? Imagine que voc√™ est√° jogando futebol, mas cada vez que a bola muda de posi√ß√£o, voc√™ precisa calcular a velocidade dela na m√£o... Seria um saco, n√©? Por isso existem as **regras de deriva√ß√£o** - elas s√£o como as regras do futebol que tornam o jogo poss√≠vel!\n\nHoje vamos dominar:\n- üî∏ **Regra da Soma**: Como derivar quando as fun√ß√µes est√£o \"somando for√ßas\"\n- üî∏ **Regra do Produto**: Quando duas fun√ß√µes est√£o \"trabalhando juntas\"\n- üî∏ **Regra do Quociente**: Quando uma fun√ß√£o est√° \"dividindo o trabalho\" com outra\n- üî∏ **Aplica√ß√µes em Fun√ß√µes de Perda**: O que realmente importa para IA!\n\nBora que vai ser **Liiindo!** üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Importando nossas ferramentas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize_scalar\n",
        "import sympy as sp\n",
        "from sympy import symbols, diff, simplify, expand\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√µes para gr√°ficos bonitos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"üéØ Setup completo! Vamos √†s regras de deriva√ß√£o!\")\n",
        "print(\"üìä Bibliotecas carregadas com sucesso\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Por Que Precisamos das Regras de Deriva√ß√£o?\n\nLembra da nossa **defini√ß√£o fundamental** do M√≥dulo 4?\n\n$$f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n\nT√°, mas imagina ter que calcular isso **na m√£o** toda vez que encontramos uma fun√ß√£o tipo:\n\n$$\\text{Loss}(w) = (w^2 + 3w) \\cdot \\sin(w) + \\frac{w^3}{w+1}$$\n\nSeria como ter que **reinventar a roda** toda vez que voc√™ quer andar de bicicleta! üö≤\n\n### A Analogia do Restaurante\n\nPensa assim: voc√™ tem um restaurante e quer saber como o **lucro total** varia. O lucro pode vir de:\n- **Soma**: Hamb√∫rguer + Batata frita (receitas se somam)\n- **Produto**: Pre√ßo √ó Quantidade vendida (receitas se multiplicam)\n- **Quociente**: Receita √∑ Custos (efici√™ncia)\n\nAs regras de deriva√ß√£o nos dizem como calcular a **taxa de varia√ß√£o** em cada caso!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-05_img_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìê Regra da Soma: \"Dividir para Conquistar\"\n\nA **regra da soma** √© moleza! Se voc√™ tem duas fun√ß√µes que est√£o se **somando** ou **subtraindo**, voc√™ deriva cada uma separadamente:\n\n### F√≥rmula Matem√°tica:\n$$\\text{Se } f(x) = g(x) + h(x)$$\n$$\\text{Ent√£o } f'(x) = g'(x) + h'(x)$$\n\n### A Intui√ß√£o:\n√â como medir a velocidade de dois carros numa estrada. Se eles est√£o na **mesma dire√ß√£o**, as velocidades se somam. Se est√£o em **dire√ß√µes opostas**, elas se subtraem!\n\n### **Dica do Pedro** üí°\nPensa na derivada como \"quanto cada pedacinho contribui para a mudan√ßa total\". Na soma, cada pedacinho contribui independentemente!\n\n### Exemplo Pr√°tico em IA:\nImagine uma fun√ß√£o de perda que tem duas componentes:\n$$\\text{Loss}(w) = \\text{MSE}(w) + \\text{Regulariza√ß√£o}(w)$$\n$$\\text{Loss}(w) = w^2 + 0.1w$$\n\nA derivada fica:\n$$\\text{Loss}'(w) = (w^2)' + (0.1w)' = 2w + 0.1$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando a Regra da Soma com SymPy\n",
        "print(\"üî∏ REGRA DA SOMA - Exemplos Pr√°ticos\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Definindo a vari√°vel simb√≥lica\n",
        "w = symbols('w')\n",
        "\n",
        "# Exemplo 1: Fun√ß√£o de perda simples\n",
        "f1 = w**2\n",
        "f2 = 3*w\n",
        "soma = f1 + f2\n",
        "\n",
        "print(f\"f1(w) = {f1}\")\n",
        "print(f\"f2(w) = {f2}\")\n",
        "print(f\"Soma: f(w) = {soma}\")\n",
        "print()\n",
        "\n",
        "# Calculando as derivadas\n",
        "df1_dw = diff(f1, w)\n",
        "df2_dw = diff(f2, w)\n",
        "dsoma_dw = diff(soma, w)\n",
        "\n",
        "print(\"üìä DERIVADAS:\")\n",
        "print(f\"f1'(w) = {df1_dw}\")\n",
        "print(f\"f2'(w) = {df2_dw}\")\n",
        "print(f\"(f1 + f2)'(w) = {dsoma_dw}\")\n",
        "print(f\"Regra da Soma: {df1_dw} + {df2_dw} = {df1_dw + df2_dw}\")\n",
        "print()\n",
        "\n",
        "# Verificando que s√£o iguais\n",
        "print(f\"‚úÖ Verifica√ß√£o: {dsoma_dw} = {df1_dw + df2_dw} -> {dsoma_dw == df1_dw + df2_dw}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a Regra da Soma\n",
        "w_vals = np.linspace(-3, 3, 100)\n",
        "\n",
        "# Convertendo para fun√ß√µes NumPy\n",
        "f1_func = lambda x: x**2\n",
        "f2_func = lambda x: 3*x\n",
        "soma_func = lambda x: x**2 + 3*x\n",
        "\n",
        "# Derivadas\n",
        "df1_func = lambda x: 2*x\n",
        "df2_func = lambda x: 3*np.ones_like(x)\n",
        "dsoma_func = lambda x: 2*x + 3\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico das fun√ß√µes originais\n",
        "ax1.plot(w_vals, f1_func(w_vals), 'b-', label='f1(w) = w¬≤', linewidth=2)\n",
        "ax1.plot(w_vals, f2_func(w_vals), 'r-', label='f2(w) = 3w', linewidth=2)\n",
        "ax1.plot(w_vals, soma_func(w_vals), 'g-', label='f1 + f2 = w¬≤ + 3w', linewidth=3)\n",
        "ax1.set_title('üî∏ Fun√ß√µes Originais')\n",
        "ax1.set_xlabel('w')\n",
        "ax1.set_ylabel('f(w)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico das derivadas\n",
        "ax2.plot(w_vals, df1_func(w_vals), 'b--', label=\"f1'(w) = 2w\", linewidth=2)\n",
        "ax2.plot(w_vals, df2_func(w_vals), 'r--', label=\"f2'(w) = 3\", linewidth=2)\n",
        "ax2.plot(w_vals, dsoma_func(w_vals), 'g-', label=\"(f1+f2)' = 2w + 3\", linewidth=3)\n",
        "ax2.set_title('üìä Derivadas - Regra da Soma')\n",
        "ax2.set_xlabel('w')\n",
        "ax2.set_ylabel(\"f'(w)\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Observa como a derivada da soma √© exatamente a soma das derivadas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Regra do Produto: \"A Dan√ßa das Fun√ß√µes\"\n\nAgora a coisa fica mais interessante! Quando duas fun√ß√µes est√£o se **multiplicando**, elas fazem uma \"dan√ßa\" especial.\n\n### F√≥rmula Matem√°tica:\n$$\\text{Se } f(x) = g(x) \\cdot h(x)$$\n$$\\text{Ent√£o } f'(x) = g'(x) \\cdot h(x) + g(x) \\cdot h'(x)$$\n\n### A Analogia da Dan√ßa üíÉüï∫\nImagine dois dan√ßarinos numa apresenta√ß√£o:\n- **Primeiro termo**: O primeiro dan√ßarino acelera (g'(x)) enquanto o segundo mant√©m o ritmo (h(x))\n- **Segundo termo**: O primeiro mant√©m o ritmo (g(x)) enquanto o segundo acelera (h'(x))\n- **Resultado**: A mudan√ßa total √© a soma dos dois efeitos!\n\n### Por que n√£o √© simplesmente g'(x) √ó h'(x)?\nPorque quando duas coisas est√£o se multiplicando, o **efeito total** depende de como **cada uma** afeta a **outra**. √â como o PIB de um pa√≠s: se a popula√ß√£o cresce E a renda per capita cresce, o efeito total n√£o √© s√≥ um dos dois!\n\n### **Dica do Pedro** üí°\nDecoreba o mantra: \"**Deriva o primeiro, mant√©m o segundo. Mant√©m o primeiro, deriva o segundo. Soma tudo!**\"\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-05_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando a Regra do Produto\n",
        "print(\"‚ö° REGRA DO PRODUTO - Exemplos Pr√°ticos\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Exemplo 1: Fun√ß√£o de perda com regulariza√ß√£o\n",
        "w = symbols('w')\n",
        "g = w**2  # Primeira fun√ß√£o\n",
        "h = w + 1  # Segunda fun√ß√£o\n",
        "produto = g * h\n",
        "\n",
        "print(f\"g(w) = {g}\")\n",
        "print(f\"h(w) = {h}\")\n",
        "print(f\"Produto: f(w) = g(w) √ó h(w) = {produto}\")\n",
        "print(f\"Expandido: f(w) = {expand(produto)}\")\n",
        "print()\n",
        "\n",
        "# Calculando as derivadas\n",
        "dg_dw = diff(g, w)\n",
        "dh_dw = diff(h, w)\n",
        "dproduto_dw = diff(produto, w)\n",
        "\n",
        "print(\"üìä DERIVADAS:\")\n",
        "print(f\"g'(w) = {dg_dw}\")\n",
        "print(f\"h'(w) = {dh_dw}\")\n",
        "print()\n",
        "\n",
        "# Aplicando a regra do produto manualmente\n",
        "regra_produto = dg_dw * h + g * dh_dw\n",
        "print(\"üî∏ REGRA DO PRODUTO:\")\n",
        "print(f\"f'(w) = g'(w)√óh(w) + g(w)√óh'(w)\")\n",
        "print(f\"f'(w) = ({dg_dw})√ó({h}) + ({g})√ó({dh_dw})\")\n",
        "print(f\"f'(w) = {regra_produto}\")\n",
        "print(f\"Simplificado: f'(w) = {simplify(regra_produto)}\")\n",
        "print()\n",
        "\n",
        "print(f\"‚úÖ Derivada direta: {dproduto_dw}\")\n",
        "print(f\"‚úÖ Pela regra do produto: {simplify(regra_produto)}\")\n",
        "print(f\"‚úÖ S√£o iguais? {simplify(dproduto_dw - regra_produto) == 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a Regra do Produto - Exemplo com Fun√ß√£o de Perda\n",
        "w_vals = np.linspace(-2, 2, 100)\n",
        "\n",
        "# Exemplo: Loss(w) = w¬≤ √ó (w + 1) - comum em regulariza√ß√£o\n",
        "g_func = lambda x: x**2\n",
        "h_func = lambda x: x + 1\n",
        "produto_func = lambda x: x**2 * (x + 1)\n",
        "\n",
        "# Derivadas\n",
        "dg_func = lambda x: 2*x\n",
        "dh_func = lambda x: np.ones_like(x)\n",
        "dproduto_func = lambda x: 3*x**2 + 2*x  # Resultado da regra do produto\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Fun√ß√µes originais\n",
        "ax1.plot(w_vals, g_func(w_vals), 'b-', label='g(w) = w¬≤', linewidth=2)\n",
        "ax1.plot(w_vals, h_func(w_vals), 'r-', label='h(w) = w + 1', linewidth=2)\n",
        "ax1.plot(w_vals, produto_func(w_vals), 'purple', label='f(w) = w¬≤(w+1)', linewidth=3)\n",
        "ax1.set_title('‚ö° Fun√ß√µes no Produto')\n",
        "ax1.set_xlabel('w')\n",
        "ax1.set_ylabel('f(w)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Derivadas e componentes da regra do produto\n",
        "termo1 = lambda x: 2*x * (x + 1)  # g'(w) √ó h(w)\n",
        "termo2 = lambda x: x**2 * 1       # g(w) √ó h'(w)\n",
        "\n",
        "ax2.plot(w_vals, termo1(w_vals), 'g--', label=\"g'(w)√óh(w) = 2w(w+1)\", linewidth=2)\n",
        "ax2.plot(w_vals, termo2(w_vals), 'orange', linestyle='--', label=\"g(w)√óh'(w) = w¬≤\", linewidth=2)\n",
        "ax2.plot(w_vals, dproduto_func(w_vals), 'purple', label=\"f'(w) = 3w¬≤ + 2w\", linewidth=3)\n",
        "ax2.set_title('üìä Regra do Produto - Componentes')\n",
        "ax2.set_xlabel('w')\n",
        "ax2.set_ylabel(\"f'(w)\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Veja como a derivada final (roxa) √© a SOMA dos dois termos da regra do produto!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé™ Regra do Quociente: \"A Gangorra Matem√°tica\"\n\nA regra do quociente √© como uma **gangorra no parquinho**! Quando o numerador sobe, o resultado tende a subir. Quando o denominador sobe, o resultado tende a descer.\n\n### F√≥rmula Matem√°tica:\n$$\\text{Se } f(x) = \\frac{g(x)}{h(x)}$$\n$$\\text{Ent√£o } f'(x) = \\frac{g'(x) \\cdot h(x) - g(x) \\cdot h'(x)}{[h(x)]^2}$$\n\n### A Analogia da Efici√™ncia üìä\nImagina que voc√™ quer medir a **efici√™ncia** do seu modelo:\n$$\\text{Efici√™ncia} = \\frac{\\text{Acur√°cia}}{\\text{Tempo de Treinamento}}$$\n\n- Se a **acur√°cia aumenta** (numerador ‚Üë), a efici√™ncia melhora\n- Se o **tempo aumenta** (denominador ‚Üë), a efici√™ncia piora\n- A **taxa de varia√ß√£o** da efici√™ncia depende de **ambos os efeitos**!\n\n### Por que tem aquele sinal de menos?\nPorque quando o **denominador cresce**, o valor da fra√ß√£o **diminui**! √â como uma gangorra: quando um lado sobe, o outro desce.\n\n### **Dica do Pedro** üí°\nLembra da f√≥rmula: **\"Deriva em cima, mant√©m embaixo MENOS mant√©m em cima, deriva embaixo. Tudo dividido pelo embaixo ao quadrado!\"** √â meio l√≠ngua-travada, mas funciona! üòÑ\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-05_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando a Regra do Quociente\n",
        "print(\"üé™ REGRA DO QUOCIENTE - Exemplos Pr√°ticos\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Exemplo: Fun√ß√£o de taxa de aprendizado adaptativa\n",
        "w = symbols('w')\n",
        "g = w**2 + 1  # Numerador\n",
        "h = w + 2     # Denominador\n",
        "quociente = g / h\n",
        "\n",
        "print(f\"g(w) = {g}  (numerador)\")\n",
        "print(f\"h(w) = {h}  (denominador)\")\n",
        "print(f\"Quociente: f(w) = g(w)/h(w) = {quociente}\")\n",
        "print()\n",
        "\n",
        "# Calculando as derivadas\n",
        "dg_dw = diff(g, w)\n",
        "dh_dw = diff(h, w)\n",
        "dquociente_dw = diff(quociente, w)\n",
        "\n",
        "print(\"üìä DERIVADAS INDIVIDUAIS:\")\n",
        "print(f\"g'(w) = {dg_dw}\")\n",
        "print(f\"h'(w) = {dh_dw}\")\n",
        "print()\n",
        "\n",
        "# Aplicando a regra do quociente manualmente\n",
        "numerador_regra = dg_dw * h - g * dh_dw\n",
        "denominador_regra = h**2\n",
        "regra_quociente = numerador_regra / denominador_regra\n",
        "\n",
        "print(\"üî∏ REGRA DO QUOCIENTE:\")\n",
        "print(f\"f'(w) = [g'(w)√óh(w) - g(w)√óh'(w)] / [h(w)]¬≤\")\n",
        "print(f\"Numerador: ({dg_dw})√ó({h}) - ({g})√ó({dh_dw})\")\n",
        "print(f\"Numerador: {numerador_regra}\")\n",
        "print(f\"Numerador simplificado: {simplify(numerador_regra)}\")\n",
        "print(f\"Denominador: ({h})¬≤ = {denominador_regra}\")\n",
        "print()\n",
        "print(f\"‚úÖ Derivada pela regra: {simplify(regra_quociente)}\")\n",
        "print(f\"‚úÖ Derivada direta: {simplify(dquociente_dw)}\")\n",
        "print(f\"‚úÖ S√£o iguais? {simplify(dquociente_dw - regra_quociente) == 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a Regra do Quociente\n",
        "w_vals = np.linspace(0.1, 3, 100)  # Evitando w = -2 que zera o denominador\n",
        "\n",
        "# Fun√ß√µes\n",
        "g_func = lambda x: x**2 + 1\n",
        "h_func = lambda x: x + 2\n",
        "quociente_func = lambda x: (x**2 + 1) / (x + 2)\n",
        "\n",
        "# Derivada do quociente: (2w(w+2) - (w¬≤+1)) / (w+2)¬≤\n",
        "dquociente_func = lambda x: (2*x*(x+2) - (x**2 + 1)) / (x + 2)**2\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico das fun√ß√µes\n",
        "ax1.plot(w_vals, g_func(w_vals), 'b-', label='g(w) = w¬≤ + 1', linewidth=2)\n",
        "ax1.plot(w_vals, h_func(w_vals), 'r-', label='h(w) = w + 2', linewidth=2)\n",
        "ax1.plot(w_vals, quociente_func(w_vals), 'green', label='f(w) = (w¬≤+1)/(w+2)', linewidth=3)\n",
        "ax1.set_title('üé™ Fun√ß√µes no Quociente')\n",
        "ax1.set_xlabel('w')\n",
        "ax1.set_ylabel('f(w)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico da derivada\n",
        "ax2.plot(w_vals, dquociente_func(w_vals), 'green', label=\"f'(w) - Regra do Quociente\", linewidth=3)\n",
        "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "ax2.set_title('üìä Derivada pela Regra do Quociente')\n",
        "ax2.set_xlabel('w')\n",
        "ax2.set_ylabel(\"f'(w)\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# An√°lise dos pontos cr√≠ticos\n",
        "print(\"üîç AN√ÅLISE:\")\n",
        "print(\"- Observe como a derivada muda de sinal\")\n",
        "print(\"- Pontos onde f'(w) = 0 s√£o m√≠nimos ou m√°ximos locais\")\n",
        "print(\"- Isso √© crucial para otimiza√ß√£o em IA!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßÆ Mermaid: Fluxograma das Regras de Deriva√ß√£o\n\n```mermaid\ngraph TD\n    A[Fun√ß√£o Composta f(x)] --> B{Que tipo de opera√ß√£o?}\n    B -->|Soma/Subtra√ß√£o| C[Regra da Soma]\n    B -->|Multiplica√ß√£o| D[Regra do Produto]\n    B -->|Divis√£o| E[Regra do Quociente]\n    \n    C --> C1[\"f'(x) = g'(x) + h'(x)\"]\n    D --> D1[\"f'(x) = g'(x)h(x) + g(x)h'(x)\"]\n    E --> E1[\"f'(x) = [g'(x)h(x) - g(x)h'(x)] / [h(x)]¬≤\"]\n    \n    C1 --> F[Aplicar no Gradiente Descendente]\n    D1 --> F\n    E1 --> F\n    \n    F --> G[Otimizar Fun√ß√£o de Perda]\n```\n\nEste fluxograma mostra como **decidir** qual regra usar e como todas levam ao mesmo objetivo: **calcular gradientes** para otimiza√ß√£o!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Aplica√ß√µes em Fun√ß√µes de Perda - O Que Realmente Importa!\n\nAgora vem a **parte boa**! Vamos ver como essas regras se aplicam em **fun√ß√µes de perda reais** que encontramos em IA.\n\n### Fun√ß√£o de Perda T√≠pica:\n$$L(w) = \\underbrace{\\frac{1}{2n}\\sum_{i=1}^{n}(y_i - w \\cdot x_i)^2}_{\\text{MSE}} + \\underbrace{\\lambda w^2}_{\\text{Regulariza√ß√£o L2}}$$\n\nVamos simplificar para uma vers√£o que podemos trabalhar:\n$$L(w) = \\underbrace{(y - wx)^2}_{\\text{Erro quadr√°tico}} + \\underbrace{\\lambda w^2}_{\\text{Regulariza√ß√£o}}$$\n\n### An√°lise das Regras:\n1. **Regra da Soma**: Entre o erro e a regulariza√ß√£o\n2. **Regra do Produto**: Dentro de $(y - wx)^2$\n3. **Regra da Cadeia** (pr√≥ximo m√≥dulo): Para o termo quadr√°tico\n\n### Por que isso importa?\nNo **Gradiente Descendente**, precisamos calcular $\\frac{\\partial L}{\\partial w}$ para saber **como ajustar** os pesos. Sem essas regras, seria imposs√≠vel!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo Real: Fun√ß√£o de Perda com Regulariza√ß√£o\n",
        "print(\"üéØ APLICA√á√ÉO EM FUN√á√ïES DE PERDA\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Definindo s√≠mbolos\n",
        "w, x, y, lam = symbols('w x y lambda')\n",
        "\n",
        "# Fun√ß√£o de perda: L(w) = (y - w*x)¬≤ + Œª*w¬≤\n",
        "erro_quadratico = (y - w*x)**2\n",
        "regularizacao = lam * w**2\n",
        "loss_total = erro_quadratico + regularizacao\n",
        "\n",
        "print(f\"üìä COMPONENTES DA FUN√á√ÉO DE PERDA:\")\n",
        "print(f\"Erro Quadr√°tico: {erro_quadratico}\")\n",
        "print(f\"Regulariza√ß√£o L2: {regularizacao}\")\n",
        "print(f\"Loss Total: L(w) = {loss_total}\")\n",
        "print()\n",
        "\n",
        "# Calculando a derivada (gradiente)\n",
        "dL_dw = diff(loss_total, w)\n",
        "print(f\"üî∏ GRADIENTE (derivada em rela√ß√£o a w):\")\n",
        "print(f\"dL/dw = {dL_dw}\")\n",
        "print(f\"Simplificado: dL/dw = {simplify(dL_dw)}\")\n",
        "print()\n",
        "\n",
        "# Analisando cada termo\n",
        "dErro_dw = diff(erro_quadratico, w)\n",
        "dReg_dw = diff(regularizacao, w)\n",
        "\n",
        "print(f\"üìà AN√ÅLISE POR COMPONENTES:\")\n",
        "print(f\"d/dw[(y-wx)¬≤] = {dErro_dw}\")\n",
        "print(f\"d/dw[Œªw¬≤] = {dReg_dw}\")\n",
        "print(f\"Total (Regra da Soma): {dErro_dw} + {dReg_dw}\")\n",
        "print()\n",
        "\n",
        "# Exemplo num√©rico\n",
        "print(f\"üßÆ EXEMPLO NUM√âRICO:\")\n",
        "print(f\"Se y=5, x=2, Œª=0.1, w=1.5:\")\n",
        "valores = {y: 5, x: 2, lam: 0.1, w: 1.5}\n",
        "loss_valor = loss_total.subs(valores)\n",
        "grad_valor = dL_dw.subs(valores)\n",
        "print(f\"L(1.5) = {float(loss_valor):.4f}\")\n",
        "print(f\"dL/dw(1.5) = {float(grad_valor):.4f}\")\n",
        "print(f\"Dire√ß√£o do ajuste: {'Diminuir w' if float(grad_valor) > 0 else 'Aumentar w'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a Fun√ß√£o de Perda e seu Gradiente\n",
        "# Usando valores espec√≠ficos: y=5, x=2, Œª=0.1\n",
        "y_val, x_val, lam_val = 5, 2, 0.1\n",
        "\n",
        "def loss_function(w):\n",
        "    \"\"\"Fun√ß√£o de perda: L(w) = (y - w*x)¬≤ + Œª*w¬≤\"\"\"\n",
        "    return (y_val - w * x_val)**2 + lam_val * w**2\n",
        "\n",
        "def gradient_function(w):\n",
        "    \"\"\"Gradiente: dL/dw = -2x(y - wx) + 2Œªw\"\"\"\n",
        "    return -2 * x_val * (y_val - w * x_val) + 2 * lam_val * w\n",
        "\n",
        "w_vals = np.linspace(0, 4, 100)\n",
        "loss_vals = [loss_function(w) for w in w_vals]\n",
        "grad_vals = [gradient_function(w) for w in w_vals]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico da fun√ß√£o de perda\n",
        "ax1.plot(w_vals, loss_vals, 'b-', linewidth=3, label='L(w) = (y-wx)¬≤ + Œªw¬≤')\n",
        "\n",
        "# Encontrar o m√≠nimo\n",
        "min_idx = np.argmin(loss_vals)\n",
        "w_min = w_vals[min_idx]\n",
        "loss_min = loss_vals[min_idx]\n",
        "\n",
        "ax1.plot(w_min, loss_min, 'ro', markersize=10, label=f'M√≠nimo em w‚âà{w_min:.2f}')\n",
        "ax1.set_title('üéØ Fun√ß√£o de Perda com Regulariza√ß√£o')\n",
        "ax1.set_xlabel('w (peso)')\n",
        "ax1.set_ylabel('L(w) (perda)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico do gradiente\n",
        "ax2.plot(w_vals, grad_vals, 'r-', linewidth=3, label=\"dL/dw\")\n",
        "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5, label='Gradiente = 0')\n",
        "ax2.plot(w_min, 0, 'ro', markersize=10, label='Ponto cr√≠tico')\n",
        "ax2.set_title('üìä Gradiente da Fun√ß√£o de Perda')\n",
        "ax2.set_xlabel('w (peso)')\n",
        "ax2.set_ylabel('dL/dw')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üîç INSIGHTS:\")\n",
        "print(f\"- O m√≠nimo ocorre onde o gradiente √© zero: w ‚âà {w_min:.3f}\")\n",
        "print(f\"- Quando dL/dw > 0: devemos DIMINUIR w\")\n",
        "print(f\"- Quando dL/dw < 0: devemos AUMENTAR w\")\n",
        "print(f\"- Isso √© exatamente o que o Gradiente Descendente faz! (M√≥dulo 11)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî• Exemplo Avan√ßado: Combinando Todas as Regras\n\nAgora vamos ver um exemplo que **combina** todas as regras que aprendemos! Imagina uma fun√ß√£o de perda mais sofisticada:\n\n$$L(w) = \\underbrace{w^2 + 3w}_{\\text{Soma}} \\times \\underbrace{\\sin(w)}_{\\text{Produto}} + \\underbrace{\\frac{w^3}{w+1}}_{\\text{Quociente}}$$\n\nEssa fun√ß√£o combina:\n- **Regra da Soma**: $w^2 + 3w$\n- **Regra do Produto**: $(w^2 + 3w) \\times \\sin(w)$\n- **Regra do Quociente**: $\\frac{w^3}{w+1}$\n\n### **Dica do Pedro** üí°\nNa vida real, as fun√ß√µes de perda podem ficar bem complexas, especialmente em redes neurais profundas. Mas os **princ√≠pios** s√£o sempre os mesmos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo Avan√ßado: Combinando Todas as Regras\n",
        "print(\"üî• EXEMPLO AVAN√áADO - Combinando Todas as Regras\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "w = symbols('w')\n",
        "\n",
        "# Definindo os componentes\n",
        "termo_soma = w**2 + 3*w\n",
        "termo_produto = termo_soma * sp.sin(w)  # (w¬≤ + 3w) √ó sin(w)\n",
        "termo_quociente = w**3 / (w + 1)       # w¬≥ / (w + 1)\n",
        "\n",
        "# Fun√ß√£o completa\n",
        "L_complexa = termo_produto + termo_quociente\n",
        "\n",
        "print(f\"üìä FUN√á√ÉO DE PERDA COMPLEXA:\")\n",
        "print(f\"Termo com Produto: (w¬≤ + 3w) √ó sin(w)\")\n",
        "print(f\"Termo com Quociente: w¬≥/(w+1)\")\n",
        "print(f\"L(w) = {L_complexa}\")\n",
        "print()\n",
        "\n",
        "# Calculando a derivada total\n",
        "dL_dw_complexa = diff(L_complexa, w)\n",
        "print(f\"üî∏ DERIVADA COMPLETA:\")\n",
        "print(f\"dL/dw = {dL_dw_complexa}\")\n",
        "print()\n",
        "\n",
        "# Analisando por partes\n",
        "print(f\"üìà AN√ÅLISE POR COMPONENTES:\")\n",
        "\n",
        "# Derivada do termo produto\n",
        "d_produto = diff(termo_produto, w)\n",
        "print(f\"d/dw[(w¬≤+3w)sin(w)] = {d_produto}\")\n",
        "print(f\"  (Usa regra da soma + regra do produto)\")\n",
        "print()\n",
        "\n",
        "# Derivada do termo quociente\n",
        "d_quociente = diff(termo_quociente, w)\n",
        "print(f\"d/dw[w¬≥/(w+1)] = {d_quociente}\")\n",
        "print(f\"  Simplificado: {simplify(d_quociente)}\")\n",
        "print(f\"  (Usa regra do quociente)\")\n",
        "print()\n",
        "\n",
        "# Verifica√ß√£o\n",
        "soma_componentes = d_produto + d_quociente\n",
        "print(f\"‚úÖ VERIFICA√á√ÉO (Regra da Soma):\")\n",
        "print(f\"dL/dw = d_produto + d_quociente\")\n",
        "print(f\"Simplificando ambos: {simplify(dL_dw_complexa - soma_componentes) == 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a Fun√ß√£o Complexa\n",
        "w_range = np.linspace(-0.5, 3, 1000)\n",
        "\n",
        "# Convertendo para fun√ß√µes NumPy (cuidado com w = -1)\n",
        "def L_complexa_func(w_vals):\n",
        "    # Evita divis√£o por zero\n",
        "    safe_vals = np.where(np.abs(w_vals + 1) < 1e-10, w_vals + 1e-10, w_vals)\n",
        "    termo1 = (safe_vals**2 + 3*safe_vals) * np.sin(safe_vals)\n",
        "    termo2 = safe_vals**3 / (safe_vals + 1)\n",
        "    return termo1 + termo2\n",
        "\n",
        "def dL_complexa_func(w_vals):\n",
        "    # Aproxima√ß√£o num√©rica da derivada\n",
        "    h = 1e-8\n",
        "    return (L_complexa_func(w_vals + h) - L_complexa_func(w_vals - h)) / (2*h)\n",
        "\n",
        "L_vals = L_complexa_func(w_range)\n",
        "dL_vals = dL_complexa_func(w_range)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# Fun√ß√£o de perda complexa\n",
        "ax1.plot(w_range, L_vals, 'purple', linewidth=3, label='L(w) - Fun√ß√£o Complexa')\n",
        "ax1.set_title('üî• Fun√ß√£o de Perda Complexa (Todas as Regras Combinadas)')\n",
        "ax1.set_xlabel('w')\n",
        "ax1.set_ylabel('L(w)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Derivada da fun√ß√£o complexa\n",
        "ax2.plot(w_range, dL_vals, 'red', linewidth=3, label=\"dL/dw\")\n",
        "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Identificando pontos cr√≠ticos (aproximados)\n",
        "zero_crossings = []\n",
        "for i in range(len(dL_vals)-1):\n",
        "    if dL_vals[i] * dL_vals[i+1] < 0:  # Mudan√ßa de sinal\n",
        "        zero_crossings.append(w_range[i])\n",
        "\n",
        "for wc in zero_crossings[:3]:  # Primeiros 3 pontos\n",
        "    ax2.plot(wc, 0, 'go', markersize=8)\n",
        "    ax1.plot(wc, L_complexa_func(np.array([wc]))[0], 'go', markersize=8)\n",
        "\n",
        "ax2.set_title('üìä Gradiente da Fun√ß√£o Complexa')\n",
        "ax2.set_xlabel('w')\n",
        "ax2.set_ylabel('dL/dw')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üéØ PONTOS CR√çTICOS ENCONTRADOS (aprox.): {len(zero_crossings)}\")\n",
        "print(f\"üìç Primeiros pontos cr√≠ticos: {zero_crossings[:3] if zero_crossings else 'Nenhum no intervalo'}\")\n",
        "print(f\"üîç Em fun√ß√µes complexas, pode haver m√∫ltiplos m√≠nimos locais!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí™ Exerc√≠cio 1: Praticando as Regras\n\nAgora √© sua vez! Vamos praticar com uma fun√ß√£o de perda inspirada em problemas reais de Machine Learning.\n\n### üéØ Desafio:\nCalcule a derivada da seguinte fun√ß√£o de perda usando as regras que aprendemos:\n\n$$L(w) = \\frac{w^2 + 2w}{w + 3} + (w^3 - w) \\times \\cos(w)$$\n\n### Passos:\n1. **Identifique** qual regra usar para cada termo\n2. **Calcule** a derivada de cada componente\n3. **Combine** usando a regra da soma\n4. **Verifique** seu resultado usando SymPy\n\n### **Dica do Pedro** üí°\nLembre-se: primeiro identifique a **estrutura** da fun√ß√£o (soma, produto, quociente), depois aplique as regras **de fora para dentro**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üí™ EXERC√çCIO 1 - Espa√ßo para sua solu√ß√£o\n",
        "print(\"üí™ EXERC√çCIO 1 - Calcule a derivada de L(w)\")\n",
        "print(\"L(w) = (w¬≤ + 2w)/(w + 3) + (w¬≥ - w) √ó cos(w)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Defina a fun√ß√£o aqui\n",
        "w = symbols('w')\n",
        "\n",
        "# COMPLETE AQUI - Defina cada termo da fun√ß√£o\n",
        "termo_quociente = # COMPLETE: (w^2 + 2w) / (w + 3)\n",
        "termo_produto =   # COMPLETE: (w^3 - w) * cos(w)\n",
        "L_exercicio =     # COMPLETE: soma dos termos\n",
        "\n",
        "print(f\"üìä SUA FUN√á√ÉO:\")\n",
        "print(f\"Termo 1 (quociente): {termo_quociente}\")\n",
        "print(f\"Termo 2 (produto): {termo_produto}\")\n",
        "print(f\"L(w) = {L_exercicio}\")\n",
        "print()\n",
        "\n",
        "# COMPLETE AQUI - Calcule as derivadas manualmente usando as regras\n",
        "print(f\"üî∏ APLICANDO AS REGRAS:\")\n",
        "\n",
        "# Para o quociente: d/dw[(w¬≤+2w)/(w+3)]\n",
        "print(\"Termo 1 - Regra do Quociente:\")\n",
        "# COMPLETE: aplique a regra do quociente\n",
        "\n",
        "# Para o produto: d/dw[(w¬≥-w)√ócos(w)]\n",
        "print(\"Termo 2 - Regra do Produto:\")\n",
        "# COMPLETE: aplique a regra do produto\n",
        "\n",
        "# Calculando com SymPy para verifica√ß√£o\n",
        "dL_exercicio = diff(L_exercicio, w)\n",
        "print(f\"\\n‚úÖ RESULTADO COM SYMPY:\")\n",
        "print(f\"dL/dw = {dL_exercicio}\")\n",
        "print(f\"Simplificado: {simplify(dL_exercicio)}\")\n",
        "\n",
        "# Descomente as linhas abaixo quando completar o exerc√≠cio\n",
        "# print(\"\\nüéØ Compare seu resultado com o SymPy!\")\n",
        "# print(\"Se chegou ao mesmo resultado, parab√©ns! üéâ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Exerc√≠cio 2: Aplica√ß√£o em Gradient Descent\n\nAgora vamos usar nossas regras de deriva√ß√£o numa aplica√ß√£o **real** de otimiza√ß√£o! \n\n### üéØ Cen√°rio:\nVoc√™ tem uma fun√ß√£o de perda simples para um problema de regress√£o:\n$$L(w) = (y - wx)^2 + 0.1w^2$$\n\nDados: $x = 3$, $y = 7$ (queremos encontrar $w$ que minimiza a perda)\n\n### Tarefa:\n1. **Calcule** $\\frac{dL}{dw}$ usando as regras de deriva√ß√£o\n2. **Implemente** um mini Gradient Descent\n3. **Visualize** como $w$ converge para o m√≠nimo\n\n### **Dica do Pedro** üí°\nLembra do **M√≥dulo 1**? Estamos literalmente \"descendo a montanha do erro\"! O gradiente nos diz a dire√ß√£o mais √≠ngreme!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ EXERC√çCIO 2 - Mini Gradient Descent\n",
        "print(\"üöÄ EXERC√çCIO 2 - Aplica√ß√£o em Gradient Descent\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Dados do problema\n",
        "x_data = 3\n",
        "y_data = 7\n",
        "lambda_reg = 0.1\n",
        "\n",
        "print(f\"üìä DADOS:\")\n",
        "print(f\"x = {x_data}, y = {y_data}, Œª = {lambda_reg}\")\n",
        "print(f\"Fun√ß√£o: L(w) = (y - wx)¬≤ + Œªw¬≤\")\n",
        "print(f\"Substituindo: L(w) = ({y_data} - {x_data}w)¬≤ + {lambda_reg}w¬≤\")\n",
        "print()\n",
        "\n",
        "# COMPLETE AQUI - Implemente as fun√ß√µes\n",
        "def loss_function(w):\n",
        "    \"\"\"Calcula L(w) = (y - wx)¬≤ + Œªw¬≤\"\"\"\n",
        "    # COMPLETE: retorne o valor da fun√ß√£o de perda\n",
        "    pass\n",
        "\n",
        "def gradient_function(w):\n",
        "    \"\"\"Calcula dL/dw usando as regras de deriva√ß√£o\"\"\"\n",
        "    # COMPLETE: calcule e retorne o gradiente\n",
        "    # Dica: use regra da soma + regra da cadeia para (y-wx)¬≤\n",
        "    # Resultado deve ser: -2x(y-wx) + 2Œªw\n",
        "    pass\n",
        "\n",
        "def gradient_descent(w_inicial, learning_rate, num_iterations):\n",
        "    \"\"\"Implementa o algoritmo de Gradient Descent\"\"\"\n",
        "    w = w_inicial\n",
        "    history = {'w': [w], 'loss': [loss_function(w)]}\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        # COMPLETE: calcule o gradiente\n",
        "        grad = # COMPLETE\n",
        "        \n",
        "        # COMPLETE: atualize w usando a regra: w = w - learning_rate * gradiente\n",
        "        w = # COMPLETE\n",
        "        \n",
        "        # Salva hist√≥rico\n",
        "        history['w'].append(w)\n",
        "        history['loss'].append(loss_function(w))\n",
        "    \n",
        "    return w, history\n",
        "\n",
        "# Testando (descomente quando completar)\n",
        "# w_inicial = 0.5\n",
        "# learning_rate = 0.01\n",
        "# num_iterations = 100\n",
        "\n",
        "# w_final, history = gradient_descent(w_inicial, learning_rate, num_iterations)\n",
        "\n",
        "# print(f\"üéØ RESULTADOS:\")\n",
        "# print(f\"w inicial: {w_inicial}\")\n",
        "# print(f\"w final: {w_final:.4f}\")\n",
        "# print(f\"Loss inicial: {history['loss'][0]:.4f}\")\n",
        "# print(f\"Loss final: {history['loss'][-1]:.4f}\")\n",
        "\n",
        "print(\"\\nüí° Complete as fun√ß√µes acima e descomente o teste!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Resumo: As Regras do Jogo Dominadas!\n\n**Liiindo!** Chegamos ao fim do nosso m√≥dulo sobre as **regras de deriva√ß√£o**! Vamos recapitular o que aprendemos:\n\n### üìö O Que Dominamos:\n\n#### 1. **Regra da Soma/Subtra√ß√£o** ‚ûï\n$$\\frac{d}{dx}[f(x) + g(x)] = f'(x) + g'(x)$$\n- **Analogia**: Como medir velocidades de carros na mesma estrada\n- **Aplica√ß√£o**: Separar componentes de loss (MSE + Regulariza√ß√£o)\n\n#### 2. **Regra do Produto** ‚úñÔ∏è\n$$\\frac{d}{dx}[f(x) \\cdot g(x)] = f'(x) \\cdot g(x) + f(x) \\cdot g'(x)$$\n- **Analogia**: A dan√ßa de dois dan√ßarinos\n- **Aplica√ß√£o**: Termos que se multiplicam nas fun√ß√µes de perda\n\n#### 3. **Regra do Quociente** ‚ûó\n$$\\frac{d}{dx}\\left[\\frac{f(x)}{g(x)}\\right] = \\frac{f'(x) \\cdot g(x) - f(x) \\cdot g'(x)}{[g(x)]^2}$$\n- **Analogia**: A gangorra matem√°tica\n- **Aplica√ß√£o**: M√©tricas de efici√™ncia e normaliza√ß√£o\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-05_img_04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÆ Conex√£o com os Pr√≥ximos M√≥dulos\n\n### **M√≥dulo 6 - Regra da Cadeia** üîó\nAs regras que aprendemos hoje s√£o **fundamentais**, mas ainda n√£o conseguimos derivar fun√ß√µes como $(3x^2 + 1)^{10}$ ou $\\sin(x^2)$. Para isso, precisamos da **Regra da Cadeia** - que √© o **cora√ß√£o** do backpropagation!\n\n### **M√≥dulos 11-12 - Gradiente Descendente** ‚¨áÔ∏è\nTodo o c√°lculo de derivadas que fizemos hoje ser√° usado para **otimizar** fun√ß√µes de perda reais. O gradiente que calculamos √© exatamente a \"**b√∫ssola**\" que nos guia na descida da montanha do erro!\n\n```mermaid\ngraph LR\n    A[M√≥dulo 5: Regras de Deriva√ß√£o] --> B[M√≥dulo 6: Regra da Cadeia]\n    B --> C[M√≥dulo 10: Gradientes]\n    C --> D[M√≥dulo 11: Gradient Descent]\n    D --> E[M√≥dulo 12: Otimizadores Avan√ßados]\n    \n    A -.-> F[Fun√ß√µes de Perda Complexas]\n    B -.-> G[Backpropagation]\n    C -.-> H[Redes Neurais]\n```\n\n### **Dica do Pedro** para o Pr√≥ximo M√≥dulo üí°\nPratique bastante as regras de hoje! No **M√≥dulo 6**, vamos combinar elas com a **Regra da Cadeia** para criar o algoritmo mais importante da IA moderna. Vai ser **√©pico**! üöÄ\n\n---\n\n**Bora** para o pr√≥ximo m√≥dulo! Nos vemos na **Regra da Cadeia** - onde a m√°gica do backpropagation acontece! üéØ‚ú®"
      ]
    }
  ]
}