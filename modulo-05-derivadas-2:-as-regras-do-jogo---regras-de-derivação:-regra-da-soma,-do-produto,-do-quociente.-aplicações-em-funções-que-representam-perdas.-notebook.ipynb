{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "derivadas_2_regras_do_jogo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¯ Derivadas 2: As Regras do Jogo\n## Dominando as Regras de DerivaÃ§Ã£o para FunÃ§Ãµes de Perda\n\n**Por Pedro Nunes Guth** ğŸ“š\n\n---\n\nFala pessoal! Lembram do nosso **MÃ³dulo 4** onde descobrimos que a derivada Ã© como a **inclinaÃ§Ã£o da montanha do erro** em cada ponto? Pois Ã©, agora chegou a hora de aprender as **regras do jogo**! \n\nTÃ¡, mas o que sÃ£o essas regras? Imagine que vocÃª estÃ¡ jogando futebol, mas cada vez que a bola muda de posiÃ§Ã£o, vocÃª precisa calcular a velocidade dela na mÃ£o... Seria um saco, nÃ©? Por isso existem as **regras de derivaÃ§Ã£o** - elas sÃ£o como as regras do futebol que tornam o jogo possÃ­vel!\n\nHoje vamos dominar:\n- ğŸ”¸ **Regra da Soma**: Como derivar quando as funÃ§Ãµes estÃ£o \"somando forÃ§as\"\n- ğŸ”¸ **Regra do Produto**: Quando duas funÃ§Ãµes estÃ£o \"trabalhando juntas\"\n- ğŸ”¸ **Regra do Quociente**: Quando uma funÃ§Ã£o estÃ¡ \"dividindo o trabalho\" com outra\n- ğŸ”¸ **AplicaÃ§Ãµes em FunÃ§Ãµes de Perda**: O que realmente importa para IA!\n\nBora que vai ser **Liiindo!** ğŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Importando nossas ferramentas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize_scalar\n",
        "import sympy as sp\n",
        "from sympy import symbols, diff, simplify, expand\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ConfiguraÃ§Ãµes para grÃ¡ficos bonitos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"ğŸ¯ Setup completo! Vamos Ã s regras de derivaÃ§Ã£o!\")\n",
        "print(\"ğŸ“Š Bibliotecas carregadas com sucesso\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§  Por Que Precisamos das Regras de DerivaÃ§Ã£o?\n\nLembra da nossa **definiÃ§Ã£o fundamental** do MÃ³dulo 4?\n\n$$f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n\nTÃ¡, mas imagina ter que calcular isso **na mÃ£o** toda vez que encontramos uma funÃ§Ã£o tipo:\n\n$$\\text{Loss}(w) = (w^2 + 3w) \\cdot \\sin(w) + \\frac{w^3}{w+1}$$\n\nSeria como ter que **reinventar a roda** toda vez que vocÃª quer andar de bicicleta! ğŸš²\n\n### A Analogia do Restaurante\n\nPensa assim: vocÃª tem um restaurante e quer saber como o **lucro total** varia. O lucro pode vir de:\n- **Soma**: HambÃºrguer + Batata frita (receitas se somam)\n- **Produto**: PreÃ§o Ã— Quantidade vendida (receitas se multiplicam)\n- **Quociente**: Receita Ã· Custos (eficiÃªncia)\n\nAs regras de derivaÃ§Ã£o nos dizem como calcular a **taxa de variaÃ§Ã£o** em cada caso!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cÃ¡lculo-para-ia-modulo-05_img_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ Regra da Soma: \"Dividir para Conquistar\"\n\nA **regra da soma** Ã© moleza! Se vocÃª tem duas funÃ§Ãµes que estÃ£o se **somando** ou **subtraindo**, vocÃª deriva cada uma separadamente:\n\n### FÃ³rmula MatemÃ¡tica:\n$$\\text{Se } f(x) = g(x) + h(x)$$\n$$\\text{EntÃ£o } f'(x) = g'(x) + h'(x)$$\n\n### A IntuiÃ§Ã£o:\nÃ‰ como medir a velocidade de dois carros numa estrada. Se eles estÃ£o na **mesma direÃ§Ã£o**, as velocidades se somam. Se estÃ£o em **direÃ§Ãµes opostas**, elas se subtraem!\n\n### **Dica do Pedro** ğŸ’¡\nPensa na derivada como \"quanto cada pedacinho contribui para a mudanÃ§a total\". Na soma, cada pedacinho contribui independentemente!\n\n### Exemplo PrÃ¡tico em IA:\nImagine uma funÃ§Ã£o de perda que tem duas componentes:\n$$\\text{Loss}(w) = \\text{MSE}(w) + \\text{RegularizaÃ§Ã£o}(w)$$\n$$\\text{Loss}(w) = w^2 + 0.1w$$\n\nA derivada fica:\n$$\\text{Loss}'(w) = (w^2)' + (0.1w)' = 2w + 0.1$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando a Regra da Soma com SymPy\n",
        "print(\"ğŸ”¸ REGRA DA SOMA - Exemplos PrÃ¡ticos\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Definindo a variÃ¡vel simbÃ³lica\n",
        "w = symbols('w')\n",
        "\n",
        "# Exemplo 1: FunÃ§Ã£o de perda simples\n",
        "f1 = w**2\n",
        "f2 = 3*w\n",
        "soma = f1 + f2\n",
        "\n",
        "print(f\"f1(w) = {f1}\")\n",
        "print(f\"f2(w) = {f2}\")\n",
        "print(f\"Soma: f(w) = {soma}\")\n",
        "print()\n",
        "\n",
        "# Calculando as derivadas\n",
        "df1_dw = diff(f1, w)\n",
        "df2_dw = diff(f2, w)\n",
        "dsoma_dw = diff(soma, w)\n",
        "\n",
        "print(\"ğŸ“Š DERIVADAS:\")\n",
        "print(f\"f1'(w) = {df1_dw}\")\n",
        "print(f\"f2'(w) = {df2_dw}\")\n",
        "print(f\"(f1 + f2)'(w) = {dsoma_dw}\")\n",
        "print(f\"Regra da Soma: {df1_dw} + {df2_dw} = {df1_dw + df2_dw}\")\n",
        "print()\n",
        "\n",
        "# Verificando que sÃ£o iguais\n",
        "print(f\"âœ… VerificaÃ§Ã£o: {dsoma_dw} = {df1_dw + df2_dw} -> {dsoma_dw == df1_dw + df2_dw}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a Regra da Soma\n",
        "w_vals = np.linspace(-3, 3, 100)\n",
        "\n",
        "# Convertendo para funÃ§Ãµes NumPy\n",
        "f1_func = lambda x: x**2\n",
        "f2_func = lambda x: 3*x\n",
        "soma_func = lambda x: x**2 + 3*x\n",
        "\n",
        "# Derivadas\n",
        "df1_func = lambda x: 2*x\n",
        "df2_func = lambda x: 3*np.ones_like(x)\n",
        "dsoma_func = lambda x: 2*x + 3\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# GrÃ¡fico das funÃ§Ãµes originais\n",
        "ax1.plot(w_vals, f1_func(w_vals), 'b-', label='f1(w) = wÂ²', linewidth=2)\n",
        "ax1.plot(w_vals, f2_func(w_vals), 'r-', label='f2(w) = 3w', linewidth=2)\n",
        "ax1.plot(w_vals, soma_func(w_vals), 'g-', label='f1 + f2 = wÂ² + 3w', linewidth=3)\n",
        "ax1.set_title('ğŸ”¸ FunÃ§Ãµes Originais')\n",
        "ax1.set_xlabel('w')\n",
        "ax1.set_ylabel('f(w)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# GrÃ¡fico das derivadas\n",
        "ax2.plot(w_vals, df1_func(w_vals), 'b--', label=\"f1'(w) = 2w\", linewidth=2)\n",
        "ax2.plot(w_vals, df2_func(w_vals), 'r--', label=\"f2'(w) = 3\", linewidth=2)\n",
        "ax2.plot(w_vals, dsoma_func(w_vals), 'g-', label=\"(f1+f2)' = 2w + 3\", linewidth=3)\n",
        "ax2.set_title('ğŸ“Š Derivadas - Regra da Soma')\n",
        "ax2.set_xlabel('w')\n",
        "ax2.set_ylabel(\"f'(w)\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¯ Observa como a derivada da soma Ã© exatamente a soma das derivadas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš¡ Regra do Produto: \"A DanÃ§a das FunÃ§Ãµes\"\n\nAgora a coisa fica mais interessante! Quando duas funÃ§Ãµes estÃ£o se **multiplicando**, elas fazem uma \"danÃ§a\" especial.\n\n### FÃ³rmula MatemÃ¡tica:\n$$\\text{Se } f(x) = g(x) \\cdot h(x)$$\n$$\\text{EntÃ£o } f'(x) = g'(x) \\cdot h(x) + g(x) \\cdot h'(x)$$\n\n### A Analogia da DanÃ§a ğŸ’ƒğŸ•º\nImagine dois danÃ§arinos numa apresentaÃ§Ã£o:\n- **Primeiro termo**: O primeiro danÃ§arino acelera (g'(x)) enquanto o segundo mantÃ©m o ritmo (h(x))\n- **Segundo termo**: O primeiro mantÃ©m o ritmo (g(x)) enquanto o segundo acelera (h'(x))\n- **Resultado**: A mudanÃ§a total Ã© a soma dos dois efeitos!\n\n### Por que nÃ£o Ã© simplesmente g'(x) Ã— h'(x)?\nPorque quando duas coisas estÃ£o se multiplicando, o **efeito total** depende de como **cada uma** afeta a **outra**. Ã‰ como o PIB de um paÃ­s: se a populaÃ§Ã£o cresce E a renda per capita cresce, o efeito total nÃ£o Ã© sÃ³ um dos dois!\n\n### **Dica do Pedro** ğŸ’¡\nDecoreba o mantra: \"**Deriva o primeiro, mantÃ©m o segundo. MantÃ©m o primeiro, deriva o segundo. Soma tudo!**\"\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cÃ¡lculo-para-ia-modulo-05_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando a Regra do Produto\n",
        "print(\"âš¡ REGRA DO PRODUTO - Exemplos PrÃ¡ticos\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Exemplo 1: FunÃ§Ã£o de perda com regularizaÃ§Ã£o\n",
        "w = symbols('w')\n",
        "g = w**2  # Primeira funÃ§Ã£o\n",
        "h = w + 1  # Segunda funÃ§Ã£o\n",
        "produto = g * h\n",
        "\n",
        "print(f\"g(w) = {g}\")\n",
        "print(f\"h(w) = {h}\")\n",
        "print(f\"Produto: f(w) = g(w) Ã— h(w) = {produto}\")\n",
        "print(f\"Expandido: f(w) = {expand(produto)}\")\n",
        "print()\n",
        "\n",
        "# Calculando as derivadas\n",
        "dg_dw = diff(g, w)\n",
        "dh_dw = diff(h, w)\n",
        "dproduto_dw = diff(produto, w)\n",
        "\n",
        "print(\"ğŸ“Š DERIVADAS:\")\n",
        "print(f\"g'(w) = {dg_dw}\")\n",
        "print(f\"h'(w) = {dh_dw}\")\n",
        "print()\n",
        "\n",
        "# Aplicando a regra do produto manualmente\n",
        "regra_produto = dg_dw * h + g * dh_dw\n",
        "print(\"ğŸ”¸ REGRA DO PRODUTO:\")\n",
        "print(f\"f'(w) = g'(w)Ã—h(w) + g(w)Ã—h'(w)\")\n",
        "print(f\"f'(w) = ({dg_dw})Ã—({h}) + ({g})Ã—({dh_dw})\")\n",
        "print(f\"f'(w) = {regra_produto}\")\n",
        "print(f\"Simplificado: f'(w) = {simplify(regra_produto)}\")\n",
        "print()\n",
        "\n",
        "print(f\"âœ… Derivada direta: {dproduto_dw}\")\n",
        "print(f\"âœ… Pela regra do produto: {simplify(regra_produto)}\")\n",
        "print(f\"âœ… SÃ£o iguais? {simplify(dproduto_dw - regra_produto) == 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a Regra do Produto - Exemplo com FunÃ§Ã£o de Perda\n",
        "w_vals = np.linspace(-2, 2, 100)\n",
        "\n",
        "# Exemplo: Loss(w) = wÂ² Ã— (w + 1) - comum em regularizaÃ§Ã£o\n",
        "g_func = lambda x: x**2\n",
        "h_func = lambda x: x + 1\n",
        "produto_func = lambda x: x**2 * (x + 1)\n",
        "\n",
        "# Derivadas\n",
        "dg_func = lambda x: 2*x\n",
        "dh_func = lambda x: np.ones_like(x)\n",
        "dproduto_func = lambda x: 3*x**2 + 2*x  # Resultado da regra do produto\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# FunÃ§Ãµes originais\n",
        "ax1.plot(w_vals, g_func(w_vals), 'b-', label='g(w) = wÂ²', linewidth=2)\n",
        "ax1.plot(w_vals, h_func(w_vals), 'r-', label='h(w) = w + 1', linewidth=2)\n",
        "ax1.plot(w_vals, produto_func(w_vals), 'purple', label='f(w) = wÂ²(w+1)', linewidth=3)\n",
        "ax1.set_title('âš¡ FunÃ§Ãµes no Produto')\n",
        "ax1.set_xlabel('w')\n",
        "ax1.set_ylabel('f(w)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Derivadas e componentes da regra do produto\n",
        "termo1 = lambda x: 2*x * (x + 1)  # g'(w) Ã— h(w)\n",
        "termo2 = lambda x: x**2 * 1       # g(w) Ã— h'(w)\n",
        "\n",
        "ax2.plot(w_vals, termo1(w_vals), 'g--', label=\"g'(w)Ã—h(w) = 2w(w+1)\", linewidth=2)\n",
        "ax2.plot(w_vals, termo2(w_vals), 'orange', linestyle='--', label=\"g(w)Ã—h'(w) = wÂ²\", linewidth=2)\n",
        "ax2.plot(w_vals, dproduto_func(w_vals), 'purple', label=\"f'(w) = 3wÂ² + 2w\", linewidth=3)\n",
        "ax2.set_title('ğŸ“Š Regra do Produto - Componentes')\n",
        "ax2.set_xlabel('w')\n",
        "ax2.set_ylabel(\"f'(w)\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¯ Veja como a derivada final (roxa) Ã© a SOMA dos dois termos da regra do produto!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸª Regra do Quociente: \"A Gangorra MatemÃ¡tica\"\n\nA regra do quociente Ã© como uma **gangorra no parquinho**! Quando o numerador sobe, o resultado tende a subir. Quando o denominador sobe, o resultado tende a descer.\n\n### FÃ³rmula MatemÃ¡tica:\n$$\\text{Se } f(x) = \\frac{g(x)}{h(x)}$$\n$$\\text{EntÃ£o } f'(x) = \\frac{g'(x) \\cdot h(x) - g(x) \\cdot h'(x)}{[h(x)]^2}$$\n\n### A Analogia da EficiÃªncia ğŸ“Š\nImagina que vocÃª quer medir a **eficiÃªncia** do seu modelo:\n$$\\text{EficiÃªncia} = \\frac{\\text{AcurÃ¡cia}}{\\text{Tempo de Treinamento}}$$\n\n- Se a **acurÃ¡cia aumenta** (numerador â†‘), a eficiÃªncia melhora\n- Se o **tempo aumenta** (denominador â†‘), a eficiÃªncia piora\n- A **taxa de variaÃ§Ã£o** da eficiÃªncia depende de **ambos os efeitos**!\n\n### Por que tem aquele sinal de menos?\nPorque quando o **denominador cresce**, o valor da fraÃ§Ã£o **diminui**! Ã‰ como uma gangorra: quando um lado sobe, o outro desce.\n\n### **Dica do Pedro** ğŸ’¡\nLembra da fÃ³rmula: **\"Deriva em cima, mantÃ©m embaixo MENOS mantÃ©m em cima, deriva embaixo. Tudo dividido pelo embaixo ao quadrado!\"** Ã‰ meio lÃ­ngua-travada, mas funciona! ğŸ˜„\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cÃ¡lculo-para-ia-modulo-05_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando a Regra do Quociente\n",
        "print(\"ğŸª REGRA DO QUOCIENTE - Exemplos PrÃ¡ticos\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Exemplo: FunÃ§Ã£o de taxa de aprendizado adaptativa\n",
        "w = symbols('w')\n",
        "g = w**2 + 1  # Numerador\n",
        "h = w + 2     # Denominador\n",
        "quociente = g / h\n",
        "\n",
        "print(f\"g(w) = {g}  (numerador)\")\n",
        "print(f\"h(w) = {h}  (denominador)\")\n",
        "print(f\"Quociente: f(w) = g(w)/h(w) = {quociente}\")\n",
        "print()\n",
        "\n",
        "# Calculando as derivadas\n",
        "dg_dw = diff(g, w)\n",
        "dh_dw = diff(h, w)\n",
        "dquociente_dw = diff(quociente, w)\n",
        "\n",
        "print(\"ğŸ“Š DERIVADAS INDIVIDUAIS:\")\n",
        "print(f\"g'(w) = {dg_dw}\")\n",
        "print(f\"h'(w) = {dh_dw}\")\n",
        "print()\n",
        "\n",
        "# Aplicando a regra do quociente manualmente\n",
        "numerador_regra = dg_dw * h - g * dh_dw\n",
        "denominador_regra = h**2\n",
        "regra_quociente = numerador_regra / denominador_regra\n",
        "\n",
        "print(\"ğŸ”¸ REGRA DO QUOCIENTE:\")\n",
        "print(f\"f'(w) = [g'(w)Ã—h(w) - g(w)Ã—h'(w)] / [h(w)]Â²\")\n",
        "print(f\"Numerador: ({dg_dw})Ã—({h}) - ({g})Ã—({dh_dw})\")\n",
        "print(f\"Numerador: {numerador_regra}\")\n",
        "print(f\"Numerador simplificado: {simplify(numerador_regra)}\")\n",
        "print(f\"Denominador: ({h})Â² = {denominador_regra}\")\n",
        "print()\n",
        "print(f\"âœ… Derivada pela regra: {simplify(regra_quociente)}\")\n",
        "print(f\"âœ… Derivada direta: {simplify(dquociente_dw)}\")\n",
        "print(f\"âœ… SÃ£o iguais? {simplify(dquociente_dw - regra_quociente) == 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a Regra do Quociente\n",
        "w_vals = np.linspace(0.1, 3, 100)  # Evitando w = -2 que zera o denominador\n",
        "\n",
        "# FunÃ§Ãµes\n",
        "g_func = lambda x: x**2 + 1\n",
        "h_func = lambda x: x + 2\n",
        "quociente_func = lambda x: (x**2 + 1) / (x + 2)\n",
        "\n",
        "# Derivada do quociente: (2w(w+2) - (wÂ²+1)) / (w+2)Â²\n",
        "dquociente_func = lambda x: (2*x*(x+2) - (x**2 + 1)) / (x + 2)**2\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# GrÃ¡fico das funÃ§Ãµes\n",
        "ax1.plot(w_vals, g_func(w_vals), 'b-', label='g(w) = wÂ² + 1', linewidth=2)\n",
        "ax1.plot(w_vals, h_func(w_vals), 'r-', label='h(w) = w + 2', linewidth=2)\n",
        "ax1.plot(w_vals, quociente_func(w_vals), 'green', label='f(w) = (wÂ²+1)/(w+2)', linewidth=3)\n",
        "ax1.set_title('ğŸª FunÃ§Ãµes no Quociente')\n",
        "ax1.set_xlabel('w')\n",
        "ax1.set_ylabel('f(w)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# GrÃ¡fico da derivada\n",
        "ax2.plot(w_vals, dquociente_func(w_vals), 'green', label=\"f'(w) - Regra do Quociente\", linewidth=3)\n",
        "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "ax2.set_title('ğŸ“Š Derivada pela Regra do Quociente')\n",
        "ax2.set_xlabel('w')\n",
        "ax2.set_ylabel(\"f'(w)\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# AnÃ¡lise dos pontos crÃ­ticos\n",
        "print(\"ğŸ” ANÃLISE:\")\n",
        "print(\"- Observe como a derivada muda de sinal\")\n",
        "print(\"- Pontos onde f'(w) = 0 sÃ£o mÃ­nimos ou mÃ¡ximos locais\")\n",
        "print(\"- Isso Ã© crucial para otimizaÃ§Ã£o em IA!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§® Mermaid: Fluxograma das Regras de DerivaÃ§Ã£o\n\n```mermaid\ngraph TD\n    A[FunÃ§Ã£o Composta f(x)] --> B{Que tipo de operaÃ§Ã£o?}\n    B -->|Soma/SubtraÃ§Ã£o| C[Regra da Soma]\n    B -->|MultiplicaÃ§Ã£o| D[Regra do Produto]\n    B -->|DivisÃ£o| E[Regra do Quociente]\n    \n    C --> C1[\"f'(x) = g'(x) + h'(x)\"]\n    D --> D1[\"f'(x) = g'(x)h(x) + g(x)h'(x)\"]\n    E --> E1[\"f'(x) = [g'(x)h(x) - g(x)h'(x)] / [h(x)]Â²\"]\n    \n    C1 --> F[Aplicar no Gradiente Descendente]\n    D1 --> F\n    E1 --> F\n    \n    F --> G[Otimizar FunÃ§Ã£o de Perda]\n```\n\nEste fluxograma mostra como **decidir** qual regra usar e como todas levam ao mesmo objetivo: **calcular gradientes** para otimizaÃ§Ã£o!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ AplicaÃ§Ãµes em FunÃ§Ãµes de Perda - O Que Realmente Importa!\n\nAgora vem a **parte boa**! Vamos ver como essas regras se aplicam em **funÃ§Ãµes de perda reais** que encontramos em IA.\n\n### FunÃ§Ã£o de Perda TÃ­pica:\n$$L(w) = \\underbrace{\\frac{1}{2n}\\sum_{i=1}^{n}(y_i - w \\cdot x_i)^2}_{\\text{MSE}} + \\underbrace{\\lambda w^2}_{\\text{RegularizaÃ§Ã£o L2}}$$\n\nVamos simplificar para uma versÃ£o que podemos trabalhar:\n$$L(w) = \\underbrace{(y - wx)^2}_{\\text{Erro quadrÃ¡tico}} + \\underbrace{\\lambda w^2}_{\\text{RegularizaÃ§Ã£o}}$$\n\n### AnÃ¡lise das Regras:\n1. **Regra da Soma**: Entre o erro e a regularizaÃ§Ã£o\n2. **Regra do Produto**: Dentro de $(y - wx)^2$\n3. **Regra da Cadeia** (prÃ³ximo mÃ³dulo): Para o termo quadrÃ¡tico\n\n### Por que isso importa?\nNo **Gradiente Descendente**, precisamos calcular $\\frac{\\partial L}{\\partial w}$ para saber **como ajustar** os pesos. Sem essas regras, seria impossÃ­vel!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo Real: FunÃ§Ã£o de Perda com RegularizaÃ§Ã£o\n",
        "print(\"ğŸ¯ APLICAÃ‡ÃƒO EM FUNÃ‡Ã•ES DE PERDA\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Definindo sÃ­mbolos\n",
        "w, x, y, lam = symbols('w x y lambda')\n",
        "\n",
        "# FunÃ§Ã£o de perda: L(w) = (y - w*x)Â² + Î»*wÂ²\n",
        "erro_quadratico = (y - w*x)**2\n",
        "regularizacao = lam * w**2\n",
        "loss_total = erro_quadratico + regularizacao\n",
        "\n",
        "print(f\"ğŸ“Š COMPONENTES DA FUNÃ‡ÃƒO DE PERDA:\")\n",
        "print(f\"Erro QuadrÃ¡tico: {erro_quadratico}\")\n",
        "print(f\"RegularizaÃ§Ã£o L2: {regularizacao}\")\n",
        "print(f\"Loss Total: L(w) = {loss_total}\")\n",
        "print()\n",
        "\n",
        "# Calculando a derivada (gradiente)\n",
        "dL_dw = diff(loss_total, w)\n",
        "print(f\"ğŸ”¸ GRADIENTE (derivada em relaÃ§Ã£o a w):\")\n",
        "print(f\"dL/dw = {dL_dw}\")\n",
        "print(f\"Simplificado: dL/dw = {simplify(dL_dw)}\")\n",
        "print()\n",
        "\n",
        "# Analisando cada termo\n",
        "dErro_dw = diff(erro_quadratico, w)\n",
        "dReg_dw = diff(regularizacao, w)\n",
        "\n",
        "print(f\"ğŸ“ˆ ANÃLISE POR COMPONENTES:\")\n",
        "print(f\"d/dw[(y-wx)Â²] = {dErro_dw}\")\n",
        "print(f\"d/dw[Î»wÂ²] = {dReg_dw}\")\n",
        "print(f\"Total (Regra da Soma): {dErro_dw} + {dReg_dw}\")\n",
        "print()\n",
        "\n",
        "# Exemplo numÃ©rico\n",
        "print(f\"ğŸ§® EXEMPLO NUMÃ‰RICO:\")\n",
        "print(f\"Se y=5, x=2, Î»=0.1, w=1.5:\")\n",
        "valores = {y: 5, x: 2, lam: 0.1, w: 1.5}\n",
        "loss_valor = loss_total.subs(valores)\n",
        "grad_valor = dL_dw.subs(valores)\n",
        "print(f\"L(1.5) = {float(loss_valor):.4f}\")\n",
        "print(f\"dL/dw(1.5) = {float(grad_valor):.4f}\")\n",
        "print(f\"DireÃ§Ã£o do ajuste: {'Diminuir w' if float(grad_valor) > 0 else 'Aumentar w'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a FunÃ§Ã£o de Perda e seu Gradiente\n",
        "# Usando valores especÃ­ficos: y=5, x=2, Î»=0.1\n",
        "y_val, x_val, lam_val = 5, 2, 0.1\n",
        "\n",
        "def loss_function(w):\n",
        "    \"\"\"FunÃ§Ã£o de perda: L(w) = (y - w*x)Â² + Î»*wÂ²\"\"\"\n",
        "    return (y_val - w * x_val)**2 + lam_val * w**2\n",
        "\n",
        "def gradient_function(w):\n",
        "    \"\"\"Gradiente: dL/dw = -2x(y - wx) + 2Î»w\"\"\"\n",
        "    return -2 * x_val * (y_val - w * x_val) + 2 * lam_val * w\n",
        "\n",
        "w_vals = np.linspace(0, 4, 100)\n",
        "loss_vals = [loss_function(w) for w in w_vals]\n",
        "grad_vals = [gradient_function(w) for w in w_vals]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# GrÃ¡fico da funÃ§Ã£o de perda\n",
        "ax1.plot(w_vals, loss_vals, 'b-', linewidth=3, label='L(w) = (y-wx)Â² + Î»wÂ²')\n",
        "\n",
        "# Encontrar o mÃ­nimo\n",
        "min_idx = np.argmin(loss_vals)\n",
        "w_min = w_vals[min_idx]\n",
        "loss_min = loss_vals[min_idx]\n",
        "\n",
        "ax1.plot(w_min, loss_min, 'ro', markersize=10, label=f'MÃ­nimo em wâ‰ˆ{w_min:.2f}')\n",
        "ax1.set_title('ğŸ¯ FunÃ§Ã£o de Perda com RegularizaÃ§Ã£o')\n",
        "ax1.set_xlabel('w (peso)')\n",
        "ax1.set_ylabel('L(w) (perda)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# GrÃ¡fico do gradiente\n",
        "ax2.plot(w_vals, grad_vals, 'r-', linewidth=3, label=\"dL/dw\")\n",
        "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5, label='Gradiente = 0')\n",
        "ax2.plot(w_min, 0, 'ro', markersize=10, label='Ponto crÃ­tico')\n",
        "ax2.set_title('ğŸ“Š Gradiente da FunÃ§Ã£o de Perda')\n",
        "ax2.set_xlabel('w (peso)')\n",
        "ax2.set_ylabel('dL/dw')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"ğŸ” INSIGHTS:\")\n",
        "print(f\"- O mÃ­nimo ocorre onde o gradiente Ã© zero: w â‰ˆ {w_min:.3f}\")\n",
        "print(f\"- Quando dL/dw > 0: devemos DIMINUIR w\")\n",
        "print(f\"- Quando dL/dw < 0: devemos AUMENTAR w\")\n",
        "print(f\"- Isso Ã© exatamente o que o Gradiente Descendente faz! (MÃ³dulo 11)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”¥ Exemplo AvanÃ§ado: Combinando Todas as Regras\n\nAgora vamos ver um exemplo que **combina** todas as regras que aprendemos! Imagina uma funÃ§Ã£o de perda mais sofisticada:\n\n$$L(w) = \\underbrace{w^2 + 3w}_{\\text{Soma}} \\times \\underbrace{\\sin(w)}_{\\text{Produto}} + \\underbrace{\\frac{w^3}{w+1}}_{\\text{Quociente}}$$\n\nEssa funÃ§Ã£o combina:\n- **Regra da Soma**: $w^2 + 3w$\n- **Regra do Produto**: $(w^2 + 3w) \\times \\sin(w)$\n- **Regra do Quociente**: $\\frac{w^3}{w+1}$\n\n### **Dica do Pedro** ğŸ’¡\nNa vida real, as funÃ§Ãµes de perda podem ficar bem complexas, especialmente em redes neurais profundas. Mas os **princÃ­pios** sÃ£o sempre os mesmos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo AvanÃ§ado: Combinando Todas as Regras\n",
        "print(\"ğŸ”¥ EXEMPLO AVANÃ‡ADO - Combinando Todas as Regras\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "w = symbols('w')\n",
        "\n",
        "# Definindo os componentes\n",
        "termo_soma = w**2 + 3*w\n",
        "termo_produto = termo_soma * sp.sin(w)  # (wÂ² + 3w) Ã— sin(w)\n",
        "termo_quociente = w**3 / (w + 1)       # wÂ³ / (w + 1)\n",
        "\n",
        "# FunÃ§Ã£o completa\n",
        "L_complexa = termo_produto + termo_quociente\n",
        "\n",
        "print(f\"ğŸ“Š FUNÃ‡ÃƒO DE PERDA COMPLEXA:\")\n",
        "print(f\"Termo com Produto: (wÂ² + 3w) Ã— sin(w)\")\n",
        "print(f\"Termo com Quociente: wÂ³/(w+1)\")\n",
        "print(f\"L(w) = {L_complexa}\")\n",
        "print()\n",
        "\n",
        "# Calculando a derivada total\n",
        "dL_dw_complexa = diff(L_complexa, w)\n",
        "print(f\"ğŸ”¸ DERIVADA COMPLETA:\")\n",
        "print(f\"dL/dw = {dL_dw_complexa}\")\n",
        "print()\n",
        "\n",
        "# Analisando por partes\n",
        "print(f\"ğŸ“ˆ ANÃLISE POR COMPONENTES:\")\n",
        "\n",
        "# Derivada do termo produto\n",
        "d_produto = diff(termo_produto, w)\n",
        "print(f\"d/dw[(wÂ²+3w)sin(w)] = {d_produto}\")\n",
        "print(f\"  (Usa regra da soma + regra do produto)\")\n",
        "print()\n",
        "\n",
        "# Derivada do termo quociente\n",
        "d_quociente = diff(termo_quociente, w)\n",
        "print(f\"d/dw[wÂ³/(w+1)] = {d_quociente}\")\n",
        "print(f\"  Simplificado: {simplify(d_quociente)}\")\n",
        "print(f\"  (Usa regra do quociente)\")\n",
        "print()\n",
        "\n",
        "# VerificaÃ§Ã£o\n",
        "soma_componentes = d_produto + d_quociente\n",
        "print(f\"âœ… VERIFICAÃ‡ÃƒO (Regra da Soma):\")\n",
        "print(f\"dL/dw = d_produto + d_quociente\")\n",
        "print(f\"Simplificando ambos: {simplify(dL_dw_complexa - soma_componentes) == 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a FunÃ§Ã£o Complexa\n",
        "w_range = np.linspace(-0.5, 3, 1000)\n",
        "\n",
        "# Convertendo para funÃ§Ãµes NumPy (cuidado com w = -1)\n",
        "def L_complexa_func(w_vals):\n",
        "    # Evita divisÃ£o por zero\n",
        "    safe_vals = np.where(np.abs(w_vals + 1) < 1e-10, w_vals + 1e-10, w_vals)\n",
        "    termo1 = (safe_vals**2 + 3*safe_vals) * np.sin(safe_vals)\n",
        "    termo2 = safe_vals**3 / (safe_vals + 1)\n",
        "    return termo1 + termo2\n",
        "\n",
        "def dL_complexa_func(w_vals):\n",
        "    # AproximaÃ§Ã£o numÃ©rica da derivada\n",
        "    h = 1e-8\n",
        "    return (L_complexa_func(w_vals + h) - L_complexa_func(w_vals - h)) / (2*h)\n",
        "\n",
        "L_vals = L_complexa_func(w_range)\n",
        "dL_vals = dL_complexa_func(w_range)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# FunÃ§Ã£o de perda complexa\n",
        "ax1.plot(w_range, L_vals, 'purple', linewidth=3, label='L(w) - FunÃ§Ã£o Complexa')\n",
        "ax1.set_title('ğŸ”¥ FunÃ§Ã£o de Perda Complexa (Todas as Regras Combinadas)')\n",
        "ax1.set_xlabel('w')\n",
        "ax1.set_ylabel('L(w)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Derivada da funÃ§Ã£o complexa\n",
        "ax2.plot(w_range, dL_vals, 'red', linewidth=3, label=\"dL/dw\")\n",
        "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Identificando pontos crÃ­ticos (aproximados)\n",
        "zero_crossings = []\n",
        "for i in range(len(dL_vals)-1):\n",
        "    if dL_vals[i] * dL_vals[i+1] < 0:  # MudanÃ§a de sinal\n",
        "        zero_crossings.append(w_range[i])\n",
        "\n",
        "for wc in zero_crossings[:3]:  # Primeiros 3 pontos\n",
        "    ax2.plot(wc, 0, 'go', markersize=8)\n",
        "    ax1.plot(wc, L_complexa_func(np.array([wc]))[0], 'go', markersize=8)\n",
        "\n",
        "ax2.set_title('ğŸ“Š Gradiente da FunÃ§Ã£o Complexa')\n",
        "ax2.set_xlabel('w')\n",
        "ax2.set_ylabel('dL/dw')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"ğŸ¯ PONTOS CRÃTICOS ENCONTRADOS (aprox.): {len(zero_crossings)}\")\n",
        "print(f\"ğŸ“ Primeiros pontos crÃ­ticos: {zero_crossings[:3] if zero_crossings else 'Nenhum no intervalo'}\")\n",
        "print(f\"ğŸ” Em funÃ§Ãµes complexas, pode haver mÃºltiplos mÃ­nimos locais!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ’ª ExercÃ­cio 1: Praticando as Regras\n\nAgora Ã© sua vez! Vamos praticar com uma funÃ§Ã£o de perda inspirada em problemas reais de Machine Learning.\n\n### ğŸ¯ Desafio:\nCalcule a derivada da seguinte funÃ§Ã£o de perda usando as regras que aprendemos:\n\n$$L(w) = \\frac{w^2 + 2w}{w + 3} + (w^3 - w) \\times \\cos(w)$$\n\n### Passos:\n1. **Identifique** qual regra usar para cada termo\n2. **Calcule** a derivada de cada componente\n3. **Combine** usando a regra da soma\n4. **Verifique** seu resultado usando SymPy\n\n### **Dica do Pedro** ğŸ’¡\nLembre-se: primeiro identifique a **estrutura** da funÃ§Ã£o (soma, produto, quociente), depois aplique as regras **de fora para dentro**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ’ª EXERCÃCIO 1 - EspaÃ§o para sua soluÃ§Ã£o\n",
        "print(\"ğŸ’ª EXERCÃCIO 1 - Calcule a derivada de L(w)\")\n",
        "print(\"L(w) = (wÂ² + 2w)/(w + 3) + (wÂ³ - w) Ã— cos(w)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Defina a funÃ§Ã£o aqui\n",
        "w = symbols('w')\n",
        "\n",
        "# COMPLETE AQUI - Defina cada termo da funÃ§Ã£o\n",
        "termo_quociente = # COMPLETE: (w^2 + 2w) / (w + 3)\n",
        "termo_produto =   # COMPLETE: (w^3 - w) * cos(w)\n",
        "L_exercicio =     # COMPLETE: soma dos termos\n",
        "\n",
        "print(f\"ğŸ“Š SUA FUNÃ‡ÃƒO:\")\n",
        "print(f\"Termo 1 (quociente): {termo_quociente}\")\n",
        "print(f\"Termo 2 (produto): {termo_produto}\")\n",
        "print(f\"L(w) = {L_exercicio}\")\n",
        "print()\n",
        "\n",
        "# COMPLETE AQUI - Calcule as derivadas manualmente usando as regras\n",
        "print(f\"ğŸ”¸ APLICANDO AS REGRAS:\")\n",
        "\n",
        "# Para o quociente: d/dw[(wÂ²+2w)/(w+3)]\n",
        "print(\"Termo 1 - Regra do Quociente:\")\n",
        "# COMPLETE: aplique a regra do quociente\n",
        "\n",
        "# Para o produto: d/dw[(wÂ³-w)Ã—cos(w)]\n",
        "print(\"Termo 2 - Regra do Produto:\")\n",
        "# COMPLETE: aplique a regra do produto\n",
        "\n",
        "# Calculando com SymPy para verificaÃ§Ã£o\n",
        "dL_exercicio = diff(L_exercicio, w)\n",
        "print(f\"\\nâœ… RESULTADO COM SYMPY:\")\n",
        "print(f\"dL/dw = {dL_exercicio}\")\n",
        "print(f\"Simplificado: {simplify(dL_exercicio)}\")\n",
        "\n",
        "# Descomente as linhas abaixo quando completar o exercÃ­cio\n",
        "# print(\"\\nğŸ¯ Compare seu resultado com o SymPy!\")\n",
        "# print(\"Se chegou ao mesmo resultado, parabÃ©ns! ğŸ‰\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ ExercÃ­cio 2: AplicaÃ§Ã£o em Gradient Descent\n\nAgora vamos usar nossas regras de derivaÃ§Ã£o numa aplicaÃ§Ã£o **real** de otimizaÃ§Ã£o! \n\n### ğŸ¯ CenÃ¡rio:\nVocÃª tem uma funÃ§Ã£o de perda simples para um problema de regressÃ£o:\n$$L(w) = (y - wx)^2 + 0.1w^2$$\n\nDados: $x = 3$, $y = 7$ (queremos encontrar $w$ que minimiza a perda)\n\n### Tarefa:\n1. **Calcule** $\\frac{dL}{dw}$ usando as regras de derivaÃ§Ã£o\n2. **Implemente** um mini Gradient Descent\n3. **Visualize** como $w$ converge para o mÃ­nimo\n\n### **Dica do Pedro** ğŸ’¡\nLembra do **MÃ³dulo 1**? Estamos literalmente \"descendo a montanha do erro\"! O gradiente nos diz a direÃ§Ã£o mais Ã­ngreme!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš€ EXERCÃCIO 2 - Mini Gradient Descent\n",
        "print(\"ğŸš€ EXERCÃCIO 2 - AplicaÃ§Ã£o em Gradient Descent\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Dados do problema\n",
        "x_data = 3\n",
        "y_data = 7\n",
        "lambda_reg = 0.1\n",
        "\n",
        "print(f\"ğŸ“Š DADOS:\")\n",
        "print(f\"x = {x_data}, y = {y_data}, Î» = {lambda_reg}\")\n",
        "print(f\"FunÃ§Ã£o: L(w) = (y - wx)Â² + Î»wÂ²\")\n",
        "print(f\"Substituindo: L(w) = ({y_data} - {x_data}w)Â² + {lambda_reg}wÂ²\")\n",
        "print()\n",
        "\n",
        "# COMPLETE AQUI - Implemente as funÃ§Ãµes\n",
        "def loss_function(w):\n",
        "    \"\"\"Calcula L(w) = (y - wx)Â² + Î»wÂ²\"\"\"\n",
        "    # COMPLETE: retorne o valor da funÃ§Ã£o de perda\n",
        "    pass\n",
        "\n",
        "def gradient_function(w):\n",
        "    \"\"\"Calcula dL/dw usando as regras de derivaÃ§Ã£o\"\"\"\n",
        "    # COMPLETE: calcule e retorne o gradiente\n",
        "    # Dica: use regra da soma + regra da cadeia para (y-wx)Â²\n",
        "    # Resultado deve ser: -2x(y-wx) + 2Î»w\n",
        "    pass\n",
        "\n",
        "def gradient_descent(w_inicial, learning_rate, num_iterations):\n",
        "    \"\"\"Implementa o algoritmo de Gradient Descent\"\"\"\n",
        "    w = w_inicial\n",
        "    history = {'w': [w], 'loss': [loss_function(w)]}\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        # COMPLETE: calcule o gradiente\n",
        "        grad = # COMPLETE\n",
        "        \n",
        "        # COMPLETE: atualize w usando a regra: w = w - learning_rate * gradiente\n",
        "        w = # COMPLETE\n",
        "        \n",
        "        # Salva histÃ³rico\n",
        "        history['w'].append(w)\n",
        "        history['loss'].append(loss_function(w))\n",
        "    \n",
        "    return w, history\n",
        "\n",
        "# Testando (descomente quando completar)\n",
        "# w_inicial = 0.5\n",
        "# learning_rate = 0.01\n",
        "# num_iterations = 100\n",
        "\n",
        "# w_final, history = gradient_descent(w_inicial, learning_rate, num_iterations)\n",
        "\n",
        "# print(f\"ğŸ¯ RESULTADOS:\")\n",
        "# print(f\"w inicial: {w_inicial}\")\n",
        "# print(f\"w final: {w_final:.4f}\")\n",
        "# print(f\"Loss inicial: {history['loss'][0]:.4f}\")\n",
        "# print(f\"Loss final: {history['loss'][-1]:.4f}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ Complete as funÃ§Ãµes acima e descomente o teste!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‰ Resumo: As Regras do Jogo Dominadas!\n\n**Liiindo!** Chegamos ao fim do nosso mÃ³dulo sobre as **regras de derivaÃ§Ã£o**! Vamos recapitular o que aprendemos:\n\n### ğŸ“š O Que Dominamos:\n\n#### 1. **Regra da Soma/SubtraÃ§Ã£o** â•\n$$\\frac{d}{dx}[f(x) + g(x)] = f'(x) + g'(x)$$\n- **Analogia**: Como medir velocidades de carros na mesma estrada\n- **AplicaÃ§Ã£o**: Separar componentes de loss (MSE + RegularizaÃ§Ã£o)\n\n#### 2. **Regra do Produto** âœ–ï¸\n$$\\frac{d}{dx}[f(x) \\cdot g(x)] = f'(x) \\cdot g(x) + f(x) \\cdot g'(x)$$\n- **Analogia**: A danÃ§a de dois danÃ§arinos\n- **AplicaÃ§Ã£o**: Termos que se multiplicam nas funÃ§Ãµes de perda\n\n#### 3. **Regra do Quociente** â—\n$$\\frac{d}{dx}\\left[\\frac{f(x)}{g(x)}\\right] = \\frac{f'(x) \\cdot g(x) - f(x) \\cdot g'(x)}{[g(x)]^2}$$\n- **Analogia**: A gangorra matemÃ¡tica\n- **AplicaÃ§Ã£o**: MÃ©tricas de eficiÃªncia e normalizaÃ§Ã£o\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cÃ¡lculo-para-ia-modulo-05_img_04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”® ConexÃ£o com os PrÃ³ximos MÃ³dulos\n\n### **MÃ³dulo 6 - Regra da Cadeia** ğŸ”—\nAs regras que aprendemos hoje sÃ£o **fundamentais**, mas ainda nÃ£o conseguimos derivar funÃ§Ãµes como $(3x^2 + 1)^{10}$ ou $\\sin(x^2)$. Para isso, precisamos da **Regra da Cadeia** - que Ã© o **coraÃ§Ã£o** do backpropagation!\n\n### **MÃ³dulos 11-12 - Gradiente Descendente** â¬‡ï¸\nTodo o cÃ¡lculo de derivadas que fizemos hoje serÃ¡ usado para **otimizar** funÃ§Ãµes de perda reais. O gradiente que calculamos Ã© exatamente a \"**bÃºssola**\" que nos guia na descida da montanha do erro!\n\n```mermaid\ngraph LR\n    A[MÃ³dulo 5: Regras de DerivaÃ§Ã£o] --> B[MÃ³dulo 6: Regra da Cadeia]\n    B --> C[MÃ³dulo 10: Gradientes]\n    C --> D[MÃ³dulo 11: Gradient Descent]\n    D --> E[MÃ³dulo 12: Otimizadores AvanÃ§ados]\n    \n    A -.-> F[FunÃ§Ãµes de Perda Complexas]\n    B -.-> G[Backpropagation]\n    C -.-> H[Redes Neurais]\n```\n\n### **Dica do Pedro** para o PrÃ³ximo MÃ³dulo ğŸ’¡\nPratique bastante as regras de hoje! No **MÃ³dulo 6**, vamos combinar elas com a **Regra da Cadeia** para criar o algoritmo mais importante da IA moderna. Vai ser **Ã©pico**! ğŸš€\n\n---\n\n**Bora** para o prÃ³ximo mÃ³dulo! Nos vemos na **Regra da Cadeia** - onde a mÃ¡gica do backpropagation acontece! ğŸ¯âœ¨"
      ]
    }
  ]
}