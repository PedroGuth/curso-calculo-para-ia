{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modulo_10_Derivadas_Parciais_Gradientes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß≠ Derivadas Parciais e Gradientes: A B√∫ssola do Aprendizado\n\n## *M√≥dulo 10 - Como encontrar a derivada de uma fun√ß√£o com v√°rias vari√°veis. O vetor gradiente e como ele aponta na dire√ß√£o de maior inclina√ß√£o*\n\n---\n\n**Por Pedro Nunes Guth**\n\nFala, galera! üöÄ Chegamos ao m√≥dulo que vai mudar sua vida no mundo da IA!\n\nNos m√≥dulos anteriores, voc√™s j√° dominaram:\n- Fun√ß√µes de m√∫ltiplas vari√°veis (nossa paisagem do erro)\n- Derivadas simples (a inclina√ß√£o em uma dimens√£o)\n- A regra da cadeia (a base do backpropagation)\n\nAgora vamos juntar tudo isso e descobrir como navegar nessa paisagem multidimensional usando nossa **b√∫ssola matem√°tica**: o gradiente!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-10_img_01.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports necess√°rios para nossa jornada\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import sympy as sp\n",
        "from matplotlib import cm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√µes visuais\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"üß≠ Bibliotecas carregadas! Bora explorar os gradientes!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ O Que S√£o Derivadas Parciais?\n\nT√°, mas o que √© uma derivada parcial afinal?\n\nImagina que voc√™ t√° numa festa no terra√ßo de um pr√©dio em Copacabana. A altura do terra√ßo varia dependendo de onde voc√™ t√° (coordenada x) E tamb√©m de que andar voc√™ t√° (coordenada y).\n\nA **derivada parcial** √© como se voc√™ perguntasse:\n- \"Se eu andar s√≥ pra frente/tr√°s (eixo x), mantendo minha posi√ß√£o lateral fixa, qu√£o √≠ngreme fica?\"\n- \"Se eu andar s√≥ pros lados (eixo y), mantendo minha posi√ß√£o frontal fixa, qu√£o √≠ngreme fica?\"\n\n### Defini√ß√£o Matem√°tica\n\nPara uma fun√ß√£o $f(x, y)$, temos:\n\n**Derivada parcial em rela√ß√£o a x:**\n$$\\frac{\\partial f}{\\partial x} = \\lim_{h \\to 0} \\frac{f(x+h, y) - f(x, y)}{h}$$\n\n**Derivada parcial em rela√ß√£o a y:**\n$$\\frac{\\partial f}{\\partial y} = \\lim_{h \\to 0} \\frac{f(x, y+h) - f(x, y)}{h}$$\n\n**Dica do Pedro:** O s√≠mbolo $\\partial$ (\"del\" ou \"d parcial\") √© usado pra diferenciar das derivadas \"normais\" que voc√™s j√° conhecem. √â s√≥ um jeito de falar: \"√≥, aqui tem mais de uma vari√°vel, ent√£o t√¥ derivando em rela√ß√£o a UMA s√≥\".\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-10_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos ver as derivadas parciais na pr√°tica\n",
        "# Definindo s√≠mbolos para trabalhar com derivadas simb√≥licas\n",
        "x, y = sp.symbols('x y')\n",
        "\n",
        "# Nossa fun√ß√£o exemplo: f(x,y) = x¬≤ + 2xy + y¬≤\n",
        "# (lembra da fun√ß√£o de custo que vimos antes?)\n",
        "f = x**2 + 2*x*y + y**2\n",
        "\n",
        "print(\"üìä Nossa fun√ß√£o: f(x,y) =\", f)\n",
        "print()\n",
        "\n",
        "# Calculando as derivadas parciais\n",
        "df_dx = sp.diff(f, x)  # Derivada parcial em rela√ß√£o a x\n",
        "df_dy = sp.diff(f, y)  # Derivada parcial em rela√ß√£o a y\n",
        "\n",
        "print(\"üéØ Derivada parcial em rela√ß√£o a x:\")\n",
        "print(f\"‚àÇf/‚àÇx = {df_dx}\")\n",
        "print()\n",
        "print(\"üéØ Derivada parcial em rela√ß√£o a y:\")\n",
        "print(f\"‚àÇf/‚àÇy = {df_dy}\")\n",
        "print()\n",
        "\n",
        "# Vamos avaliar num ponto espec√≠fico (2, 1)\n",
        "ponto_x, ponto_y = 2, 1\n",
        "valor_df_dx = df_dx.subs([(x, ponto_x), (y, ponto_y)])\n",
        "valor_df_dy = df_dy.subs([(x, ponto_x), (y, ponto_y)])\n",
        "\n",
        "print(f\"üí° No ponto ({ponto_x}, {ponto_y}):\")\n",
        "print(f\"‚àÇf/‚àÇx = {valor_df_dx}\")\n",
        "print(f\"‚àÇf/‚àÇy = {valor_df_dy}\")\n",
        "print()\n",
        "print(\"üîç Interpreta√ß√£o:\")\n",
        "print(f\"- Se aumentarmos x (mantendo y=1), a fun√ß√£o cresce {valor_df_dx} unidades por unidade de x\")\n",
        "print(f\"- Se aumentarmos y (mantendo x=2), a fun√ß√£o cresce {valor_df_dy} unidades por unidade de y\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Visualizando Derivadas Parciais\n\nBora ver isso graficamente! Liiindo!\n\nVou mostrar como as derivadas parciais s√£o literalmente as inclina√ß√µes das \"fatias\" da nossa fun√ß√£o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando uma visualiza√ß√£o das derivadas parciais\n",
        "def nossa_funcao(x, y):\n",
        "    \"\"\"Nossa fun√ß√£o f(x,y) = x¬≤ + 2xy + y¬≤\"\"\"\n",
        "    return x**2 + 2*x*y + y**2\n",
        "\n",
        "def derivada_parcial_x(x, y):\n",
        "    \"\"\"‚àÇf/‚àÇx = 2x + 2y\"\"\"\n",
        "    return 2*x + 2*y\n",
        "\n",
        "def derivada_parcial_y(x, y):\n",
        "    \"\"\"‚àÇf/‚àÇy = 2x + 2y\"\"\"\n",
        "    return 2*x + 2*y\n",
        "\n",
        "# Criando a grade de pontos\n",
        "x_vals = np.linspace(-3, 3, 100)\n",
        "y_vals = np.linspace(-3, 3, 100)\n",
        "X, Y = np.meshgrid(x_vals, y_vals)\n",
        "Z = nossa_funcao(X, Y)\n",
        "\n",
        "# Plotando a fun√ß√£o 3D e as derivadas\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Subplot 1: Fun√ß√£o original\n",
        "ax1 = fig.add_subplot(131, projection='3d')\n",
        "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
        "ax1.set_title('Fun√ß√£o Original f(x,y)')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_zlabel('f(x,y)')\n",
        "\n",
        "# Subplot 2: Derivada parcial em x\n",
        "ax2 = fig.add_subplot(132)\n",
        "# Fatia em y=1 (fixo)\n",
        "y_fixo = 1\n",
        "z_fatia_x = nossa_funcao(x_vals, y_fixo)\n",
        "inclinacao_x = derivada_parcial_x(x_vals, y_fixo)\n",
        "\n",
        "ax2.plot(x_vals, z_fatia_x, 'b-', linewidth=2, label=f'f(x, y={y_fixo})')\n",
        "ax2.plot(x_vals, inclinacao_x, 'r--', linewidth=2, label='‚àÇf/‚àÇx')\n",
        "ax2.set_title(f'Fatia da fun√ß√£o em y={y_fixo}')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('Valor')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Subplot 3: Derivada parcial em y\n",
        "ax3 = fig.add_subplot(133)\n",
        "# Fatia em x=1 (fixo)\n",
        "x_fixo = 1\n",
        "z_fatia_y = nossa_funcao(x_fixo, y_vals)\n",
        "inclinacao_y = derivada_parcial_y(x_fixo, y_vals)\n",
        "\n",
        "ax3.plot(y_vals, z_fatia_y, 'b-', linewidth=2, label=f'f(x={x_fixo}, y)')\n",
        "ax3.plot(y_vals, inclinacao_y, 'g--', linewidth=2, label='‚àÇf/‚àÇy')\n",
        "ax3.set_title(f'Fatia da fun√ß√£o em x={x_fixo}')\n",
        "ax3.set_xlabel('y')\n",
        "ax3.set_ylabel('Valor')\n",
        "ax3.legend()\n",
        "ax3.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Olha s√≥ que lindo! As derivadas parciais mostram a inclina√ß√£o em cada dire√ß√£o!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß≠ O Vetor Gradiente: Nossa B√∫ssola M√°gica\n\nAgora vem a parte mais dahora! O **gradiente** √© simplesmente um vetor que junta todas as derivadas parciais.\n\n### Defini√ß√£o do Gradiente\n\nPara uma fun√ß√£o $f(x, y)$, o gradiente √©:\n\n$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$\n\nO s√≠mbolo $\\nabla$ (\"nabla\") √© nosso operador gradiente.\n\n### Por Que o Gradiente √â Especial?\n\nO gradiente tem uma propriedade **INCR√çVEL**:\n\nüéØ **Ele sempre aponta na dire√ß√£o de MAIOR crescimento da fun√ß√£o!**\n\n√â como se fosse uma seta que diz: \"√ì, se voc√™ quer que a fun√ß√£o cres√ßa o m√°ximo poss√≠vel, v√° nessa dire√ß√£o aqui!\"\n\n### Analogia do Pedro\n\nImagina que voc√™ t√° perdido no P√£o de A√ß√∫car numa neblina. Voc√™ quer chegar no topo o mais r√°pido poss√≠vel. O gradiente √© como um GPS que sempre aponta na dire√ß√£o mais √≠ngreme pra cima!\n\n**Dica do Pedro:** Se voc√™ quer ir pro vale (minimizar a fun√ß√£o), √© s√≥ seguir na dire√ß√£o **OPOSTA** ao gradiente! √â exatamente isso que fazemos no Gradiente Descendente (pr√≥ximo m√≥dulo).\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-10_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando o c√°lculo do gradiente\n",
        "def calcular_gradiente(x, y):\n",
        "    \"\"\"Calcula o gradiente da nossa fun√ß√£o no ponto (x, y)\"\"\"\n",
        "    df_dx = 2*x + 2*y  # ‚àÇf/‚àÇx\n",
        "    df_dy = 2*x + 2*y  # ‚àÇf/‚àÇy\n",
        "    return np.array([df_dx, df_dy])\n",
        "\n",
        "# Testando em alguns pontos\n",
        "pontos_teste = [(0, 0), (1, 1), (-1, 2), (2, -1)]\n",
        "\n",
        "print(\"üß≠ Calculando gradientes em diferentes pontos:\\n\")\n",
        "\n",
        "for i, (px, py) in enumerate(pontos_teste):\n",
        "    grad = calcular_gradiente(px, py)\n",
        "    valor_funcao = nossa_funcao(px, py)\n",
        "    magnitude = np.linalg.norm(grad)  # Tamanho do vetor\n",
        "    \n",
        "    print(f\"üìç Ponto ({px}, {py}):\")\n",
        "    print(f\"   f({px}, {py}) = {valor_funcao}\")\n",
        "    print(f\"   ‚àáf = [{grad[0]}, {grad[1]}]\")\n",
        "    print(f\"   |‚àáf| = {magnitude:.2f} (magnitude do gradiente)\")\n",
        "    \n",
        "    if magnitude == 0:\n",
        "        print(f\"   üéØ PONTO CR√çTICO! Gradiente = 0\")\n",
        "    else:\n",
        "        print(f\"   üöÄ Dire√ß√£o de maior crescimento: [{grad[0]/magnitude:.2f}, {grad[1]/magnitude:.2f}]\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Visualizando o Campo de Gradientes\n\nBora ver como esses vetores gradiente ficam espalhados pela nossa paisagem! Vai ficar liiindo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando um campo de vetores gradiente\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Criando uma grade mais esparsa para os vetores\n",
        "x_vec = np.linspace(-2, 2, 15)\n",
        "y_vec = np.linspace(-2, 2, 15)\n",
        "X_vec, Y_vec = np.meshgrid(x_vec, y_vec)\n",
        "\n",
        "# Calculando os gradientes em cada ponto\n",
        "U = 2*X_vec + 2*Y_vec  # ‚àÇf/‚àÇx\n",
        "V = 2*X_vec + 2*Y_vec  # ‚àÇf/‚àÇy\n",
        "\n",
        "# Plot 1: Contorno da fun√ß√£o + campo de gradientes\n",
        "x_contour = np.linspace(-3, 3, 100)\n",
        "y_contour = np.linspace(-3, 3, 100)\n",
        "X_cont, Y_cont = np.meshgrid(x_contour, y_contour)\n",
        "Z_cont = nossa_funcao(X_cont, Y_cont)\n",
        "\n",
        "contour = ax1.contour(X_cont, Y_cont, Z_cont, levels=15, alpha=0.6)\n",
        "ax1.clabel(contour, inline=True, fontsize=8)\n",
        "\n",
        "# Adicionando os vetores gradiente\n",
        "ax1.quiver(X_vec, Y_vec, U, V, \n",
        "          scale=50, scale_units='xy', angles='xy', \n",
        "          color='red', alpha=0.7, width=0.003)\n",
        "\n",
        "ax1.set_title('Campo de Gradientes sobre Curvas de N√≠vel')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.axis('equal')\n",
        "\n",
        "# Plot 2: Mapa de calor da magnitude do gradiente\n",
        "magnitude_grad = np.sqrt(U**2 + V**2)\n",
        "heatmap = ax2.imshow(magnitude_grad, extent=[-2, 2, -2, 2], \n",
        "                    origin='lower', cmap='plasma', alpha=0.8)\n",
        "ax2.contour(X_vec, Y_vec, magnitude_grad, levels=10, colors='white', alpha=0.5)\n",
        "\n",
        "# Adicionando colorbar\n",
        "plt.colorbar(heatmap, ax=ax2, label='|‚àáf|')\n",
        "\n",
        "ax2.set_title('Magnitude do Gradiente')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üî• Repara como:\")\n",
        "print(\"üìç Os vetores vermelhos SEMPRE apontam perpendicular √†s curvas de n√≠vel\")\n",
        "print(\"üìç Quanto mais 'quente' a cor, maior a magnitude do gradiente\")\n",
        "print(\"üìç No centro (0,0), o gradiente √© zero = ponto cr√≠tico!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Fluxograma: Do Conceito √† Aplica√ß√£o\n\nVamos visualizar como todos esses conceitos se conectam:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\ngraph TD\n    A[Fun√ß√£o f(x,y)] --> B[Calcular ‚àÇf/‚àÇx]\n    A --> C[Calcular ‚àÇf/‚àÇy]\n    B --> D[Montar Vetor Gradiente ‚àáf]\n    C --> D\n    D --> E[Dire√ß√£o de Maior Crescimento]\n    D --> F[Inverter Dire√ß√£o = Maior Decrescimento]\n    E --> G[M√°ximo Local]\n    F --> H[M√≠nimo Local]\n    H --> I[Gradiente Descendente]\n    I --> J[Otimiza√ß√£o de IA]\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Exemplo Pr√°tico: Fun√ß√£o de Custo de Machine Learning\n\nBora ver isso aplicado numa fun√ß√£o de custo real de ML! \n\nVamos usar o exemplo cl√°ssico da regress√£o linear: queremos encontrar os melhores par√¢metros $w_0$ (intercepto) e $w_1$ (inclina√ß√£o) para uma reta.\n\nA fun√ß√£o de custo Mean Squared Error (MSE) √©:\n\n$$J(w_0, w_1) = \\frac{1}{2m} \\sum_{i=1}^{m} (w_0 + w_1 x^{(i)} - y^{(i)})^2$$\n\n**Dica do Pedro:** Essa √© a mesma ideia que vimos no M√≥dulo 9, mas agora vamos calcular o gradiente pra saber pra onde \"descer\"!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando dados sint√©ticos para regress√£o linear\n",
        "np.random.seed(42)\n",
        "m = 20  # n√∫mero de amostras\n",
        "X_data = np.random.uniform(-2, 2, m)\n",
        "y_true = 1.5 * X_data + 0.5 + np.random.normal(0, 0.3, m)  # y = 1.5x + 0.5 + ru√≠do\n",
        "\n",
        "def custo_mse(w0, w1, X, y):\n",
        "    \"\"\"Fun√ß√£o de custo MSE\"\"\"\n",
        "    predicoes = w0 + w1 * X\n",
        "    erro = predicoes - y\n",
        "    return np.mean(erro**2) / 2\n",
        "\n",
        "def gradiente_mse(w0, w1, X, y):\n",
        "    \"\"\"Gradiente da fun√ß√£o MSE\"\"\"\n",
        "    m = len(X)\n",
        "    predicoes = w0 + w1 * X\n",
        "    erro = predicoes - y\n",
        "    \n",
        "    # Derivadas parciais\n",
        "    dJ_dw0 = np.mean(erro)  # ‚àÇJ/‚àÇw0\n",
        "    dJ_dw1 = np.mean(erro * X)  # ‚àÇJ/‚àÇw1\n",
        "    \n",
        "    return np.array([dJ_dw0, dJ_dw1])\n",
        "\n",
        "# Visualizando os dados\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot 1: Dados originais\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(X_data, y_true, alpha=0.7, color='blue')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Dados de Treino')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Superf√≠cie de custo\n",
        "w0_range = np.linspace(-1, 2, 50)\n",
        "w1_range = np.linspace(0, 3, 50)\n",
        "W0, W1 = np.meshgrid(w0_range, w1_range)\n",
        "J_surface = np.zeros_like(W0)\n",
        "\n",
        "for i in range(len(w0_range)):\n",
        "    for j in range(len(w1_range)):\n",
        "        J_surface[j, i] = custo_mse(W0[j, i], W1[j, i], X_data, y_true)\n",
        "\n",
        "ax2 = plt.subplot(1, 3, 2, projection='3d')\n",
        "surf = ax2.plot_surface(W0, W1, J_surface, cmap='viridis', alpha=0.7)\n",
        "ax2.set_xlabel('w0 (intercepto)')\n",
        "ax2.set_ylabel('w1 (inclina√ß√£o)')\n",
        "ax2.set_zlabel('Custo J')\n",
        "ax2.set_title('Superf√≠cie de Custo MSE')\n",
        "\n",
        "# Plot 3: Curvas de n√≠vel + gradientes\n",
        "plt.subplot(1, 3, 3)\n",
        "contour = plt.contour(W0, W1, J_surface, levels=20)\n",
        "plt.clabel(contour, inline=True, fontsize=8)\n",
        "\n",
        "# Calculando gradientes em alguns pontos\n",
        "w0_points = [0, 0.5, 1.0, 1.5]\n",
        "w1_points = [1, 1.5, 2.0, 2.5]\n",
        "\n",
        "for w0_p, w1_p in zip(w0_points, w1_points):\n",
        "    grad = gradiente_mse(w0_p, w1_p, X_data, y_true)\n",
        "    plt.arrow(w0_p, w1_p, -grad[0]*0.5, -grad[1]*0.5, \n",
        "             head_width=0.05, head_length=0.05, fc='red', ec='red')\n",
        "\n",
        "plt.xlabel('w0 (intercepto)')\n",
        "plt.ylabel('w1 (inclina√ß√£o)')\n",
        "plt.title('Gradientes (setas vermelhas)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Repara que as setas vermelhas apontam na dire√ß√£o do M√çNIMO!\")\n",
        "print(\"   (Invertemos o gradiente pra mostrar a dire√ß√£o de descida)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßÆ Propriedades Importantes do Gradiente\n\nAntes de partir pra pr√°tica, vamos consolidar as propriedades mais importantes:\n\n### 1. **Dire√ß√£o de Maior Crescimento**\n$$\\nabla f \\text{ aponta na dire√ß√£o onde } f \\text{ cresce mais rapidamente}$$\n\n### 2. **Perpendicular √†s Curvas de N√≠vel**\n$$\\nabla f \\perp \\text{curvas de n√≠vel}$$\n\n### 3. **Magnitude = Taxa M√°xima de Varia√ß√£o**\n$$|\\nabla f| = \\text{m√°xima taxa de varia√ß√£o no ponto}$$\n\n### 4. **Gradiente Zero = Ponto Cr√≠tico**\n$$\\nabla f = 0 \\Rightarrow \\text{poss√≠vel m√°ximo, m√≠nimo ou ponto de sela}$$\n\n### Regras de Deriva√ß√£o para Gradientes\n\n**Linearidade:**\n$$\\nabla(af + bg) = a\\nabla f + b\\nabla g$$\n\n**Regra do Produto:**\n$$\\nabla(fg) = f\\nabla g + g\\nabla f$$\n\n**Regra da Cadeia (multivari√°vel):**\n$$\\nabla f(g(x,y), h(x,y)) = \\frac{\\partial f}{\\partial g}\\nabla g + \\frac{\\partial f}{\\partial h}\\nabla h$$\n\n**Dica do Pedro:** A regra da cadeia multivari√°vel √© o cora√ß√£o do backpropagation! Voc√™s v√£o ver isso funcionando no pr√≥ximo m√≥dulo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando as propriedades do gradiente\n",
        "def demonstrar_propriedades():\n",
        "    \"\"\"Demonstra as propriedades principais do gradiente\"\"\"\n",
        "    \n",
        "    # Definindo uma fun√ß√£o mais complexa\n",
        "    def f_complexa(x, y):\n",
        "        return x**2 + 2*y**2 + x*y - 2*x - 4*y + 5\n",
        "    \n",
        "    def grad_f_complexa(x, y):\n",
        "        df_dx = 2*x + y - 2\n",
        "        df_dy = 4*y + x - 4\n",
        "        return np.array([df_dx, df_dy])\n",
        "    \n",
        "    print(\"üîç Testando propriedades do gradiente\\n\")\n",
        "    \n",
        "    # Propriedade 1: Dire√ß√£o de maior crescimento\n",
        "    ponto = np.array([1, 1])\n",
        "    grad = grad_f_complexa(ponto[0], ponto[1])\n",
        "    \n",
        "    if np.linalg.norm(grad) > 0:\n",
        "        direcao_grad = grad / np.linalg.norm(grad)  # Normalizar\n",
        "        \n",
        "        # Testar v√°rias dire√ß√µes\n",
        "        angulos = np.linspace(0, 2*np.pi, 8)\n",
        "        direcoes = np.array([[np.cos(a), np.sin(a)] for a in angulos])\n",
        "        \n",
        "        print(f\"üìç No ponto {ponto}:\")\n",
        "        print(f\"   Gradiente: [{grad[0]:.3f}, {grad[1]:.3f}]\")\n",
        "        print(f\"   Dire√ß√£o do gradiente: [{direcao_grad[0]:.3f}, {direcao_grad[1]:.3f}]\")\n",
        "        print()\n",
        "        \n",
        "        # Calculando derivada direcional em v√°rias dire√ß√µes\n",
        "        step = 0.01\n",
        "        derivadas_direcionais = []\n",
        "        \n",
        "        for direcao in direcoes:\n",
        "            # Derivada direcional = gradiente ¬∑ dire√ß√£o\n",
        "            deriv_dir = np.dot(grad, direcao)\n",
        "            derivadas_direcionais.append(deriv_dir)\n",
        "        \n",
        "        derivadas_direcionais = np.array(derivadas_direcionais)\n",
        "        max_idx = np.argmax(derivadas_direcionais)\n",
        "        \n",
        "        print(\"üéØ Derivadas direcionais em diferentes dire√ß√µes:\")\n",
        "        for i, (angulo, deriv) in enumerate(zip(angulos, derivadas_direcionais)):\n",
        "            simbolo = \"üî•\" if i == max_idx else \"  \"\n",
        "            print(f\"   {simbolo} √Çngulo {angulo:.2f}: {deriv:.3f}\")\n",
        "        \n",
        "        print(f\"\\n‚úÖ Confirmado: M√°xima derivada direcional na dire√ß√£o do gradiente!\")\n",
        "        print(f\"   Valor m√°ximo: {derivadas_direcionais[max_idx]:.3f}\")\n",
        "        print(f\"   Magnitude do gradiente: {np.linalg.norm(grad):.3f}\")\n",
        "    \n",
        "    # Propriedade 2: Encontrar pontos cr√≠ticos\n",
        "    print(\"\\nüîç Procurando pontos cr√≠ticos (‚àáf = 0):\")\n",
        "    # Para nossa fun√ß√£o, resolvemos o sistema:\n",
        "    # 2x + y - 2 = 0\n",
        "    # x + 4y - 4 = 0\n",
        "    \n",
        "    # Solu√ß√£o: x = 4/7, y = 6/7\n",
        "    x_critico = 4/7\n",
        "    y_critico = 6/7\n",
        "    \n",
        "    grad_critico = grad_f_complexa(x_critico, y_critico)\n",
        "    valor_critico = f_complexa(x_critico, y_critico)\n",
        "    \n",
        "    print(f\"üìç Ponto cr√≠tico encontrado: ({x_critico:.4f}, {y_critico:.4f})\")\n",
        "    print(f\"   Gradiente no ponto: [{grad_critico[0]:.6f}, {grad_critico[1]:.6f}]\")\n",
        "    print(f\"   Valor da fun√ß√£o: {valor_critico:.4f}\")\n",
        "    \n",
        "demonstrar_propriedades()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Comparando Diferentes Tipos de Fun√ß√µes\n\nVamos ver como o gradiente se comporta em diferentes \"paisagens\"!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparando diferentes fun√ß√µes e seus gradientes\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Fun√ß√£o 1: Paraboloide (convexa)\n",
        "def f1(x, y):\n",
        "    return x**2 + y**2\n",
        "\n",
        "def grad_f1(x, y):\n",
        "    return np.array([2*x, 2*y])\n",
        "\n",
        "# Fun√ß√£o 2: Ponto de sela\n",
        "def f2(x, y):\n",
        "    return x**2 - y**2\n",
        "\n",
        "def grad_f2(x, y):\n",
        "    return np.array([2*x, -2*y])\n",
        "\n",
        "# Fun√ß√£o 3: Fun√ß√£o de Rosenbrock (banana function)\n",
        "def f3(x, y):\n",
        "    a, b = 1, 100\n",
        "    return (a - x)**2 + b*(y - x**2)**2\n",
        "\n",
        "def grad_f3(x, y):\n",
        "    a, b = 1, 100\n",
        "    df_dx = -2*(a - x) - 4*b*x*(y - x**2)\n",
        "    df_dy = 2*b*(y - x**2)\n",
        "    return np.array([df_dx, df_dy])\n",
        "\n",
        "funcoes = [f1, f2, f3]\n",
        "gradientes = [grad_f1, grad_f2, grad_f3]\n",
        "nomes = ['Paraboloide (Convexa)', 'Ponto de Sela', 'Rosenbrock (Banana)']\n",
        "ranges = [(-2, 2), (-2, 2), (-1.5, 1.5)]\n",
        "\n",
        "for i, (func, grad_func, nome, (min_val, max_val)) in enumerate(zip(funcoes, gradientes, nomes, ranges)):\n",
        "    # Criando a grade\n",
        "    x_range = np.linspace(min_val, max_val, 100)\n",
        "    y_range = np.linspace(min_val, max_val, 100)\n",
        "    X, Y = np.meshgrid(x_range, y_range)\n",
        "    Z = func(X, Y)\n",
        "    \n",
        "    # Superf√≠cie 3D\n",
        "    ax_3d = axes[0, i]\n",
        "    if i == 2:  # Rosenbrock precisa de escala logar√≠tmica\n",
        "        Z_log = np.log10(Z + 1)\n",
        "        surf = ax_3d.contour(X, Y, Z_log, levels=15)\n",
        "    else:\n",
        "        surf = ax_3d.contour(X, Y, Z, levels=15)\n",
        "    \n",
        "    ax_3d.set_title(f'{nome}\\n(Curvas de N√≠vel)')\n",
        "    ax_3d.set_xlabel('x')\n",
        "    ax_3d.set_ylabel('y')\n",
        "    ax_3d.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Campo de gradientes\n",
        "    ax_grad = axes[1, i]\n",
        "    \n",
        "    # Grade mais esparsa para vetores\n",
        "    x_vec = np.linspace(min_val, max_val, 12)\n",
        "    y_vec = np.linspace(min_val, max_val, 12)\n",
        "    X_vec, Y_vec = np.meshgrid(x_vec, y_vec)\n",
        "    \n",
        "    U, V = np.zeros_like(X_vec), np.zeros_like(Y_vec)\n",
        "    for row in range(X_vec.shape[0]):\n",
        "        for col in range(X_vec.shape[1]):\n",
        "            grad_val = grad_func(X_vec[row, col], Y_vec[row, col])\n",
        "            U[row, col] = grad_val[0]\n",
        "            V[row, col] = grad_val[1]\n",
        "    \n",
        "    # Normalizando os vetores para visualiza√ß√£o\n",
        "    magnitude = np.sqrt(U**2 + V**2)\n",
        "    # Evitar divis√£o por zero\n",
        "    mask = magnitude > 1e-10\n",
        "    U_norm, V_norm = np.zeros_like(U), np.zeros_like(V)\n",
        "    U_norm[mask] = U[mask] / magnitude[mask]\n",
        "    V_norm[mask] = V[mask] / magnitude[mask]\n",
        "    \n",
        "    # Plot do campo de vetores\n",
        "    if i == 2:  # Rosenbrock\n",
        "        contour_bg = ax_grad.contour(X, Y, np.log10(Z + 1), levels=10, alpha=0.3)\n",
        "    else:\n",
        "        contour_bg = ax_grad.contour(X, Y, Z, levels=10, alpha=0.3)\n",
        "    \n",
        "    ax_grad.quiver(X_vec, Y_vec, U_norm, V_norm, magnitude,\n",
        "                  scale=15, scale_units='xy', angles='xy', cmap='plasma')\n",
        "    \n",
        "    ax_grad.set_title(f'Campo de Gradientes\\n(Cor = Magnitude)')\n",
        "    ax_grad.set_xlabel('x')\n",
        "    ax_grad.set_ylabel('y')\n",
        "    ax_grad.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üî• Compara√ß√£o das paisagens:\")\n",
        "print(\"üìç Paraboloide: Todos os gradientes apontam pro centro (m√≠nimo global)\")\n",
        "print(\"üìç Ponto de Sela: Gradientes se afastam numa dire√ß√£o, se aproximam na outra\")\n",
        "print(\"üìç Rosenbrock: Paisagem complexa com vale estreito (desafio para otimiza√ß√£o!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéÆ Exerc√≠cio Pr√°tico 1: Calculadora de Gradientes\n\nBora praticar! Implementa uma calculadora que recebe uma fun√ß√£o simb√≥lica e calcula o gradiente automaticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO: Complete a fun√ß√£o abaixo\n",
        "def calculadora_gradiente(expressao_str, ponto):\n",
        "    \"\"\"\n",
        "    Calcula o gradiente de uma fun√ß√£o simb√≥lica num ponto espec√≠fico.\n",
        "    \n",
        "    Args:\n",
        "        expressao_str: String com a express√£o (ex: 'x**2 + y**2')\n",
        "        ponto: Tupla (x, y) onde calcular o gradiente\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (gradiente_array, magnitude, dire√ß√£o_unit√°ria)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Passo 1: Definir s√≠mbolos\n",
        "    x, y = sp.symbols('x y')\n",
        "    \n",
        "    # Passo 2: Converter string para express√£o simb√≥lica\n",
        "    # COMPLETE AQUI:\n",
        "    f = sp.sympify(expressao_str)\n",
        "    \n",
        "    # Passo 3: Calcular derivadas parciais\n",
        "    # COMPLETE AQUI:\n",
        "    df_dx = sp.diff(f, x)\n",
        "    df_dy = sp.diff(f, y)\n",
        "    \n",
        "    # Passo 4: Avaliar no ponto dado\n",
        "    px, py = ponto\n",
        "    # COMPLETE AQUI:\n",
        "    grad_x = float(df_dx.subs([(x, px), (y, py)]))\n",
        "    grad_y = float(df_dy.subs([(x, px), (y, py)]))\n",
        "    \n",
        "    gradiente = np.array([grad_x, grad_y])\n",
        "    \n",
        "    # Passo 5: Calcular magnitude e dire√ß√£o unit√°ria\n",
        "    # COMPLETE AQUI:\n",
        "    magnitude = np.linalg.norm(gradiente)\n",
        "    if magnitude > 0:\n",
        "        direcao_unitaria = gradiente / magnitude\n",
        "    else:\n",
        "        direcao_unitaria = np.array([0, 0])\n",
        "    \n",
        "    return gradiente, magnitude, direcao_unitaria\n",
        "\n",
        "# Testando a fun√ß√£o\n",
        "print(\"üßÆ Testando a Calculadora de Gradientes\\n\")\n",
        "\n",
        "testes = [\n",
        "    ('x**2 + y**2', (1, 1)),\n",
        "    ('x*y + x**2', (2, 3)),\n",
        "    ('sin(x) + cos(y)', (0, 0)),\n",
        "    ('exp(x + y)', (0, 0))\n",
        "]\n",
        "\n",
        "for expressao, ponto in testes:\n",
        "    grad, mag, direcao = calculadora_gradiente(expressao, ponto)\n",
        "    \n",
        "    print(f\"üìç f(x,y) = {expressao}\")\n",
        "    print(f\"   Ponto: {ponto}\")\n",
        "    print(f\"   Gradiente: [{grad[0]:.4f}, {grad[1]:.4f}]\")\n",
        "    print(f\"   Magnitude: {mag:.4f}\")\n",
        "    print(f\"   Dire√ß√£o unit√°ria: [{direcao[0]:.4f}, {direcao[1]:.4f}]\")\n",
        "    print()\n",
        "\n",
        "print(\"‚úÖ Exerc√≠cio conclu√≠do! Parab√©ns!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Conex√£o com o Pr√≥ximo M√≥dulo: Gradiente Descendente\n\nAgora que voc√™s dominaram o gradiente, vamos ver como usar ele pra **otimizar** fun√ß√µes!\n\n### A Ideia Central\n\nSe o gradiente aponta pra onde a fun√ß√£o **cresce mais**, ent√£o:\n- **-‚àáf** aponta pra onde a fun√ß√£o **decresce mais**!\n- Seguindo a dire√ß√£o **-‚àáf**, chegamos no m√≠nimo!\n\n### Algoritmo do Gradiente Descendente (Preview)\n\n```\n1. Comece num ponto inicial Œ∏‚ÇÄ\n2. Calcule o gradiente ‚àáf(Œ∏)\n3. Atualize: Œ∏ = Œ∏ - Œ±‚àáf(Œ∏)  (Œ± = learning rate)\n4. Repita at√© convergir\n```\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-10_img_04.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview do Gradiente Descendente\n",
        "def preview_gradiente_descendente():\n",
        "    \"\"\"Uma pr√©via do que veremos no pr√≥ximo m√≥dulo\"\"\"\n",
        "    \n",
        "    # Fun√ß√£o simples: f(x,y) = (x-1)¬≤ + (y-2)¬≤\n",
        "    def f_exemplo(x, y):\n",
        "        return (x - 1)**2 + (y - 2)**2\n",
        "    \n",
        "    def grad_f_exemplo(x, y):\n",
        "        return np.array([2*(x - 1), 2*(y - 2)])\n",
        "    \n",
        "    # Par√¢metros\n",
        "    ponto_inicial = np.array([-2.0, 4.0])\n",
        "    learning_rate = 0.1\n",
        "    num_iteracoes = 20\n",
        "    \n",
        "    # Executando o algoritmo\n",
        "    trajetoria = [ponto_inicial.copy()]\n",
        "    ponto_atual = ponto_inicial.copy()\n",
        "    \n",
        "    print(\"üöÄ Preview do Gradiente Descendente\\n\")\n",
        "    print(f\"Objetivo: Minimizar f(x,y) = (x-1)¬≤ + (y-2)¬≤\")\n",
        "    print(f\"Ponto inicial: {ponto_inicial}\")\n",
        "    print(f\"Learning rate: {learning_rate}\\n\")\n",
        "    \n",
        "    for i in range(num_iteracoes):\n",
        "        # Calcular gradiente\n",
        "        grad = grad_f_exemplo(ponto_atual[0], ponto_atual[1])\n",
        "        \n",
        "        # Atualizar posi√ß√£o (passo do gradiente descendente)\n",
        "        ponto_atual = ponto_atual - learning_rate * grad\n",
        "        trajetoria.append(ponto_atual.copy())\n",
        "        \n",
        "        if i < 5 or i % 5 == 0:  # Mostrar apenas algumas itera√ß√µes\n",
        "            valor_atual = f_exemplo(ponto_atual[0], ponto_atual[1])\n",
        "            print(f\"Itera√ß√£o {i+1:2d}: ({ponto_atual[0]:.3f}, {ponto_atual[1]:.3f}) | f = {valor_atual:.6f}\")\n",
        "    \n",
        "    trajetoria = np.array(trajetoria)\n",
        "    \n",
        "    # Visualiza√ß√£o\n",
        "    x_range = np.linspace(-3, 3, 100)\n",
        "    y_range = np.linspace(-1, 5, 100)\n",
        "    X, Y = np.meshgrid(x_range, y_range)\n",
        "    Z = f_exemplo(X, Y)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    \n",
        "    # Curvas de n√≠vel\n",
        "    contour = plt.contour(X, Y, Z, levels=20, alpha=0.6)\n",
        "    plt.clabel(contour, inline=True, fontsize=8)\n",
        "    \n",
        "    # Trajet√≥ria do gradiente descendente\n",
        "    plt.plot(trajetoria[:, 0], trajetoria[:, 1], 'ro-', \n",
        "             linewidth=2, markersize=6, alpha=0.8, label='Trajet√≥ria')\n",
        "    \n",
        "    # Ponto inicial e final\n",
        "    plt.plot(trajetoria[0, 0], trajetoria[0, 1], 'go', \n",
        "             markersize=10, label='In√≠cio')\n",
        "    plt.plot(trajetoria[-1, 0], trajetoria[-1, 1], 'bs', \n",
        "             markersize=10, label='Final')\n",
        "    \n",
        "    # M√≠nimo te√≥rico\n",
        "    plt.plot(1, 2, 'k*', markersize=15, label='M√≠nimo Verdadeiro')\n",
        "    \n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.title('Preview: Gradiente Descendente em A√ß√£o!')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.axis('equal')\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nüéØ Resultado final: ({trajetoria[-1, 0]:.6f}, {trajetoria[-1, 1]:.6f})\")\n",
        "    print(f\"üéØ M√≠nimo te√≥rico: (1.000000, 2.000000)\")\n",
        "    print(f\"üéØ Erro final: {np.linalg.norm(trajetoria[-1] - np.array([1, 2])):.6f}\")\n",
        "    \n",
        "    return trajetoria\n",
        "\n",
        "trajetoria_final = preview_gradiente_descendente()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° Algoritmo: Do Gradiente √† Otimiza√ß√£o\n\nVamos ver o fluxo completo de como o gradiente nos leva √† otimiza√ß√£o:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\ngraph TD\n    A[Fun√ß√£o de Custo J(Œ∏)] --> B[Ponto Inicial Œ∏‚ÇÄ]\n    B --> C[Calcular ‚àáJ(Œ∏)]\n    C --> D{|‚àáJ| < Œµ?}\n    D -->|N√£o| E[Œ∏ = Œ∏ - Œ±‚àáJ(Œ∏)]\n    E --> C\n    D -->|Sim| F[Convergiu! Œ∏* encontrado]\n    F --> G[Par√¢metros Otimizados]\n    G --> H[Modelo Treinado]\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Exerc√≠cio Pr√°tico 2: Desafio da Otimiza√ß√£o\n\nAgora √© sua vez! Vamos implementar um mini-otimizador usando gradientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO DESAFIO: Implementar um otimizador simples\n",
        "def mini_otimizador(func, grad_func, ponto_inicial, learning_rate=0.01, max_iter=100, tolerancia=1e-6):\n",
        "    \"\"\"\n",
        "    Mini-otimizador usando gradiente descendente\n",
        "    \n",
        "    Args:\n",
        "        func: Fun√ß√£o a ser minimizada\n",
        "        grad_func: Fun√ß√£o que calcula o gradiente\n",
        "        ponto_inicial: Ponto de partida [x, y]\n",
        "        learning_rate: Taxa de aprendizado\n",
        "        max_iter: M√°ximo de itera√ß√µes\n",
        "        tolerancia: Crit√©rio de parada\n",
        "    \n",
        "    Returns:\n",
        "        dict: Resultados da otimiza√ß√£o\n",
        "    \"\"\"\n",
        "    \n",
        "    ponto = np.array(ponto_inicial, dtype=float)\n",
        "    historico_pontos = [ponto.copy()]\n",
        "    historico_valores = [func(ponto[0], ponto[1])]\n",
        "    historico_gradientes = []\n",
        "    \n",
        "    print(f\"üöÄ Iniciando otimiza√ß√£o...\")\n",
        "    print(f\"   Ponto inicial: [{ponto[0]:.4f}, {ponto[1]:.4f}]\")\n",
        "    print(f\"   Valor inicial: {historico_valores[0]:.6f}\\n\")\n",
        "    \n",
        "    for i in range(max_iter):\n",
        "        # COMPLETE AQUI: Calcule o gradiente\n",
        "        grad = grad_func(ponto[0], ponto[1])\n",
        "        historico_gradientes.append(grad.copy())\n",
        "        \n",
        "        # COMPLETE AQUI: Verifique crit√©rio de parada\n",
        "        grad_magnitude = np.linalg.norm(grad)\n",
        "        if grad_magnitude < tolerancia:\n",
        "            print(f\"‚úÖ Convergiu na itera√ß√£o {i}! |‚àáf| = {grad_magnitude:.8f}\")\n",
        "            break\n",
        "        \n",
        "        # COMPLETE AQUI: Fa√ßa o passo do gradiente descendente\n",
        "        ponto = ponto - learning_rate * grad\n",
        "        \n",
        "        # Registrar progresso\n",
        "        valor_atual = func(ponto[0], ponto[1])\n",
        "        historico_pontos.append(ponto.copy())\n",
        "        historico_valores.append(valor_atual)\n",
        "        \n",
        "        # Mostrar progresso a cada 10 itera√ß√µes\n",
        "        if i % 10 == 0 or i < 5:\n",
        "            print(f\"Iter {i:3d}: [{ponto[0]:8.4f}, {ponto[1]:8.4f}] | f = {valor_atual:10.6f} | |‚àáf| = {grad_magnitude:.6f}\")\n",
        "    \n",
        "    return {\n",
        "        'ponto_final': ponto,\n",
        "        'valor_final': historico_valores[-1],\n",
        "        'iteracoes': len(historico_pontos) - 1,\n",
        "        'historico_pontos': np.array(historico_pontos),\n",
        "        'historico_valores': np.array(historico_valores),\n",
        "        'convergiu': grad_magnitude < tolerancia\n",
        "    }\n",
        "\n",
        "# Testando com a fun√ß√£o de Rosenbrock (desafio cl√°ssico!)\n",
        "def rosenbrock(x, y):\n",
        "    \"\"\"Fun√ß√£o de Rosenbrock: f(x,y) = (1-x)¬≤ + 100(y-x¬≤)¬≤\"\"\"\n",
        "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
        "\n",
        "def grad_rosenbrock(x, y):\n",
        "    \"\"\"Gradiente da fun√ß√£o de Rosenbrock\"\"\"\n",
        "    df_dx = -2*(1 - x) - 400*x*(y - x**2)\n",
        "    df_dy = 200*(y - x**2)\n",
        "    return np.array([df_dx, df_dy])\n",
        "\n",
        "print(\"üåü DESAFIO: Otimizando a Fun√ß√£o de Rosenbrock\")\n",
        "print(\"   M√≠nimo conhecido: (1, 1) com f(1,1) = 0\")\n",
        "print(\"   Esta √© uma fun√ß√£o notoriamente dif√≠cil de otimizar!\\n\")\n",
        "\n",
        "resultado = mini_otimizador(\n",
        "    rosenbrock, \n",
        "    grad_rosenbrock, \n",
        "    ponto_inicial=[-1.2, 1.0],  # Ponto inicial padr√£o para Rosenbrock\n",
        "    learning_rate=0.001,  # Learning rate pequeno para estabilidade\n",
        "    max_iter=1000,\n",
        "    tolerancia=1e-5\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä RESULTADO FINAL:\")\n",
        "print(f\"   Ponto encontrado: [{resultado['ponto_final'][0]:.6f}, {resultado['ponto_final'][1]:.6f}]\")\n",
        "print(f\"   Valor final: {resultado['valor_final']:.8f}\")\n",
        "print(f\"   Itera√ß√µes: {resultado['iteracoes']}\")\n",
        "print(f\"   Convergiu: {resultado['convergiu']}\")\n",
        "print(f\"   Erro do m√≠nimo: {np.linalg.norm(resultado['ponto_final'] - np.array([1, 1])):.6f}\")\n",
        "\n",
        "# Visualizando o resultado\n",
        "if len(resultado['historico_pontos']) > 1:\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Plot 1: Trajet√≥ria\n",
        "    plt.subplot(1, 2, 1)\n",
        "    x_range = np.linspace(-1.5, 1.5, 100)\n",
        "    y_range = np.linspace(-0.5, 1.5, 100)\n",
        "    X, Y = np.meshgrid(x_range, y_range)\n",
        "    Z = rosenbrock(X, Y)\n",
        "    \n",
        "    plt.contour(X, Y, np.log10(Z + 1), levels=20, alpha=0.6)\n",
        "    \n",
        "    trajetoria = resultado['historico_pontos']\n",
        "    plt.plot(trajetoria[:, 0], trajetoria[:, 1], 'ro-', alpha=0.7, markersize=3)\n",
        "    plt.plot(trajetoria[0, 0], trajetoria[0, 1], 'go', markersize=8, label='In√≠cio')\n",
        "    plt.plot(trajetoria[-1, 0], trajetoria[-1, 1], 'bs', markersize=8, label='Final')\n",
        "    plt.plot(1, 1, 'k*', markersize=12, label='M√≠nimo Verdadeiro')\n",
        "    \n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.title('Trajet√≥ria na Fun√ß√£o de Rosenbrock')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Converg√™ncia\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.semilogy(resultado['historico_valores'])\n",
        "    plt.xlabel('Itera√ß√£o')\n",
        "    plt.ylabel('Valor da Fun√ß√£o (log)')\n",
        "    plt.title('Converg√™ncia')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nüéâ Parab√©ns! Voc√™ implementou seu primeiro otimizador!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Resumo: O Que Aprendemos Hoje\n\nLiiindo! Chegamos ao fim do m√≥dulo mais importante at√© agora! üéâ\n\n### üéØ Conceitos Principais\n\n1. **Derivadas Parciais**\n   - $\\frac{\\partial f}{\\partial x}$: taxa de varia√ß√£o \"congelando\" outras vari√°veis\n   - Representam inclina√ß√£o em dire√ß√µes espec√≠ficas\n\n2. **Vetor Gradiente**\n   - $\\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}]$\n   - **Sempre aponta na dire√ß√£o de maior crescimento**\n   - Perpendicular √†s curvas de n√≠vel\n\n3. **Propriedades do Gradiente**\n   - Magnitude = m√°xima taxa de varia√ß√£o\n   - $\\nabla f = 0$ = ponto cr√≠tico\n   - $-\\nabla f$ = dire√ß√£o de maior decrescimento\n\n### üîó Conex√µes com IA\n\n- **Fun√ß√µes de Custo**: Paisagens multidimensionais para otimizar\n- **Backpropagation**: Usa regra da cadeia + gradientes\n- **Gradiente Descendente**: Segue $-\\nabla J$ para minimizar custo\n- **Otimiza√ß√£o**: Base de todo aprendizado supervisionado\n\n### üí° Dicas Importantes\n\n1. **Interpreta√ß√£o Geom√©trica**: Gradiente = \"GPS\" matem√°tico\n2. **Visualiza√ß√£o**: Sempre desenhe pra entender\n3. **Implementa√ß√£o**: NumPy + SymPy s√£o seus amigos\n4. **Debugging**: Verifique se $\\nabla f = 0$ nos m√≠nimos conhecidos\n\n**Dica Final do Pedro:** O gradiente √© sua b√∫ssola no mundo da IA. Entendendo ele, voc√™ entende como as m√°quinas \"aprendem\" a encontrar solu√ß√µes √≥timas!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-10_img_05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Prepara√ß√£o para o Pr√≥ximo M√≥dulo\n\nNo **M√≥dulo 11**, vamos finalmente implementar o algoritmo completo do **Gradiente Descendente**!\n\n### O Que Vem Por A√≠\n\n1. **Algoritmo Completo**: Implementa√ß√£o passo a passo\n2. **Learning Rate**: Como escolher a taxa de aprendizado\n3. **Crit√©rios de Parada**: Quando parar a otimiza√ß√£o\n4. **Problemas Comuns**: M√≠nimos locais, overshoot, etc.\n5. **Aplica√ß√£o Real**: Treinando uma rede neural simples\n\n### Para Casa üìù\n\n1. Pratique calculando gradientes √† m√£o\n2. Experimente com diferentes fun√ß√µes na calculadora\n3. Tente visualizar gradientes de fun√ß√µes que voc√™ criar\n4. Pense: \"Como o gradiente me ajudaria a descer uma montanha?\"\n\n---\n\n**Bora pro pr√≥ximo m√≥dulo dominar a otimiza√ß√£o!** üî•\n\n*Pedro Nunes Guth - Expert em IA & Matem√°tica Descomplicada*"
      ]
    }
  ]
}