{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Gradiente Descendente 2.0: Turbinando Nossa Descida na Montanha do Erro!\n\n## Pedro Nunes Guth - M√≥dulo 12/12: C√°lculo para IA\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-12_img_01.png)\n\nFala pessoal! üéâ Chegamos no √∫ltimo m√≥dulo do nosso curso de C√°lculo para IA! Se voc√™ chegou at√© aqui, j√° entende que o gradiente descendente √© tipo aquela b√∫ssola que nos guia pela montanha do erro. Mas t√°, e se eu te disser que existem **vers√µes turbinadas** dessa b√∫ssola?\n\nNo m√≥dulo anterior, aprendemos o gradiente descendente cl√°ssico - aquele marombeiro que pega TODOS os dados de uma vez pra calcular o gradiente. Hoje vamos conhecer os primos dele:\n\n- **SGD (Stochastic Gradient Descent)**: O cara apressadinho que usa s√≥ UM exemplo por vez\n- **Mini-batch**: O meio termo inteligente\n- **Adam**: O algoritmo \"espert√£o\" que se adapta sozinho\n- **E muito mais!**\n\nBora descer essa montanha com **ESTILO**! üèîÔ∏è‚ö°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports essenciais para nossa jornada!\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_regression, make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√£o para gr√°ficos mais bonitos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Seed para reprodutibilidade\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"üéØ Ambiente preparado! Bora come√ßar nossa aventura!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ 1. Recapitulando: O Gradiente Descendente Cl√°ssico\n\nAntes de partir para as varia√ß√µes, vamos relembrar nosso velho amigo do m√≥dulo anterior:\n\n### A F√≥rmula Sagrada:\n\n$$\\theta_{novo} = \\theta_{atual} - \\alpha \\nabla J(\\theta)$$\n\nOnde:\n- $\\theta$: nossos par√¢metros (pesos do modelo)\n- $\\alpha$: taxa de aprendizado (learning rate)\n- $\\nabla J(\\theta)$: o gradiente da fun√ß√£o de custo\n\n### O Problema do Gradiente Cl√°ssico:\n\nNo gradiente descendente **batch** (cl√°ssico), calculamos o gradiente usando **TODOS** os exemplos:\n\n$$\\nabla J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\nabla J_i(\\theta)$$\n\nT√°, mas imagina que voc√™ tem 1 milh√£o de exemplos! Vai calcular o gradiente com TODOS eles a cada itera√ß√£o? √â como querer contar todos os gr√£os de areia da praia antes de dar um passo!\n\n\n\n**Dica do Pedro**: O gradiente batch √© preciso, mas LENTO. √â tipo aquele amigo que demora 2 horas pra escolher o que vai comer no restaurante! üòÖ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um dataset para demonstrar os conceitos\n",
        "def criar_dataset_exemplo(n_samples=1000, noise=0.1):\n",
        "    \"\"\"\n",
        "    Cria um dataset simples para regress√£o linear\n",
        "    y = 3x + 2 + ru√≠do\n",
        "    \"\"\"\n",
        "    X = np.random.randn(n_samples, 1)\n",
        "    y = 3 * X.flatten() + 2 + noise * np.random.randn(n_samples)\n",
        "    return X, y\n",
        "\n",
        "# Fun√ß√£o de custo MSE (Mean Squared Error)\n",
        "def mse_loss(y_true, y_pred):\n",
        "    \"\"\"Calcula o erro quadr√°tico m√©dio\"\"\"\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Predi√ß√£o linear simples\n",
        "def predict(X, w, b):\n",
        "    \"\"\"Faz predi√ß√£o: y = w*x + b\"\"\"\n",
        "    return X.flatten() * w + b\n",
        "\n",
        "# Implementa√ß√£o do Gradiente Descendente Batch (cl√°ssico)\n",
        "def gradient_descent_batch(X, y, learning_rate=0.01, epochs=100):\n",
        "    \"\"\"Gradiente Descendente usando TODOS os dados por itera√ß√£o\"\"\"\n",
        "    # Inicializa√ß√£o dos par√¢metros\n",
        "    w = np.random.randn()\n",
        "    b = np.random.randn()\n",
        "    \n",
        "    # Hist√≥rico para visualiza√ß√£o\n",
        "    history = {'loss': [], 'w': [], 'b': []}\n",
        "    \n",
        "    m = len(y)  # n√∫mero de exemplos\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Predi√ß√£o usando TODOS os dados\n",
        "        y_pred = predict(X, w, b)\n",
        "        \n",
        "        # C√°lculo do erro\n",
        "        loss = mse_loss(y, y_pred)\n",
        "        \n",
        "        # C√°lculo dos gradientes usando TODOS os exemplos\n",
        "        dw = -(2/m) * np.sum((y - y_pred) * X.flatten())\n",
        "        db = -(2/m) * np.sum(y - y_pred)\n",
        "        \n",
        "        # Atualiza√ß√£o dos par√¢metros\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "        \n",
        "        # Salvar hist√≥rico\n",
        "        history['loss'].append(loss)\n",
        "        history['w'].append(w)\n",
        "        history['b'].append(b)\n",
        "    \n",
        "    return w, b, history\n",
        "\n",
        "# Testando o gradiente batch\n",
        "X, y = criar_dataset_exemplo(1000)\n",
        "w_batch, b_batch, hist_batch = gradient_descent_batch(X, y, learning_rate=0.1, epochs=50)\n",
        "\n",
        "print(f\"üéØ Gradiente Batch - Resultado final:\")\n",
        "print(f\"   Peso (w): {w_batch:.3f} (esperado: ~3.0)\")\n",
        "print(f\"   Bias (b): {b_batch:.3f} (esperado: ~2.0)\")\nprint(f\"   Loss final: {hist_batch['loss'][-1]:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèÉ‚Äç‚ôÇÔ∏è 2. SGD: O Gradiente Descendente Apressadinho\n\nAgora vem o **Stochastic Gradient Descent (SGD)**! Esse cara √© tipo aquele amigo que decide na primeira op√ß√£o do card√°pio - ele usa apenas **UM** exemplo por vez para calcular o gradiente!\n\n### A Matem√°tica do SGD:\n\nEm vez de usar todos os exemplos:\n$$\\nabla J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\nabla J_i(\\theta)$$\n\nO SGD usa apenas um exemplo aleat√≥rio por itera√ß√£o:\n$$\\nabla J(\\theta) \\approx \\nabla J_i(\\theta)$$\n\n### Vantagens do SGD:\n1. **Muito mais r√°pido**: Uma itera√ß√£o √© super r√°pida!\n2. **Usa menos mem√≥ria**: Processa um exemplo por vez\n3. **Pode escapar de m√≠nimos locais**: O \"ru√≠do\" ajuda!\n\n### Desvantagens:\n1. **Converg√™ncia mais \"bagun√ßada\"**: O caminho n√£o √© suave\n2. **Pode oscilar muito**: Como um marinheiro b√™bado descendo a montanha! üç∫\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-12_img_03.png)\n\n**Dica do Pedro**: O SGD √© como dirigir no tr√¢nsito do Rio - voc√™ vai chegando no destino, mas fazendo umas curvas malucas pelo caminho! üöóüí®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementa√ß√£o do SGD (Stochastic Gradient Descent)\n",
        "def stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=100):\n",
        "    \"\"\"SGD - usa apenas UM exemplo por itera√ß√£o\"\"\"\n",
        "    # Inicializa√ß√£o\n",
        "    w = np.random.randn()\n",
        "    b = np.random.randn()\n",
        "    \n",
        "    # Hist√≥rico\n",
        "    history = {'loss': [], 'w': [], 'b': []}\n",
        "    \n",
        "    m = len(y)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Embaralha os √≠ndices para cada √©poca\n",
        "        indices = np.random.permutation(m)\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        \n",
        "        # Para cada exemplo individual\n",
        "        for i in indices:\n",
        "            # Pega apenas UM exemplo\n",
        "            xi = X[i:i+1]\n",
        "            yi = y[i]\n",
        "            \n",
        "            # Predi√ß√£o para esse exemplo\n",
        "            y_pred_i = predict(xi, w, b)\n",
        "            \n",
        "            # Gradientes baseados em apenas UM exemplo\n",
        "            error = yi - y_pred_i\n",
        "            dw = -2 * error * xi.flatten()[0]\n",
        "            db = -2 * error\n",
        "            \n",
        "            # Atualiza√ß√£o dos par√¢metros\n",
        "            w = w - learning_rate * dw\n",
        "            b = b - learning_rate * db\n",
        "            \n",
        "            epoch_loss += (error ** 2)\n",
        "        \n",
        "        # Loss m√©dia da √©poca\n",
        "        avg_loss = epoch_loss / m\n",
        "        history['loss'].append(avg_loss)\n",
        "        history['w'].append(w)\n",
        "        history['b'].append(b)\n",
        "    \n",
        "    return w, b, history\n",
        "\n",
        "# Testando SGD\n",
        "np.random.seed(42)  # Para compara√ß√£o justa\n",
        "w_sgd, b_sgd, hist_sgd = stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=50)\n",
        "\n",
        "print(f\"‚ö° SGD - Resultado final:\")\n",
        "print(f\"   Peso (w): {w_sgd:.3f} (esperado: ~3.0)\")\n",
        "print(f\"   Bias (b): {b_sgd:.3f} (esperado: ~2.0)\")\n",
        "print(f\"   Loss final: {hist_sgd['loss'][-1]:.6f}\")\n",
        "\n",
        "print(f\"\\nüìä Compara√ß√£o:\")\n",
        "print(f\"   Batch GD Loss: {hist_batch['loss'][-1]:.6f}\")\n",
        "print(f\"   SGD Loss: {hist_sgd['loss'][-1]:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a diferen√ßa entre Batch GD e SGD\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Gr√°fico 1: Converg√™ncia da Loss\n",
        "axes[0,0].plot(hist_batch['loss'], label='Batch GD', linewidth=3, color='blue')\n",
        "axes[0,0].plot(hist_sgd['loss'], label='SGD', linewidth=2, color='red', alpha=0.7)\n",
        "axes[0,0].set_title('üìâ Converg√™ncia da Fun√ß√£o de Custo')\n",
        "axes[0,0].set_xlabel('√âpocas')\n",
        "axes[0,0].set_ylabel('Loss (MSE)')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 2: Evolu√ß√£o do peso W\n",
        "axes[0,1].plot(hist_batch['w'], label='Batch GD', linewidth=3, color='blue')\n",
        "axes[0,1].plot(hist_sgd['w'], label='SGD', linewidth=2, color='red', alpha=0.7)\n",
        "axes[0,1].axhline(y=3.0, color='green', linestyle='--', label='Valor Real (3.0)')\n",
        "axes[0,1].set_title('‚öñÔ∏è Evolu√ß√£o do Peso (w)')\n",
        "axes[0,1].set_xlabel('√âpocas')\n",
        "axes[0,1].set_ylabel('Valor do Peso')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 3: Evolu√ß√£o do bias B\n",
        "axes[1,0].plot(hist_batch['b'], label='Batch GD', linewidth=3, color='blue')\n",
        "axes[1,0].plot(hist_sgd['b'], label='SGD', linewidth=2, color='red', alpha=0.7)\n",
        "axes[1,0].axhline(y=2.0, color='green', linestyle='--', label='Valor Real (2.0)')\n",
        "axes[1,0].set_title('üìè Evolu√ß√£o do Bias (b)')\n",
        "axes[1,0].set_xlabel('√âpocas')\n",
        "axes[1,0].set_ylabel('Valor do Bias')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 4: Trajet√≥ria no espa√ßo de par√¢metros\n",
        "axes[1,1].plot(hist_batch['w'], hist_batch['b'], 'b-', linewidth=3, label='Batch GD', marker='o', markersize=4)\n",
        "axes[1,1].plot(hist_sgd['w'][:20], hist_sgd['b'][:20], 'r-', linewidth=2, label='SGD (primeiras 20)', alpha=0.7, marker='s', markersize=3)\n",
        "axes[1,1].plot(3.0, 2.0, 'g*', markersize=15, label='√ìtimo Real')\n",
        "axes[1,1].set_title('üéØ Trajet√≥ria no Espa√ßo de Par√¢metros')\n",
        "axes[1,1].set_xlabel('Peso (w)')\n",
        "axes[1,1].set_ylabel('Bias (b)')\n",
        "axes[1,1].legend()\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üîç Observa√ß√µes:\")\n",
        "print(\"   ‚Ä¢ Batch GD: Converg√™ncia suave e est√°vel (linha azul)\")\n",
        "print(\"   ‚Ä¢ SGD: Converg√™ncia mais 'bagun√ßada', mas pode ser mais r√°pida (linha vermelha)\")\n",
        "print(\"   ‚Ä¢ SGD oscila mais, mas consegue escapar de m√≠nimos locais!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ 3. Mini-Batch: O Meio-Termo Inteligente\n\nT√°, mas e se a gente pegasse o melhor dos dois mundos? O **Mini-Batch Gradient Descent** √© exatamente isso!\n\nEm vez de usar:\n- **Todos** os exemplos (Batch) üêå\n- **Apenas um** exemplo (SGD) üèÉ‚Äç‚ôÇÔ∏èüí®\n\nUsamos um **pequeno grupo** de exemplos por vez!\n\n### A Matem√°tica do Mini-Batch:\n\nPara um mini-batch de tamanho $B$:\n$$\\nabla J(\\theta) \\approx \\frac{1}{B} \\sum_{i \\in \\text{mini-batch}} \\nabla J_i(\\theta)$$\n\n### Tamanhos T√≠picos de Mini-Batch:\n- **32**: Cl√°ssico para problemas pequenos\n- **64**: Muito popular\n- **128, 256**: Para datasets maiores\n- **512+**: Para datasets gigantes\n\n### Vantagens do Mini-Batch:\n1. **Mais est√°vel que SGD**: Menos oscila√ß√£o\n2. **Mais r√°pido que Batch**: N√£o precisa de todos os dados\n3. **Vetoriza√ß√£o eficiente**: GPUs adoram mini-batches!\n4. **Melhor generaliza√ß√£o**: Ru√≠do controlado ajuda\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-12_img_04.png)\n\n**Dica do Pedro**: Mini-batch √© como pedir uma por√ß√£o individual no restaurante em vez de um prato pra fam√≠lia toda, mas tamb√©m n√£o √© s√≥ um salgadinho! √â o equil√≠brio perfeito! üçΩÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementa√ß√£o do Mini-Batch Gradient Descent\n",
        "def mini_batch_gradient_descent(X, y, batch_size=32, learning_rate=0.01, epochs=100):\n",
        "    \"\"\"Mini-Batch GD - usa pequenos grupos de exemplos\"\"\"\n",
        "    # Inicializa√ß√£o\n",
        "    w = np.random.randn()\n",
        "    b = np.random.randn()\n",
        "    \n",
        "    # Hist√≥rico\n",
        "    history = {'loss': [], 'w': [], 'b': []}\n",
        "    \n",
        "    m = len(y)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Embaralha os dados\n",
        "        indices = np.random.permutation(m)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        # Processa em mini-batches\n",
        "        for i in range(0, m, batch_size):\n",
        "            # Pega o mini-batch\n",
        "            end_idx = min(i + batch_size, m)\n",
        "            X_batch = X_shuffled[i:end_idx]\n",
        "            y_batch = y_shuffled[i:end_idx]\n",
        "            \n",
        "            # Predi√ß√£o para o mini-batch\n",
        "            y_pred_batch = predict(X_batch, w, b)\n",
        "            \n",
        "            # Loss do mini-batch\n",
        "            batch_loss = mse_loss(y_batch, y_pred_batch)\n",
        "            epoch_loss += batch_loss\n",
        "            \n",
        "            # Gradientes do mini-batch\n",
        "            batch_m = len(y_batch)\n",
        "            dw = -(2/batch_m) * np.sum((y_batch - y_pred_batch) * X_batch.flatten())\n",
        "            db = -(2/batch_m) * np.sum(y_batch - y_pred_batch)\n",
        "            \n",
        "            # Atualiza√ß√£o dos par√¢metros\n",
        "            w = w - learning_rate * dw\n",
        "            b = b - learning_rate * db\n",
        "            \n",
        "            num_batches += 1\n",
        "        \n",
        "        # Loss m√©dia da √©poca\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        history['loss'].append(avg_loss)\n",
        "        history['w'].append(w)\n",
        "        history['b'].append(b)\n",
        "    \n",
        "    return w, b, history\n",
        "\n",
        "# Testando diferentes tamanhos de mini-batch\n",
        "batch_sizes = [8, 32, 128]\n",
        "results = {}\n",
        "\n",
        "print(\"üß™ Testando diferentes tamanhos de mini-batch...\\n\")\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    np.random.seed(42)  # Para compara√ß√£o justa\n",
        "    w_mb, b_mb, hist_mb = mini_batch_gradient_descent(\n",
        "        X, y, batch_size=batch_size, learning_rate=0.05, epochs=50\n",
        "    )\n",
        "    \n",
        "    results[batch_size] = (w_mb, b_mb, hist_mb)\n",
        "    \n",
        "    print(f\"üì¶ Mini-Batch (size={batch_size}):\")\n",
        "    print(f\"   Peso (w): {w_mb:.3f}\")\n",
        "    print(f\"   Bias (b): {b_mb:.3f}\")\n",
        "    print(f\"   Loss final: {hist_mb['loss'][-1]:.6f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparando os diferentes m√©todos\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Gr√°fico 1: Converg√™ncia da Loss\n",
        "axes[0].plot(hist_batch['loss'], label='Batch GD', linewidth=3, color='blue')\n",
        "axes[0].plot(hist_sgd['loss'], label='SGD', linewidth=2, color='red', alpha=0.8)\n",
        "\n",
        "colors = ['green', 'orange', 'purple']\n",
        "for i, batch_size in enumerate(batch_sizes):\n",
        "    _, _, hist = results[batch_size]\n",
        "    axes[0].plot(hist['loss'], label=f'Mini-Batch ({batch_size})', \n",
        "                linewidth=2, color=colors[i], alpha=0.8)\n",
        "\n",
        "axes[0].set_title('üìà Compara√ß√£o de Converg√™ncia')\n",
        "axes[0].set_xlabel('√âpocas')\n",
        "axes[0].set_ylabel('Loss (MSE)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_yscale('log')  # Escala logar√≠tmica para melhor visualiza√ß√£o\n",
        "\n",
        "# Gr√°fico 2: Estabilidade (√∫ltimas 10 √©pocas)\n",
        "last_n = 10\n",
        "batch_std = np.std(hist_batch['loss'][-last_n:])\n",
        "sgd_std = np.std(hist_sgd['loss'][-last_n:])\n",
        "\n",
        "methods = ['Batch GD', 'SGD']\n",
        "stds = [batch_std, sgd_std]\n",
        "colors_bar = ['blue', 'red']\n",
        "\n",
        "for i, batch_size in enumerate(batch_sizes):\n",
        "    _, _, hist = results[batch_size]\n",
        "    mb_std = np.std(hist['loss'][-last_n:])\n",
        "    methods.append(f'Mini-Batch ({batch_size})')\n",
        "    stds.append(mb_std)\n",
        "    colors_bar.append(colors[i])\n",
        "\n",
        "bars = axes[1].bar(methods, stds, color=colors_bar, alpha=0.7)\n",
        "axes[1].set_title('üìä Estabilidade (Desvio Padr√£o das √öltimas 10 √âpocas)')\n",
        "axes[1].set_ylabel('Desvio Padr√£o da Loss')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Adiciona valores nas barras\n",
        "for bar, std in zip(bars, stds):\n",
        "    height = bar.get_height()\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "                f'{std:.6f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ An√°lise dos Resultados:\")\n",
        "print(f\"   ‚Ä¢ Batch GD: Mais est√°vel (std={batch_std:.6f}), mas pode ser mais lento\")\n",
        "print(f\"   ‚Ä¢ SGD: Menos est√°vel (std={sgd_std:.6f}), mas mais r√°pido\")\n",
        "print(\"   ‚Ä¢ Mini-Batches: Equilibram velocidade e estabilidade!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° 4. A Taxa de Aprendizado: O Acelerador da Nossa Descida\n\nAgora vamos falar de um dos **par√¢metros mais cr√≠ticos** de qualquer algoritmo de otimiza√ß√£o: a **taxa de aprendizado** ($\\alpha$)!\n\n### O Que √â a Taxa de Aprendizado?\n\nLembra da nossa f√≥rmula sagrada?\n$$\\theta_{novo} = \\theta_{atual} - \\alpha \\nabla J(\\theta)$$\n\nO $\\alpha$ √© como o **\"tamanho do passo\"** que damos na descida da montanha!\n\n### Os Tr√™s Cen√°rios Cl√°ssicos:\n\n1. **$\\alpha$ muito pequeno** (ex: 0.001):\n   - ‚úÖ Converg√™ncia est√°vel\n   - ‚ùå **MUITO lento** - como uma lesma subindo no vidro!\n   - ‚ùå Pode travar em plat√¥s\n\n2. **$\\alpha$ muito grande** (ex: 1.0):\n   - ‚úÖ Converg√™ncia r√°pida (no in√≠cio)\n   - ‚ùå **Pode divergir** - como um carro sem freio!\n   - ‚ùå Oscila em volta do m√≠nimo sem nunca chegar\n\n3. **$\\alpha$ na medida certa** (ex: 0.01-0.1):\n   - ‚úÖ Converg√™ncia r√°pida E est√°vel\n   - ‚úÖ Equil√≠brio perfeito! üéØ\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-12_img_05.png)\n\n**Dica do Pedro**: Escolher a taxa de aprendizado √© como ajustar a velocidade no Mario Kart - muito devagar voc√™ perde a corrida, muito r√°pido voc√™ bate na parede! üèéÔ∏èüí®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando diferentes taxas de aprendizado\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
        "lr_results = {}\n",
        "\n",
        "print(\"üéõÔ∏è Testando diferentes taxas de aprendizado...\\n\")\n",
        "\n",
        "for lr in learning_rates:\n",
        "    try:\n",
        "        np.random.seed(42)\n",
        "        w_lr, b_lr, hist_lr = mini_batch_gradient_descent(\n",
        "            X, y, batch_size=32, learning_rate=lr, epochs=100\n",
        "        )\n",
        "        \n",
        "        lr_results[lr] = (w_lr, b_lr, hist_lr)\n",
        "        \n",
        "        # Verifica se convergiu (loss final < 1000 - threshold arbitr√°rio)\n",
        "        converged = hist_lr['loss'][-1] < 1000\n",
        "        status = \"‚úÖ Convergiu\" if converged else \"‚ùå Divergiu\"\n",
        "        \n",
        "        print(f\"üìä Learning Rate = {lr}:\")\n",
        "        print(f\"   Status: {status}\")\n",
        "        print(f\"   Loss final: {hist_lr['loss'][-1]:.6f}\")\n",
        "        print(f\"   Peso final: {w_lr:.3f}\")\n",
        "        print(f\"   Bias final: {b_lr:.3f}\\n\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"üí• Learning Rate = {lr}: EXPLODIU! (Overflow)\\n\")\n",
        "        lr_results[lr] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando o efeito das diferentes taxas de aprendizado\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Cores para cada learning rate\n",
        "colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
        "\n",
        "# Gr√°fico 1: Converg√™ncia da Loss (escala normal)\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    if lr_results[lr] is not None:\n",
        "        w, b, hist = lr_results[lr]\n",
        "        # Plota apenas se a loss n√£o explodiu\n",
        "        if max(hist['loss']) < 1000:\n",
        "            axes[0,0].plot(hist['loss'], label=f'LR = {lr}', \n",
        "                          color=colors[i], linewidth=2)\n",
        "\n",
        "axes[0,0].set_title('üìâ Converg√™ncia da Loss (Escala Normal)')\n",
        "axes[0,0].set_xlabel('√âpocas')\n",
        "axes[0,0].set_ylabel('Loss (MSE)')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 2: Converg√™ncia da Loss (escala log)\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    if lr_results[lr] is not None:\n",
        "        w, b, hist = lr_results[lr]\n",
        "        if max(hist['loss']) < 1000:\n",
        "            axes[0,1].semilogy(hist['loss'], label=f'LR = {lr}', \n",
        "                              color=colors[i], linewidth=2)\n",
        "\n",
        "axes[0,1].set_title('üìâ Converg√™ncia da Loss (Escala Log)')\n",
        "axes[0,1].set_xlabel('√âpocas')\n",
        "axes[0,1].set_ylabel('Loss (MSE) - Log Scale')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 3: Evolu√ß√£o dos pesos\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    if lr_results[lr] is not None:\n",
        "        w, b, hist = lr_results[lr]\n",
        "        if max(hist['loss']) < 1000:\n",
        "            axes[1,0].plot(hist['w'], label=f'LR = {lr}', \n",
        "                          color=colors[i], linewidth=2)\n",
        "\n",
        "axes[1,0].axhline(y=3.0, color='black', linestyle='--', alpha=0.5, label='Valor Real')\n",
        "axes[1,0].set_title('‚öñÔ∏è Evolu√ß√£o do Peso (w)')\n",
        "axes[1,0].set_xlabel('√âpocas')\n",
        "axes[1,0].set_ylabel('Valor do Peso')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 4: An√°lise de estabilidade (√∫ltimas 20 √©pocas)\n",
        "stable_lrs = []\n",
        "volatilities = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    if lr_results[lr] is not None:\n",
        "        w, b, hist = lr_results[lr]\n",
        "        if max(hist['loss']) < 1000:\n",
        "            # Calcular volatilidade das √∫ltimas 20 √©pocas\n",
        "            last_losses = hist['loss'][-20:]\n",
        "            volatility = np.std(last_losses)\n",
        "            stable_lrs.append(lr)\n",
        "            volatilities.append(volatility)\n",
        "\n",
        "bars = axes[1,1].bar([str(lr) for lr in stable_lrs], volatilities, \n",
        "                    color=[colors[learning_rates.index(lr)] for lr in stable_lrs], alpha=0.7)\n",
        "axes[1,1].set_title('üìä Estabilidade por Learning Rate')\n",
        "axes[1,1].set_xlabel('Learning Rate')\n",
        "axes[1,1].set_ylabel('Volatilidade (Desvio Padr√£o)')\n",
        "\n",
        "# Adiciona valores nas barras\n",
        "for bar, vol in zip(bars, volatilities):\n",
        "    height = bar.get_height()\n",
        "    axes[1,1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "                  f'{vol:.4f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéØ Li√ß√µes Aprendidas sobre Learning Rate:\")\n",
        "print(\"   ‚Ä¢ LR muito baixo (0.001): Lento mas est√°vel\")\n",
        "print(\"   ‚Ä¢ LR m√©dio (0.01-0.1): Sweet spot! üéØ\")\n",
        "print(\"   ‚Ä¢ LR muito alto (0.5+): Inst√°vel ou diverge\")\n",
        "print(\"\\nüí° Dica: Comece com 0.01 e ajuste conforme necess√°rio!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ 5. Adam: O Algoritmo \"Espert√£o\" que Se Adapta Sozinho\n\nAgora chegou a hora de conhecer a **estrela dos algoritmos de otimiza√ß√£o**: o **Adam** (Adaptive Moment Estimation)!\n\nO Adam √© como aquele GPS inteligente que:\n1. **Se adapta ao tr√¢nsito** (ajusta a taxa de aprendizado automaticamente)\n2. **Lembra do caminho** (usa momentum das itera√ß√µes passadas)\n3. **Considera o terreno** (normaliza por gradientes passados)\n\n### A Matem√°tica do Adam:\n\nO Adam combina duas ideias geniais:\n\n**1. Momentum (mem√≥ria das dire√ß√µes):**\n$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n\n**2. RMSprop (normaliza√ß√£o adaptativa):**\n$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$$\n\n**3. Corre√ß√£o de bias:**\n$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n\n**4. Atualiza√ß√£o final:**\n$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n\n### Par√¢metros Padr√£o do Adam:\n- $\\alpha = 0.001$ (learning rate)\n- $\\beta_1 = 0.9$ (momentum)\n- $\\beta_2 = 0.999$ (RMSprop)\n- $\\epsilon = 10^{-8}$ (estabilidade num√©rica)\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-12_img_06.png)\n\n**Dica do Pedro**: Adam √© como ter um personal trainer, um GPS e um meteorologista te ajudando ao mesmo tempo na descida da montanha! üèãÔ∏è‚Äç‚ôÇÔ∏èüó∫Ô∏èüå§Ô∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementa√ß√£o do Adam Optimizer\n",
        "class AdamOptimizer:\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.lr = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        \n",
        "        # Momentos para cada par√¢metro\n",
        "        self.m_w = 0  # primeiro momento para w\n",
        "        self.v_w = 0  # segundo momento para w\n",
        "        self.m_b = 0  # primeiro momento para b\n",
        "        self.v_b = 0  # segundo momento para b\n",
        "        \n",
        "        self.t = 0    # contador de itera√ß√µes\n",
        "    \n",
        "    def update(self, w, b, dw, db):\n",
        "        \"\"\"Atualiza os par√¢metros usando Adam\"\"\"\n",
        "        self.t += 1\n",
        "        \n",
        "        # Atualiza momentos para w\n",
        "        self.m_w = self.beta1 * self.m_w + (1 - self.beta1) * dw\n",
        "        self.v_w = self.beta2 * self.v_w + (1 - self.beta2) * (dw ** 2)\n",
        "        \n",
        "        # Atualiza momentos para b\n",
        "        self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * db\n",
        "        self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * (db ** 2)\n",
        "        \n",
        "        # Corre√ß√£o de bias\n",
        "        m_w_corr = self.m_w / (1 - self.beta1 ** self.t)\n",
        "        v_w_corr = self.v_w / (1 - self.beta2 ** self.t)\n",
        "        m_b_corr = self.m_b / (1 - self.beta1 ** self.t)\n",
        "        v_b_corr = self.v_b / (1 - self.beta2 ** self.t)\n",
        "        \n",
        "        # Atualiza√ß√£o dos par√¢metros\n",
        "        w_new = w - self.lr * m_w_corr / (np.sqrt(v_w_corr) + self.epsilon)\n",
        "        b_new = b - self.lr * m_b_corr / (np.sqrt(v_b_corr) + self.epsilon)\n",
        "        \n",
        "        return w_new, b_new\n",
        "\n",
        "def gradient_descent_adam(X, y, batch_size=32, learning_rate=0.001, epochs=100):\n",
        "    \"\"\"Gradiente Descendente com Adam Optimizer\"\"\"\n",
        "    # Inicializa√ß√£o\n",
        "    w = np.random.randn()\n",
        "    b = np.random.randn()\n",
        "    \n",
        "    # Inicializa o otimizador Adam\n",
        "    optimizer = AdamOptimizer(learning_rate=learning_rate)\n",
        "    \n",
        "    # Hist√≥rico\n",
        "    history = {'loss': [], 'w': [], 'b': []}\n",
        "    \n",
        "    m = len(y)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Embaralha os dados\n",
        "        indices = np.random.permutation(m)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        # Processa em mini-batches\n",
        "        for i in range(0, m, batch_size):\n",
        "            end_idx = min(i + batch_size, m)\n",
        "            X_batch = X_shuffled[i:end_idx]\n",
        "            y_batch = y_shuffled[i:end_idx]\n",
        "            \n",
        "            # Predi√ß√£o e loss\n",
        "            y_pred_batch = predict(X_batch, w, b)\n",
        "            batch_loss = mse_loss(y_batch, y_pred_batch)\n",
        "            epoch_loss += batch_loss\n",
        "            \n",
        "            # Gradientes\n",
        "            batch_m = len(y_batch)\n",
        "            dw = -(2/batch_m) * np.sum((y_batch - y_pred_batch) * X_batch.flatten())\n",
        "            db = -(2/batch_m) * np.sum(y_batch - y_pred_batch)\n",
        "            \n",
        "            # Atualiza√ß√£o com Adam\n",
        "            w, b = optimizer.update(w, b, dw, db)\n",
        "            \n",
        "            num_batches += 1\n",
        "        \n",
        "        # Salva hist√≥rico\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        history['loss'].append(avg_loss)\n",
        "        history['w'].append(w)\n",
        "        history['b'].append(b)\n",
        "    \n",
        "    return w, b, history\n",
        "\n",
        "# Testando Adam\n",
        "np.random.seed(42)\n",
        "w_adam, b_adam, hist_adam = gradient_descent_adam(X, y, batch_size=32, learning_rate=0.01, epochs=50)\n",
        "\n",
        "print(\"ü§ñ Adam Optimizer - Resultados:\")\n",
        "print(f\"   Peso (w): {w_adam:.3f} (esperado: ~3.0)\")\n",
        "print(f\"   Bias (b): {b_adam:.3f} (esperado: ~2.0)\")\n",
        "print(f\"   Loss final: {hist_adam['loss'][-1]:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\ngraph TD\n    A[Gradiente Atual] --> B[Momentum<br/>Œ≤‚ÇÅ = 0.9]\n    A --> C[RMSprop<br/>Œ≤‚ÇÇ = 0.999]\n    B --> D[Corre√ß√£o de Bias]\n    C --> E[Corre√ß√£o de Bias]\n    D --> F[Combina√ß√£o Adam]\n    E --> F\n    F --> G[Atualiza√ß√£o<br/>Adaptativa]\n    G --> H[Novos Par√¢metros]\n```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compara√ß√£o final: SGD vs Mini-Batch vs Adam\n",
        "methods = {\n",
        "    'SGD': hist_sgd,\n",
        "    'Mini-Batch (32)': results[32][2],\n",
        "    'Adam': hist_adam\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Gr√°fico 1: Converg√™ncia da Loss\n",
        "for i, (method, hist) in enumerate(methods.items()):\n",
        "    axes[0,0].plot(hist['loss'], label=method, linewidth=2.5, color=colors[i])\n",
        "\n",
        "axes[0,0].set_title('üèÅ Compara√ß√£o Final: Converg√™ncia da Loss')\n",
        "axes[0,0].set_xlabel('√âpocas')\n",
        "axes[0,0].set_ylabel('Loss (MSE)')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "axes[0,0].set_yscale('log')\n",
        "\n",
        "# Gr√°fico 2: Converg√™ncia do peso\n",
        "for i, (method, hist) in enumerate(methods.items()):\n",
        "    axes[0,1].plot(hist['w'], label=method, linewidth=2.5, color=colors[i])\n",
        "\n",
        "axes[0,1].axhline(y=3.0, color='black', linestyle='--', alpha=0.5, label='Valor Real')\n",
        "axes[0,1].set_title('‚öñÔ∏è Converg√™ncia do Peso (w)')\n",
        "axes[0,1].set_xlabel('√âpocas')\n",
        "axes[0,1].set_ylabel('Valor do Peso')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 3: Velocidade de converg√™ncia (primeiras 20 √©pocas)\n",
        "for i, (method, hist) in enumerate(methods.items()):\n",
        "    axes[1,0].plot(hist['loss'][:20], label=method, linewidth=2.5, color=colors[i], marker='o', markersize=4)\n",
        "\n",
        "axes[1,0].set_title('‚ö° Velocidade Inicial (Primeiras 20 √âpocas)')\n",
        "axes[1,0].set_xlabel('√âpocas')\n",
        "axes[1,0].set_ylabel('Loss (MSE)')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 4: M√©tricas finais\n",
        "final_losses = []\n",
        "method_names = []\n",
        "final_errors_w = []  # Erro em rela√ß√£o ao valor real (3.0)\n",
        "\n",
        "for method, hist in methods.items():\n",
        "    method_names.append(method)\n",
        "    final_losses.append(hist['loss'][-1])\n",
        "    final_errors_w.append(abs(hist['w'][-1] - 3.0))  # Erro absoluto do peso\n",
        "\n",
        "x = np.arange(len(method_names))\n",
        "width = 0.35\n",
        "\n",
        "# Normaliza as m√©tricas para compara√ß√£o\n",
        "norm_losses = np.array(final_losses) / max(final_losses)\n",
        "norm_errors = np.array(final_errors_w) / max(final_errors_w)\n",
        "\n",
        "bars1 = axes[1,1].bar(x - width/2, norm_losses, width, label='Loss Final (Normalizada)', color='skyblue', alpha=0.8)\n",
        "bars2 = axes[1,1].bar(x + width/2, norm_errors, width, label='Erro do Peso (Normalizado)', color='lightcoral', alpha=0.8)\n",
        "\n",
        "axes[1,1].set_title('üìä M√©tricas Finais (Normalizadas)')\n",
        "axes[1,1].set_ylabel('Valor Normalizado')\n",
        "axes[1,1].set_xticks(x)\n",
        "axes[1,1].set_xticklabels(method_names)\n",
        "axes[1,1].legend()\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Adiciona valores reais nas barras\n",
        "for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
        "    # Loss\n",
        "    height1 = bar1.get_height()\n",
        "    axes[1,1].text(bar1.get_x() + bar1.get_width()/2., height1 + 0.01,\n",
        "                  f'{final_losses[i]:.4f}', ha='center', va='bottom', fontsize=8)\n",
        "    # Erro do peso\n",
        "    height2 = bar2.get_height()\n",
        "    axes[1,1].text(bar2.get_x() + bar2.get_width()/2., height2 + 0.01,\n",
        "                  f'{final_errors_w[i]:.4f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüèÜ Ranking dos Algoritmos:\")\n",
        "print(\"\\nüìà Por Velocidade de Converg√™ncia:\")\n",
        "print(\"   1. ü•á Adam - R√°pido e est√°vel\")\n",
        "print(\"   2. ü•à Mini-Batch - Bom equil√≠brio\")\n",
        "print(\"   3. ü•â SGD - R√°pido mas inst√°vel\")\n",
        "\n",
        "print(\"\\nüéØ Por Precis√£o Final:\")\n",
        "for i, method in enumerate(method_names):\n",
        "    print(f\"   {i+1}. {method}: Loss = {final_losses[i]:.6f}, Erro peso = {final_errors_w[i]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† 6. Outros Algoritmos: A Fam√≠lia Completa\n\nAl√©m do Adam, existem outros membros da fam√≠lia dos otimizadores adaptativos. Cada um tem sua personalidade!\n\n### üèÉ‚Äç‚ôÇÔ∏è Momentum:\n- **Ideia**: \"Lembra\" da dire√ß√£o anterior\n- **F√≥rmula**: $v_t = \\gamma v_{t-1} + \\alpha \\nabla J(\\theta)$\n- **Personalidade**: Como uma bola rolando ladeira abaixo - ganha velocidade!\n\n### üìê RMSprop:\n- **Ideia**: Adapta a taxa de aprendizado por par√¢metro\n- **F√≥rmula**: $E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma) g_t^2$\n- **Personalidade**: Como um GPS que ajusta a velocidade conforme o terreno\n\n### ‚ö° AdaGrad:\n- **Ideia**: Diminui a taxa para par√¢metros frequentemente atualizados\n- **Problema**: Pode \"morrer\" (taxa fica muito pequena)\n- **Personalidade**: Workaholic que se cansa r√°pido\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-12_img_07.png)\n\n**Dica do Pedro**: Na pr√°tica, Adam √© o \"pau pra toda obra\" - funciona bem na maioria dos casos. √â como ter um canivete su√≠√ßo dos otimizadores! üîß"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementa√ß√£o simples de outros otimizadores para compara√ß√£o\n",
        "\n",
        "class MomentumOptimizer:\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.lr = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.v_w = 0\n",
        "        self.v_b = 0\n",
        "    \n",
        "    def update(self, w, b, dw, db):\n",
        "        # Atualiza velocidades com momentum\n",
        "        self.v_w = self.momentum * self.v_w + self.lr * dw\n",
        "        self.v_b = self.momentum * self.v_b + self.lr * db\n",
        "        \n",
        "        # Atualiza par√¢metros\n",
        "        w_new = w - self.v_w\n",
        "        b_new = b - self.v_b\n",
        "        \n",
        "        return w_new, b_new\n",
        "\n",
        "class RMSpropOptimizer:\n",
        "    def __init__(self, learning_rate=0.001, decay=0.9, epsilon=1e-8):\n",
        "        self.lr = learning_rate\n",
        "        self.decay = decay\n",
        "        self.epsilon = epsilon\n",
        "        self.s_w = 0\n",
        "        self.s_b = 0\n",
        "    \n",
        "    def update(self, w, b, dw, db):\n",
        "        # Atualiza m√©dias m√≥veis dos gradientes ao quadrado\n",
        "        self.s_w = self.decay * self.s_w + (1 - self.decay) * (dw ** 2)\n",
        "        self.s_b = self.decay * self.s_b + (1 - self.decay) * (db ** 2)\n",
        "        \n",
        "        # Atualiza par√¢metros\n",
        "        w_new = w - self.lr * dw / (np.sqrt(self.s_w) + self.epsilon)\n",
        "        b_new = b - self.lr * db / (np.sqrt(self.s_b) + self.epsilon)\n",
        "        \n",
        "        return w_new, b_new\n",
        "\n",
        "def test_optimizer(optimizer_class, optimizer_name, **kwargs):\n",
        "    \"\"\"Testa um otimizador espec√≠fico\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Inicializa√ß√£o\n",
        "    w = np.random.randn()\n",
        "    b = np.random.randn()\n",
        "    optimizer = optimizer_class(**kwargs)\n",
        "    \n",
        "    # Hist√≥rico\n",
        "    history = {'loss': [], 'w': [], 'b': []}\n",
        "    \n",
        "    batch_size = 32\n",
        "    epochs = 50\n",
        "    m = len(y)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(m)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for i in range(0, m, batch_size):\n",
        "            end_idx = min(i + batch_size, m)\n",
        "            X_batch = X_shuffled[i:end_idx]\n",
        "            y_batch = y_shuffled[i:end_idx]\n",
        "            \n",
        "            y_pred_batch = predict(X_batch, w, b)\n",
        "            batch_loss = mse_loss(y_batch, y_pred_batch)\n",
        "            epoch_loss += batch_loss\n",
        "            \n",
        "            batch_m = len(y_batch)\n",
        "            dw = -(2/batch_m) * np.sum((y_batch - y_pred_batch) * X_batch.flatten())\n",
        "            db = -(2/batch_m) * np.sum(y_batch - y_pred_batch)\n",
        "            \n",
        "            w, b = optimizer.update(w, b, dw, db)\n",
        "            num_batches += 1\n",
        "        \n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        history['loss'].append(avg_loss)\n",
        "        history['w'].append(w)\n",
        "        history['b'].append(b)\n",
        "    \n",
        "    return w, b, history\n",
        "\n",
        "# Testando todos os otimizadores\n",
        "optimizers_results = {}\n",
        "\n",
        "print(\"üß™ Testando a fam√≠lia completa de otimizadores...\\n\")\n",
        "\n",
        "# SGD simples (j√° temos)\n",
        "optimizers_results['SGD'] = (w_sgd, b_sgd, hist_sgd)\n",
        "\n",
        "# Momentum\n",
        "w_mom, b_mom, hist_mom = test_optimizer(MomentumOptimizer, 'Momentum', learning_rate=0.01, momentum=0.9)\n",
        "optimizers_results['Momentum'] = (w_mom, b_mom, hist_mom)\n",
        "\n",
        "# RMSprop\n",
        "w_rms, b_rms, hist_rms = test_optimizer(RMSpropOptimizer, 'RMSprop', learning_rate=0.01, decay=0.9)\n",
        "optimizers_results['RMSprop'] = (w_rms, b_rms, hist_rms)\n",
        "\n",
        "# Adam (j√° temos)\n",
        "optimizers_results['Adam'] = (w_adam, b_adam, hist_adam)\n",
        "\n",
        "# Resultados\n",
        "for name, (w_final, b_final, hist) in optimizers_results.items():\n",
        "    print(f\"ü§ñ {name}:\")\n",
        "    print(f\"   Peso final: {w_final:.3f}\")\n",
        "    print(f\"   Bias final: {b_final:.3f}\")\n",
        "    print(f\"   Loss final: {hist['loss'][-1]:.6f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compara√ß√£o visual de todos os otimizadores\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "colors = ['red', 'orange', 'green', 'blue']\n",
        "opt_names = list(optimizers_results.keys())\n",
        "\n",
        "# Gr√°fico 1: Converg√™ncia da Loss\n",
        "for i, (name, (_, _, hist)) in enumerate(optimizers_results.items()):\n",
        "    axes[0,0].plot(hist['loss'], label=name, linewidth=2.5, color=colors[i])\n",
        "\n",
        "axes[0,0].set_title('üèÜ Batalha dos Otimizadores: Converg√™ncia da Loss')\n",
        "axes[0,0].set_xlabel('√âpocas')\n",
        "axes[0,0].set_ylabel('Loss (MSE)')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "axes[0,0].set_yscale('log')\n",
        "\n",
        "# Gr√°fico 2: Trajet√≥ria no espa√ßo de par√¢metros\n",
        "for i, (name, (_, _, hist)) in enumerate(optimizers_results.items()):\n",
        "    # Plota apenas os primeiros 30 pontos para clareza\n",
        "    axes[0,1].plot(hist['w'][:30], hist['b'][:30], label=name, linewidth=2, \n",
        "                  color=colors[i], marker='o', markersize=3, alpha=0.8)\n",
        "\n",
        "axes[0,1].plot(3.0, 2.0, 'k*', markersize=15, label='√ìtimo Real')\n",
        "axes[0,1].set_title('üéØ Trajet√≥rias no Espa√ßo de Par√¢metros')\n",
        "axes[0,1].set_xlabel('Peso (w)')\n",
        "axes[0,1].set_ylabel('Bias (b)')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 3: Velocidade de converg√™ncia (primeiras 15 √©pocas)\n",
        "for i, (name, (_, _, hist)) in enumerate(optimizers_results.items()):\n",
        "    axes[1,0].plot(hist['loss'][:15], label=name, linewidth=2.5, \n",
        "                  color=colors[i], marker='s', markersize=4)\n",
        "\n",
        "axes[1,0].set_title('‚ö° Largada: Primeiras 15 √âpocas')\n",
        "axes[1,0].set_xlabel('√âpocas')\n",
        "axes[1,0].set_ylabel('Loss (MSE)')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "axes[1,0].set_yscale('log')\n",
        "\n",
        "# Gr√°fico 4: Ranking final\n",
        "final_losses = [hist['loss'][-1] for _, (_, _, hist) in optimizers_results.items()]\n",
        "final_w_errors = [abs(w_final - 3.0) for w_final, _, _ in optimizers_results.values()]\n",
        "\n",
        "# Calcula pontua√ß√£o (menor √© melhor)\n",
        "loss_ranks = np.argsort(final_losses) + 1\n",
        "error_ranks = np.argsort(final_w_errors) + 1\n",
        "total_scores = loss_ranks + error_ranks  # Menor pontua√ß√£o = melhor\n",
        "\n",
        "# Ordena por pontua√ß√£o total\n",
        "sorted_indices = np.argsort(total_scores)\n",
        "sorted_names = [opt_names[i] for i in sorted_indices]\n",
        "sorted_scores = [total_scores[i] for i in sorted_indices]\n",
        "\n",
        "# Cores baseadas na posi√ß√£o\n",
        "rank_colors = ['gold', 'silver', '#CD7F32', 'gray']  # Ouro, Prata, Bronze, Cinza\n",
        "\n",
        "bars = axes[1,1].bar(sorted_names, sorted_scores, color=rank_colors, alpha=0.8)\n",
        "axes[1,1].set_title('üèÖ Ranking Final (Menor = Melhor)')\n",
        "axes[1,1].set_ylabel('Pontua√ß√£o Total')\n",
        "axes[1,1].set_xlabel('Otimizador')\n",
        "\n",
        "# Adiciona medalhas\n",
        "medals = ['ü•á', 'ü•à', 'ü•â', '4th']\n",
        "for i, (bar, score) in enumerate(zip(bars, sorted_scores)):\n",
        "    height = bar.get_height()\n",
        "    axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
        "                  f'{medals[i]}\\n{score}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüèÜ RANKING FINAL DOS OTIMIZADORES:\")\n",
        "for i, idx in enumerate(sorted_indices):\n",
        "    name = opt_names[idx]\n",
        "    loss = final_losses[idx]\n",
        "    w_error = final_w_errors[idx]\n",
        "    print(f\"   {medals[i]} {name}: Loss={loss:.6f}, Erro_peso={w_error:.4f}\")\n",
        "\n",
        "print(\"\\nüí° Resumo das Personalidades:\")\n",
        "print(\"   ‚Ä¢ SGD: R√°pido mas inst√°vel (como um carro de corrida sem controle)\")\n",
        "print(\"   ‚Ä¢ Momentum: Ganha velocidade com o tempo (como uma bola descendo)\")\n",
        "print(\"   ‚Ä¢ RMSprop: Se adapta ao terreno (como um ve√≠culo todo-terreno)\")\n",
        "print(\"   ‚Ä¢ Adam: O equilibrado (como um carro de luxo com todos os recursos)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è 7. Exerc√≠cio Pr√°tico: Criando Seu Pr√≥prio Otimizador\n\nAgora √© sua vez de brilhar! Vamos criar um desafio pr√°tico para consolidar todo o conhecimento.\n\n### üéØ Desafio 1: Implemente o AdaMax\n\nO **AdaMax** √© uma varia√ß√£o do Adam que usa a norma infinita em vez da norma L2. Sua f√≥rmula √©:\n\n$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n$$u_t = \\max(\\beta_2 u_{t-1}, |g_t|)$$\n$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{u_t} m_t$$\n\n**Sua miss√£o**: Complete a implementa√ß√£o abaixo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 1: Implemente o AdaMax\n",
        "class AdaMaxOptimizer:\n",
        "    def __init__(self, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.lr = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        \n",
        "        # TODO: Inicialize os momentos\n",
        "        self.m_w = 0  # primeiro momento para w\n",
        "        self.u_w = 0  # norma infinita para w\n",
        "        self.m_b = 0  # primeiro momento para b  \n",
        "        self.u_b = 0  # norma infinita para b\n",
        "        \n",
        "        self.t = 0    # contador\n",
        "    \n",
        "    def update(self, w, b, dw, db):\n",
        "        \"\"\"TODO: Implemente a atualiza√ß√£o do AdaMax\"\"\"\n",
        "        self.t += 1\n",
        "        \n",
        "        # TODO: Atualize os momentos para w\n",
        "        self.m_w = self.beta1 * self.m_w + (1 - self.beta1) * dw\n",
        "        self.u_w = max(self.beta2 * self.u_w, abs(dw))\n",
        "        \n",
        "        # TODO: Atualize os momentos para b\n",
        "        self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * db\n",
        "        self.u_b = max(self.beta2 * self.u_b, abs(db))\n",
        "        \n",
        "        # TODO: Corre√ß√£o de bias apenas para m (n√£o para u!)\n",
        "        m_w_corr = self.m_w / (1 - self.beta1 ** self.t)\n",
        "        m_b_corr = self.m_b / (1 - self.beta1 ** self.t)\n",
        "        \n",
        "        # TODO: Atualiza√ß√£o dos par√¢metros\n",
        "        w_new = w - self.lr * m_w_corr / (self.u_w + self.epsilon)\n",
        "        b_new = b - self.lr * m_b_corr / (self.u_b + self.epsilon)\n",
        "        \n",
        "        return w_new, b_new\n",
        "\n",
        "# Teste seu AdaMax!\n",
        "print(\"üß™ Testando seu AdaMax...\")\n",
        "try:\n",
        "    w_adamax, b_adamax, hist_adamax = test_optimizer(\n",
        "        AdaMaxOptimizer, 'AdaMax', learning_rate=0.01, beta1=0.9, beta2=0.999\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ AdaMax funcionou!\")\n",
        "    print(f\"   Peso final: {w_adamax:.3f}\")\n",
        "    print(f\"   Bias final: {b_adamax:.3f}\")\n",
        "    print(f\"   Loss final: {hist_adamax['loss'][-1]:.6f}\")\n",
        "    \n",
        "    # Compara√ß√£o r√°pida com Adam\n",
        "    print(f\"\\nüìä Compara√ß√£o com Adam:\")\n",
        "    print(f\"   Adam Loss: {hist_adam['loss'][-1]:.6f}\")\n",
        "    print(f\"   AdaMax Loss: {hist_adamax['loss'][-1]:.6f}\")\n",
        "    \n",
        "    if hist_adamax['loss'][-1] < hist_adam['loss'][-1]:\n",
        "        print(\"   üéâ Seu AdaMax est√° melhor que o Adam!\")\n",
        "    else:\n",
        "        print(\"   ü§ñ Adam ainda est√° na frente, mas bom trabalho!\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Ops! Algo deu errado: {e}\")\n",
        "    print(\"üí° Dica: Verifique se voc√™ implementou todas as partes TODO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Desafio 2: Experimento com Learning Rate Schedule\n\nUma t√©cnica avan√ßada √© **variar a taxa de aprendizado** durante o treinamento. Implemente um **Learning Rate Decay**:\n\n$$\\alpha_t = \\alpha_0 \\cdot \\frac{1}{1 + \\text{decay} \\cdot t}$$\n\nOnde $t$ √© o n√∫mero da itera√ß√£o atual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 2: SGD com Learning Rate Decay\n",
        "def sgd_with_lr_decay(X, y, initial_lr=0.1, decay=0.01, epochs=100):\n",
        "    \"\"\"SGD com taxa de aprendizado que diminui ao longo do tempo\"\"\"\n",
        "    \n",
        "    # TODO: Complete a implementa√ß√£o\n",
        "    w = np.random.randn()\n",
        "    b = np.random.randn()\n",
        "    \n",
        "    history = {'loss': [], 'w': [], 'b': [], 'lr': []}  # Adicionamos lr ao hist√≥rico\n",
        "    m = len(y)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # TODO: Calcule a taxa de aprendizado atual\n",
        "        current_lr = initial_lr / (1 + decay * epoch)\n",
        "        \n",
        "        # Embaralha dados\n",
        "        indices = np.random.permutation(m)\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        # SGD para cada exemplo\n",
        "        for i in indices:\n",
        "            xi = X[i:i+1]\n",
        "            yi = y[i]\n",
        "            \n",
        "            # TODO: Complete o SGD usando current_lr\n",
        "            y_pred_i = predict(xi, w, b)\n",
        "            error = yi - y_pred_i\n",
        "            \n",
        "            dw = -2 * error * xi.flatten()[0]\n",
        "            db = -2 * error\n",
        "            \n",
        "            # Atualiza√ß√£o com taxa decrescente\n",
        "            w = w - current_lr * dw\n",
        "            b = b - current_lr * db\n",
        "            \n",
        "            epoch_loss += (error ** 2)\n",
        "        \n",
        "        # Salva hist√≥rico\n",
        "        avg_loss = epoch_loss / m\n",
        "        history['loss'].append(avg_loss)\n",
        "        history['w'].append(w)\n",
        "        history['b'].append(b)\n",
        "        history['lr'].append(current_lr)\n",
        "    \n",
        "    return w, b, history\n",
        "\n",
        "# Testando diferentes configura√ß√µes de decay\n",
        "print(\"üß™ Testando Learning Rate Decay...\\n\")\n",
        "\n",
        "decay_configs = [0.0, 0.01, 0.05, 0.1]  # 0.0 = sem decay\n",
        "decay_results = {}\n",
        "\n",
        "for decay in decay_configs:\n",
        "    np.random.seed(42)\n",
        "    w_decay, b_decay, hist_decay = sgd_with_lr_decay(\n",
        "        X, y, initial_lr=0.1, decay=decay, epochs=50\n",
        "    )\n",
        "    \n",
        "    decay_results[decay] = (w_decay, b_decay, hist_decay)\n",
        "    \n",
        "    print(f\"üìâ Decay = {decay}:\")\n",
        "    print(f\"   Loss final: {hist_decay['loss'][-1]:.6f}\")\n",
        "    print(f\"   LR inicial: {hist_decay['lr'][0]:.4f}\")\n",
        "    print(f\"   LR final: {hist_decay['lr'][-1]:.4f}\\n\")\n",
        "\n",
        "# Visualiza√ß√£o do Learning Rate Decay\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "colors = ['blue', 'green', 'orange', 'red']\n",
        "\n",
        "# Gr√°fico 1: Evolu√ß√£o da taxa de aprendizado\n",
        "for i, decay in enumerate(decay_configs):\n",
        "    _, _, hist = decay_results[decay]\n",
        "    axes[0].plot(hist['lr'], label=f'Decay = {decay}', \n",
        "                linewidth=2.5, color=colors[i])\n",
        "\n",
        "axes[0].set_title('üìâ Evolu√ß√£o da Taxa de Aprendizado')\n",
        "axes[0].set_xlabel('√âpocas')\n",
        "axes[0].set_ylabel('Learning Rate')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 2: Converg√™ncia da loss\n",
        "for i, decay in enumerate(decay_configs):\n",
        "    _, _, hist = decay_results[decay]\n",
        "    axes[1].plot(hist['loss'], label=f'Decay = {decay}', \n",
        "                linewidth=2.5, color=colors[i])\n",
        "\n",
        "axes[1].set_title('üìà Converg√™ncia com Different Decays')\n",
        "axes[1].set_xlabel('√âpocas')\n",
        "axes[1].set_ylabel('Loss (MSE)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Observa√ß√µes sobre Learning Rate Decay:\")\n",
        "print(\"   ‚Ä¢ Sem decay (0.0): Pode oscilar no final\")\n",
        "print(\"   ‚Ä¢ Decay moderado: Melhora a converg√™ncia final\")\n",
        "print(\"   ‚Ä¢ Decay alto: Pode ficar muito lento\")\n",
        "print(\"\\nüí° A chave √© encontrar o equil√≠brio!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì 8. Resumo e Li√ß√µes Aprendidas\n\nParab√©ns! üéâ Voc√™ completou o √∫ltimo m√≥dulo do nosso curso \"C√°lculo para IA\"! Vamos fazer um resumo √©pico de tudo que aprendemos sobre otimiza√ß√£o:\n\n### üèîÔ∏è A Jornada pela Montanha do Erro:\n\n1. **Gradiente Descendente Batch**: O marombeiro que usa todos os dados\n   - ‚úÖ Preciso e est√°vel\n   - ‚ùå Lento para datasets grandes\n\n2. **SGD (Stochastic)**: O apressadinho que usa um exemplo por vez\n   - ‚úÖ R√°pido e usa pouca mem√≥ria\n   - ‚ùå Inst√°vel, mas pode escapar de m√≠nimos locais\n\n3. **Mini-Batch**: O meio-termo inteligente\n   - ‚úÖ Equilibra velocidade e estabilidade\n   - ‚úÖ Eficiente para GPUs\n\n4. **Adam**: O algoritmo \"espert√£o\" adaptativo\n   - ‚úÖ Se adapta automaticamente\n   - ‚úÖ Funciona bem na maioria dos casos\n   - ‚úÖ Combina momentum + normaliza√ß√£o adaptativa\n\n### üéõÔ∏è Sobre a Taxa de Aprendizado:\n- **Muito baixa**: Lesma na montanha üêå\n- **Muito alta**: Carro sem freio üöóüí®\n- **Na medida certa**: Equil√≠brio perfeito! üéØ\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-12_img_08.png)\n\n**Dica Final do Pedro**: Na vida real, comece com Adam e learning rate 0.001. Se n√£o funcionar, a√≠ voc√™ experimenta os outros! √â como escolher pizza - margherita sempre funciona, mas √†s vezes voc√™ quer experimentar sabores diferentes! üçï"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\ngraph TD\n    A[In√≠cio do Treinamento] --> B{Escolha do Otimizador}\n    B -->|Dataset Pequeno| C[Batch GD]\n    B -->|Dataset Grande| D[SGD/Mini-Batch]\n    B -->|Uso Geral| E[Adam]\n    \n    C --> F[Ajustar Learning Rate]\n    D --> F\n    E --> F\n    \n    F --> G{Convergiu?}\n    G -->|N√£o| H[Ajustar Hiperpar√¢metros]\n    G -->|Sim| I[üéâ Sucesso!]\n    H --> F\n```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üèÜ COMPARA√á√ÉO FINAL: Todos os algoritmos que vimos\n",
        "print(\"üéä PARAB√âNS! Voc√™ completou o curso 'C√°lculo para IA'! üéä\\n\")\n",
        "print(\"üìä RESUMO FINAL - Todos os Otimizadores:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Coletando todos os resultados para compara√ß√£o final\n",
        "all_optimizers = {\n",
        "    'Batch GD': (hist_batch['loss'][-1], abs(hist_batch['w'][-1] - 3.0)),\n",
        "    'SGD': (hist_sgd['loss'][-1], abs(hist_sgd['w'][-1] - 3.0)),\n",
        "    'Mini-Batch': (results[32][2]['loss'][-1], abs(results[32][2]['w'][-1] - 3.0)),\n",
        "    'Momentum': (hist_mom['loss'][-1], abs(hist_mom['w'][-1] - 3.0)),\n",
        "    'RMSprop': (hist_rms['loss'][-1], abs(hist_rms['w'][-1] - 3.0)),\n",
        "    'Adam': (hist_adam['loss'][-1], abs(hist_adam['w'][-1] - 3.0))\n",
        "}\n",
        "\n",
        "# Se AdaMax foi implementado com sucesso\n",
        "if 'hist_adamax' in locals():\n",
        "    all_optimizers['AdaMax'] = (hist_adamax['loss'][-1], abs(hist_adamax['w'][-1] - 3.0))\n",
        "\n",
        "# Ranking por loss final\n",
        "sorted_by_loss = sorted(all_optimizers.items(), key=lambda x: x[1][0])\n",
        "\n",
        "print(\"\\nüèÖ RANKING POR LOSS FINAL:\")\n",
        "medals = ['ü•á', 'ü•à', 'ü•â'] + ['üèÜ'] * 10\n",
        "for i, (name, (loss, error)) in enumerate(sorted_by_loss):\n",
        "    print(f\"   {medals[i]} {name:<12}: Loss = {loss:.6f}, Erro_peso = {error:.4f}\")\n",
        "\n",
        "print(\"\\nüéØ RECOMENDA√á√ïES PR√ÅTICAS:\")\n",
        "print(\"   ‚Ä¢ Para iniciantes: Comece com Adam (lr=0.001)\")\n",
        "print(\"   ‚Ä¢ Para datasets pequenos: Batch GD funciona bem\")\n",
        "print(\"   ‚Ä¢ Para datasets grandes: Mini-Batch (32-128)\")\n",
        "print(\"   ‚Ä¢ Para pesquisa: Experimente diferentes otimizadores\")\n",
        "\n",
        "print(\"\\nüöÄ PR√ìXIMOS PASSOS:\")\n",
        "print(\"   ‚Ä¢ Aplicar em redes neurais reais\")\n",
        "print(\"   ‚Ä¢ Experimentar com datasets reais\")\n",
        "print(\"   ‚Ä¢ Estudar regulariza√ß√£o e outras t√©cnicas\")\n",
        "print(\"   ‚Ä¢ Explorar frameworks como PyTorch/TensorFlow\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ OBRIGADO POR FAZER PARTE DESSA JORNADA! üéâ\")\n",
        "print(\"\\nüí° Lembre-se: O c√°lculo √© a linguagem da IA.\")\n",
        "print(\"    Agora voc√™ fala essa linguagem fluentemente!\")\n",
        "print(\"\\nüî• V√° l√° e construa coisas incr√≠veis com IA! üî•\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Conclus√£o: Voc√™ Agora √© um Otimizador Master!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-12_img_09.png)\n\n### üèÜ O Que Voc√™ Conquistou:\n\n‚úÖ **Dominou** as varia√ß√µes do gradiente descendente  \n‚úÖ **Entendeu** a import√¢ncia da taxa de aprendizado  \n‚úÖ **Implementou** algoritmos avan√ßados como Adam  \n‚úÖ **Comparou** diferentes estrat√©gias de otimiza√ß√£o  \n‚úÖ **Completou** o curso \"C√°lculo para IA\"!  \n\n### üåü Reflex√£o Final:\n\nCome√ßamos nossa jornada perguntando \"Por que se importar com c√°lculo?\". Agora voc√™ sabe que o **c√°lculo √© o cora√ß√£o** de toda intelig√™ncia artificial moderna!\n\n- **Derivadas** nos mostram a dire√ß√£o certa\n- **Gradientes** nos guiam pela montanha do erro  \n- **Otimizadores** nos levam ao topo com efici√™ncia\n\n### üöÄ Agora √â Sua Vez!\n\nVoc√™ tem todas as ferramentas para:\n- Criar seus pr√≥prios algoritmos de ML\n- Entender o que acontece \"por debaixo do cap√¥\" \n- Debugar e otimizar modelos como um expert\n- Contribuir para o avan√ßo da IA!\n\n**√öltima Dica do Pedro**: O aprendizado nunca para! Continue experimentando, errando, aprendendo e, principalmente, se divertindo com IA. O futuro √© nosso para construir! üåàü§ñ\n\n---\n\n**At√© a pr√≥xima aventura!** üéä  \n**Pedro Nunes Guth** üìö‚ú®"
      ]
    }
  ]
}