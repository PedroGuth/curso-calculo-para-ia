{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Fun√ß√µes e Gr√°ficos: Visualizando o Problema\n\n## *M√≥dulo 2 - C√°lculo para IA*\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-02_img_01.png)\n\nFala galera! Sou o **Pedro Guth** e hoje vamos mergulhar no mundo das **fun√ß√µes e gr√°ficos**! üöÄ\n\nLembra do M√≥dulo 1 quando falamos sobre a **montanha do erro**? Pois √©, hoje vamos aprender a **visualizar** essa montanha de verdade! \n\n**T√°, mas por que isso √© importante?**\n\nImagina que voc√™ est√° dirigindo no Google Maps sem ver o mapa - s√≥ ouvindo as instru√ß√µes. Meio complicado, n√©? √â exatamente isso que acontece quando tentamos entender IA sem visualizar as fun√ß√µes!\n\n**Neste notebook voc√™ vai aprender:**\n- O que s√£o fun√ß√µes matem√°ticas de verdade\n- Por que visualizar √© T√ÉO importante\n- Como criar e interpretar gr√°ficos\n- Fun√ß√µes de custo na pr√°tica\n- E muito mais!\n\n**Bora come√ßar!** üéØ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Importando nossas ferramentas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurando o estilo dos gr√°ficos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configurando para aparecer gr√°ficos inline\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"üéâ Bibliotecas carregadas! Vamos visualizar!\")\n",
        "print(\"üìä Numpy version:\", np.__version__)\n",
        "print(\"üìà Matplotlib pronto para os gr√°ficos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î O que s√£o Fun√ß√µes Matem√°ticas?\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-02_img_02.png)\n\n**T√°, mas o que √© uma fun√ß√£o afinal?**\n\nPensa numa **m√°quina de salgadinho**: voc√™ coloca uma moeda (entrada), ela processa, e sai um salgadinho (sa√≠da). A fun√ß√£o √© exatamente isso!\n\n**Matematicamente:**\n$$f(x) = y$$\n\nOnde:\n- $x$ √© a **entrada** (input)\n- $f$ √© a **fun√ß√£o** (processamento) \n- $y$ √© a **sa√≠da** (output)\n\n**Exemplos cl√°ssicos:**\n- $f(x) = 2x + 1$ (fun√ß√£o linear)\n- $f(x) = x^2$ (fun√ß√£o quadr√°tica)\n- $f(x) = e^x$ (fun√ß√£o exponencial)\n\n**Na IA, as fun√ß√µes representam:**\n- **Modelos**: $f(x) = \\text{predi√ß√£o}$\n- **Custos**: $C(\\theta) = \\text{erro do modelo}$\n- **Ativa√ß√µes**: $\\sigma(x) = \\text{neur√¥nio ativado}$\n\n**üí° Dica do Pedro:** Toda vez que voc√™ v√™ algo do tipo \"entrada ‚Üí processamento ‚Üí sa√≠da\", voc√™ est√° vendo uma fun√ß√£o!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar nossas primeiras fun√ß√µes!\n",
        "\n",
        "def funcao_linear(x):\n",
        "    \"\"\"Fun√ß√£o linear: f(x) = 2x + 1\"\"\"\n",
        "    return 2*x + 1\n",
        "\n",
        "def funcao_quadratica(x):\n",
        "    \"\"\"Fun√ß√£o quadr√°tica: f(x) = x¬≤\"\"\"\n",
        "    return x**2\n",
        "\n",
        "def funcao_exponencial(x):\n",
        "    \"\"\"Fun√ß√£o exponencial: f(x) = e^x\"\"\"\n",
        "    return np.exp(x)\n",
        "\n",
        "# Testando nossas fun√ß√µes\n",
        "print(\"üßÆ Testando nossas fun√ß√µes:\")\n",
        "print(f\"Linear f(3) = {funcao_linear(3)}\")\n",
        "print(f\"Quadr√°tica f(3) = {funcao_quadratica(3)}\")\n",
        "print(f\"Exponencial f(1) = {funcao_exponencial(1):.2f}\")\n",
        "\n",
        "print(\"\\nüéØ Liiindo! Fun√ß√µes criadas com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Por que Visualizar √© Fundamental?\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-02_img_03.png)\n\n**Imagine tentar entender o Brasil s√≥ olhando coordenadas GPS vs usar o Google Maps...**\n\n√â exatamente essa a diferen√ßa entre trabalhar com fun√ß√µes s√≥ numericamente vs visualiz√°-las!\n\n**üß† O c√©rebro humano processa informa√ß√µes visuais:**\n- **50.000x mais r√°pido** que texto\n- Identifica **padr√µes** instantaneamente\n- Detecta **anomalias** facilmente\n\n**Na IA, visualizar fun√ß√µes nos ajuda a:**\n\n1. **Entender o comportamento**: Como a fun√ß√£o \"se comporta\"?\n2. **Identificar m√≠nimos/m√°ximos**: Onde est√£o os pontos √≥timos?\n3. **Ver a inclina√ß√£o**: Qu√£o √≠ngreme √© a \"montanha\"?\n4. **Detectar problemas**: Explos√£o de gradientes, overfitting...\n5. **Otimizar modelos**: Escolher hiperpar√¢metros melhores\n\n**üí° Dica do Pedro:** Se voc√™ n√£o consegue desenhar sua fun√ß√£o de custo, provavelmente n√£o entendeu seu modelo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos ver a diferen√ßa entre n√∫meros e gr√°ficos!\n",
        "\n",
        "# Criando dados\n",
        "x = np.linspace(-3, 3, 100)\n",
        "y_linear = funcao_linear(x)\n",
        "y_quad = funcao_quadratica(x)\n",
        "\n",
        "# Primeiro: s√≥ n√∫meros (dif√≠cil de entender)\n",
        "print(\"üìä APENAS N√öMEROS (confuso):\")\n",
        "print(\"x  | Linear | Quadr√°tica\")\n",
        "print(\"-\" * 25)\n",
        "for i in range(0, len(x), 20):\n",
        "    print(f\"{x[i]:4.1f} | {y_linear[i]:6.1f} | {y_quad[i]:8.1f}\")\n",
        "\n",
        "print(\"\\nü§î Conseguiu entender o padr√£o? Dif√≠cil n√©...\")\n",
        "print(\"\\nüëá Agora vamos VISUALIZAR!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agora com gr√°ficos (muito mais f√°cil!)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Fun√ß√£o Linear\n",
        "ax1.plot(x, y_linear, 'b-', linewidth=3, label='f(x) = 2x + 1')\n",
        "ax1.set_title('üìà Fun√ß√£o Linear', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('f(x)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "# Fun√ß√£o Quadr√°tica\n",
        "ax2.plot(x, y_quad, 'r-', linewidth=3, label='f(x) = x¬≤')\n",
        "ax2.set_title('üìâ Fun√ß√£o Quadr√°tica', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('f(x)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Agora sim! D√° pra ver claramente:\")\n",
        "print(\"üìà Linear: sempre cresce na mesma taxa\")\n",
        "print(\"üìâ Quadr√°tica: tem formato de U (par√°bola)\")\n",
        "print(\"\\nüí° A visualiza√ß√£o revela TUDO instantaneamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Fun√ß√µes de Custo: O Cora√ß√£o da IA\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-02_img_04.png)\n\n**Agora vamos ao que interessa: FUN√á√ïES DE CUSTO!**\n\nLembra da **montanha do erro** do M√≥dulo 1? Pois √©, essa montanha √© matematicamente uma **fun√ß√£o de custo**!\n\n**ü§î T√°, mas o que √© uma fun√ß√£o de custo?**\n\n√â uma fun√ß√£o que **mede o qu√£o errado** seu modelo est√°. Quanto maior o valor, pior o modelo!\n\n**Matematicamente:**\n$$C(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n\nOnde:\n- $C(\\theta)$ = fun√ß√£o de custo\n- $\\theta$ = par√¢metros do modelo\n- $m$ = n√∫mero de exemplos\n- $h_\\theta(x^{(i)})$ = predi√ß√£o do modelo\n- $y^{(i)}$ = valor real\n\n**Tipos mais comuns:**\n1. **MSE (Mean Squared Error)**: Para regress√£o\n2. **Cross-Entropy**: Para classifica√ß√£o\n3. **MAE (Mean Absolute Error)**: Mais robusta\n\n**üí° Dica do Pedro:** O objetivo da IA √© encontrar o **fundo do vale** dessa montanha!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar dados sint√©ticos para nosso exemplo\n",
        "np.random.seed(42)  # Para reprodutibilidade\n",
        "\n",
        "# Gerando dados de exemplo: y = 2x + 1 + ru√≠do\n",
        "x_data = np.linspace(0, 10, 50)\n",
        "y_real = 2 * x_data + 1  # Fun√ß√£o real (sem ru√≠do)\n",
        "y_data = y_real + np.random.normal(0, 2, len(x_data))  # Com ru√≠do\n",
        "\n",
        "print(\"üìä Dataset criado!\")\n",
        "print(f\"   üìà {len(x_data)} pontos de dados\")\n",
        "print(f\"   üéØ Fun√ß√£o real: y = 2x + 1\")\n",
        "print(f\"   üîÄ Ru√≠do adicionado para simular dados reais\")\n",
        "\n",
        "# Visualizando nossos dados\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x_data, y_data, alpha=0.6, s=50, label='Dados com ru√≠do')\n",
        "plt.plot(x_data, y_real, 'r--', linewidth=2, label='Fun√ß√£o real: y = 2x + 1')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('üìä Nossos Dados de Treinamento', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéØ Agora vamos criar um modelo para tentar 'adivinhar' a linha vermelha!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definindo nosso modelo linear simples: h(x) = Œ∏‚ÇÄ + Œ∏‚ÇÅx\n",
        "\n",
        "def modelo_linear(x, theta0, theta1):\n",
        "    \"\"\"Nosso modelo: h(x) = Œ∏‚ÇÄ + Œ∏‚ÇÅx\"\"\"\n",
        "    return theta0 + theta1 * x\n",
        "\n",
        "def funcao_custo_mse(theta0, theta1, x, y):\n",
        "    \"\"\"Fun√ß√£o de custo MSE\"\"\"\n",
        "    m = len(x)\n",
        "    predicoes = modelo_linear(x, theta0, theta1)\n",
        "    custo = (1/(2*m)) * np.sum((predicoes - y)**2)\n",
        "    return custo\n",
        "\n",
        "# Testando diferentes par√¢metros\n",
        "print(\"üßÆ Testando diferentes par√¢metros:\")\n",
        "print(\"\\nŒ∏‚ÇÄ  | Œ∏‚ÇÅ  | Custo\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "parametros_teste = [\n",
        "    (0, 0),    # Muito ruim\n",
        "    (1, 1),    # Razo√°vel\n",
        "    (1, 2),    # Melhor\n",
        "    (1, 2.1),  # Ainda melhor\n",
        "    (0.8, 2.05) # Pr√≥ximo do √≥timo\n",
        "]\n",
        "\n",
        "for theta0, theta1 in parametros_teste:\n",
        "    custo = funcao_custo_mse(theta0, theta1, x_data, y_data)\n",
        "    print(f\"{theta0:3.1f} | {theta1:3.1f} | {custo:6.2f}\")\n",
        "\n",
        "print(\"\\nüéØ Repara como o custo vai diminuindo conforme nos aproximamos dos valores reais!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos visualizar como diferentes modelos se ajustam aos dados\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "parametros = [\n",
        "    (0, 0, \"Modelo Terr√≠vel\"),\n",
        "    (1, 1, \"Modelo Ruim\"), \n",
        "    (1, 2, \"Modelo Bom\"),\n",
        "    (1, 2.1, \"Modelo Muito Bom\"),\n",
        "    (0.8, 2.05, \"Modelo Excelente\"),\n",
        "    (1, 2, \"Compara√ß√£o Final\")\n",
        "]\n",
        "\n",
        "for i, (theta0, theta1, titulo) in enumerate(parametros):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # Dados\n",
        "    ax.scatter(x_data, y_data, alpha=0.6, s=30, label='Dados')\n",
        "    \n",
        "    if i == 5:  # √öltima subplot - compara√ß√£o\n",
        "        ax.plot(x_data, y_real, 'g--', linewidth=3, label='Real: y = 2x + 1')\n",
        "        ax.plot(x_data, modelo_linear(x_data, 1, 2), 'r-', linewidth=3, label='Modelo: y = 1 + 2x')\n",
        "    else:\n",
        "        # Modelo atual\n",
        "        y_pred = modelo_linear(x_data, theta0, theta1)\n",
        "        ax.plot(x_data, y_pred, 'r-', linewidth=2, label=f'y = {theta0} + {theta1}x')\n",
        "        \n",
        "        # Fun√ß√£o real\n",
        "        ax.plot(x_data, y_real, 'g--', alpha=0.7, label='Real')\n",
        "    \n",
        "    custo = funcao_custo_mse(theta0, theta1, x_data, y_data)\n",
        "    ax.set_title(f'{titulo}\\nCusto: {custo:.2f}', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Viu como a visualiza√ß√£o mostra CLARAMENTE qual modelo √© melhor?\")\n",
        "print(\"üìâ Quanto menor o custo, melhor o ajuste aos dados!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üó∫Ô∏è Mapeando a Paisagem do Custo\n\nAgora vem a parte mais **LIIIINDA** do neg√≥cio: vamos mapear toda a paisagem da fun√ß√£o de custo!\n\n**ü§î O que isso significa?**\n\nSignifica que vamos testar **TODOS** os poss√≠veis valores de $\\theta_0$ e $\\theta_1$ e ver como fica o custo!\n\n**Matematicamente, vamos criar:**\n$$C(\\theta_0, \\theta_1) = \\text{superf√≠cie 3D}$$\n\n**Por que isso √© importante?**\n- Visualizar onde est√° o **m√≠nimo global**\n- Entender a **topografia** do problema\n- Ver se existem **m√≠nimos locais**\n- Planejar a **estrat√©gia de otimiza√ß√£o**\n\n**üí° Dica do Pedro:** Isso √© exatamente o que o Gradiente Descendente vai \"navegar\" nos pr√≥ximos m√≥dulos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando a grade de par√¢metros para mapear a paisagem\n",
        "theta0_range = np.linspace(-2, 4, 50)\n",
        "theta1_range = np.linspace(0, 4, 50)\n",
        "\n",
        "# Criando a grade 2D\n",
        "Theta0, Theta1 = np.meshgrid(theta0_range, theta1_range)\n",
        "\n",
        "# Calculando o custo para cada combina√ß√£o de par√¢metros\n",
        "Custos = np.zeros_like(Theta0)\n",
        "\n",
        "print(\"üó∫Ô∏è Mapeando a paisagem do custo...\")\n",
        "print(f\"   üìä Testando {len(theta0_range)} √ó {len(theta1_range)} = {len(theta0_range) * len(theta1_range)} combina√ß√µes\")\n",
        "\n",
        "for i in range(len(theta0_range)):\n",
        "    for j in range(len(theta1_range)):\n",
        "        Custos[j, i] = funcao_custo_mse(Theta0[j, i], Theta1[j, i], x_data, y_data)\n",
        "\n",
        "print(\"‚úÖ Paisagem mapeada!\")\n",
        "print(f\"   üìà Custo m√≠nimo: {np.min(Custos):.2f}\")\n",
        "print(f\"   üìâ Custo m√°ximo: {np.max(Custos):.2f}\")\n",
        "\n",
        "# Encontrando o ponto de m√≠nimo\n",
        "min_idx = np.unravel_index(np.argmin(Custos), Custos.shape)\n",
        "theta0_otimo = Theta0[min_idx]\n",
        "theta1_otimo = Theta1[min_idx]\n",
        "custo_minimo = Custos[min_idx]\n",
        "\n",
        "print(f\"\\nüéØ PONTO √ìTIMO ENCONTRADO:\")\n",
        "print(f\"   Œ∏‚ÇÄ √≥timo: {theta0_otimo:.2f}\")\n",
        "print(f\"   Œ∏‚ÇÅ √≥timo: {theta1_otimo:.2f}\")\n",
        "print(f\"   Custo m√≠nimo: {custo_minimo:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a paisagem em 3D - A MONTANHA DO ERRO!\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Subplot 1: Superf√≠cie 3D\n",
        "ax1 = fig.add_subplot(131, projection='3d')\n",
        "surface = ax1.plot_surface(Theta0, Theta1, Custos, \n",
        "                          cmap='viridis', alpha=0.8,\n",
        "                          linewidth=0, antialiased=True)\n",
        "\n",
        "# Marcando o ponto √≥timo\n",
        "ax1.scatter([theta0_otimo], [theta1_otimo], [custo_minimo], \n",
        "           color='red', s=100, label='M√≠nimo Global')\n",
        "\n",
        "ax1.set_xlabel('Œ∏‚ÇÄ')\n",
        "ax1.set_ylabel('Œ∏‚ÇÅ')\n",
        "ax1.set_zlabel('Custo')\n",
        "ax1.set_title('üèîÔ∏è A Montanha do Erro!', fontweight='bold')\n",
        "\n",
        "# Subplot 2: Curvas de n√≠vel\n",
        "ax2 = fig.add_subplot(132)\n",
        "contour = ax2.contour(Theta0, Theta1, Custos, levels=20, cmap='viridis')\n",
        "ax2.clabel(contour, inline=True, fontsize=8)\n",
        "ax2.scatter(theta0_otimo, theta1_otimo, color='red', s=100, marker='x', \n",
        "           linewidth=3, label='M√≠nimo Global')\n",
        "ax2.set_xlabel('Œ∏‚ÇÄ')\n",
        "ax2.set_ylabel('Œ∏‚ÇÅ')\n",
        "ax2.set_title('üó∫Ô∏è Mapa Topogr√°fico', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: Heatmap\n",
        "ax3 = fig.add_subplot(133)\n",
        "heatmap = ax3.imshow(Custos, extent=[theta0_range[0], theta0_range[-1], \n",
        "                                   theta1_range[0], theta1_range[-1]], \n",
        "                    origin='lower', cmap='viridis', aspect='auto')\n",
        "ax3.scatter(theta0_otimo, theta1_otimo, color='red', s=100, marker='x', \n",
        "           linewidth=3, label='M√≠nimo Global')\n",
        "ax3.set_xlabel('Œ∏‚ÇÄ')\n",
        "ax3.set_ylabel('Œ∏‚ÇÅ')\n",
        "ax3.set_title('üî• Mapa de Calor', fontweight='bold')\n",
        "ax3.legend()\n",
        "plt.colorbar(heatmap, ax=ax3, label='Custo')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ LIIINDO! Agora voc√™ pode VER a paisagem completa!\")\n",
        "print(\"üìç O ponto vermelho √© onde queremos chegar!\")\n",
        "print(\"üèîÔ∏è A montanha azul-escura representa custos altos (ruim)\")\n",
        "print(\"üåø A regi√£o amarela representa custos baixos (bom)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Tipos de Fun√ß√µes de Custo\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-02_img_05.png)\n\n**Bora conhecer os principais tipos de fun√ß√µes de custo!**\n\nCada problema tem sua fun√ß√£o de custo ideal. √â como escolher a ferramenta certa para cada trabalho!\n\n### 1. **MSE (Mean Squared Error)**\n$$MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2$$\n\n**Quando usar:** Regress√£o linear, quando queremos penalizar erros grandes\n\n### 2. **MAE (Mean Absolute Error)**\n$$MAE = \\frac{1}{m} \\sum_{i=1}^{m} |y_i - \\hat{y}_i|$$\n\n**Quando usar:** Quando temos outliers, mais robusta\n\n### 3. **Cross-Entropy**\n$$CE = -\\frac{1}{m} \\sum_{i=1}^{m} y_i \\log(\\hat{y}_i)$$\n\n**Quando usar:** Classifica√ß√£o, especialmente com probabilidades\n\n**üí° Dica do Pedro:** MSE penaliza erros grandes MUITO mais que erros pequenos (por causa do quadrado)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparando diferentes fun√ß√µes de custo\n",
        "\n",
        "def mse_custo(y_real, y_pred):\n",
        "    \"\"\"Mean Squared Error\"\"\"\n",
        "    return np.mean((y_real - y_pred)**2)\n",
        "\n",
        "def mae_custo(y_real, y_pred):\n",
        "    \"\"\"Mean Absolute Error\"\"\"\n",
        "    return np.mean(np.abs(y_real - y_pred))\n",
        "\n",
        "def huber_custo(y_real, y_pred, delta=1.0):\n",
        "    \"\"\"Huber Loss - Combina MSE e MAE\"\"\"\n",
        "    erro = y_real - y_pred\n",
        "    condicao = np.abs(erro) <= delta\n",
        "    quadratico = 0.5 * erro**2\n",
        "    linear = delta * np.abs(erro) - 0.5 * delta**2\n",
        "    return np.mean(np.where(condicao, quadratico, linear))\n",
        "\n",
        "# Criando dados com outliers para demonstrar as diferen√ßas\n",
        "y_real_exemplo = np.array([1, 2, 3, 4, 5])\n",
        "y_pred_normal = np.array([1.1, 2.2, 2.9, 3.8, 5.1])  # Predi√ß√µes normais\n",
        "y_pred_outlier = np.array([1.1, 2.2, 2.9, 3.8, 10])  # Uma predi√ß√£o muito errada\n",
        "\n",
        "print(\"üìä COMPARANDO FUN√á√ïES DE CUSTO:\\n\")\n",
        "\n",
        "print(\"Cen√°rio 1: Predi√ß√µes normais\")\n",
        "print(f\"Real:     {y_real_exemplo}\")\n",
        "print(f\"Predito:  {y_pred_normal}\")\n",
        "print(f\"MSE:  {mse_custo(y_real_exemplo, y_pred_normal):.3f}\")\n",
        "print(f\"MAE:  {mae_custo(y_real_exemplo, y_pred_normal):.3f}\")\n",
        "print(f\"Huber: {huber_custo(y_real_exemplo, y_pred_normal):.3f}\")\n",
        "\n",
        "print(\"\\nCen√°rio 2: COM OUTLIER (√∫ltimo valor muito errado)\")\n",
        "print(f\"Real:     {y_real_exemplo}\")\n",
        "print(f\"Predito:  {y_pred_outlier}\")\n",
        "print(f\"MSE:  {mse_custo(y_real_exemplo, y_pred_outlier):.3f}  ‚Üê EXPLODIU!\")\n",
        "print(f\"MAE:  {mae_custo(y_real_exemplo, y_pred_outlier):.3f}  ‚Üê Mais robusta\")\n",
        "print(f\"Huber: {huber_custo(y_real_exemplo, y_pred_outlier):.3f}  ‚Üê Meio termo\")\n",
        "\n",
        "print(\"\\nüéØ Viu a diferen√ßa? MSE penaliza MUITO mais os outliers!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando como cada fun√ß√£o de custo se comporta\n",
        "\n",
        "# Criando erros de -5 a 5\n",
        "erros = np.linspace(-5, 5, 100)\n",
        "\n",
        "# Calculando as perdas para cada erro\n",
        "mse_perdas = erros**2\n",
        "mae_perdas = np.abs(erros)\n",
        "huber_perdas = np.where(np.abs(erros) <= 1, \n",
        "                       0.5 * erros**2,\n",
        "                       np.abs(erros) - 0.5)\n",
        "\n",
        "# Plotando\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.plot(erros, mse_perdas, 'r-', linewidth=3, label='MSE (Quadr√°tica)', alpha=0.8)\n",
        "plt.plot(erros, mae_perdas, 'b-', linewidth=3, label='MAE (Linear)', alpha=0.8)\n",
        "plt.plot(erros, huber_perdas, 'g-', linewidth=3, label='Huber (H√≠brida)', alpha=0.8)\n",
        "\n",
        "plt.xlabel('Erro (y_real - y_pred)', fontsize=12)\n",
        "plt.ylabel('Perda', fontsize=12)\n",
        "plt.title('üìä Compara√ß√£o das Fun√ß√µes de Custo', fontsize=16, fontweight='bold')\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Destacando regi√µes importantes\n",
        "plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
        "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Adicionando anota√ß√µes\n",
        "plt.annotate('MSE penaliza\\nerros grandes!', \n",
        "            xy=(3, 9), xytext=(1.5, 15),\n",
        "            arrowprops=dict(arrowstyle='->', color='red'),\n",
        "            fontsize=10, ha='center')\n",
        "\n",
        "plt.annotate('MAE √© linear\\n(mais robusta)', \n",
        "            xy=(4, 4), xytext=(2.5, 8),\n",
        "            arrowprops=dict(arrowstyle='->', color='blue'),\n",
        "            fontsize=10, ha='center')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Agora fica claro porque cada fun√ß√£o √© √∫til em situa√ß√µes diferentes!\")\n",
        "print(\"üìà MSE: Boa para erros 'normais', ruim com outliers\")\n",
        "print(\"üìâ MAE: Robusta a outliers, mas pode ser lenta para converger\")\n",
        "print(\"üé® Huber: O melhor dos dois mundos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Anatomia de uma Fun√ß√£o: Propriedades Importantes\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-02_img_06.png)\n\n**Agora vamos destrinchar uma fun√ß√£o como um detetive!** üïµÔ∏è‚Äç‚ôÇÔ∏è\n\nQuando olhamos para uma fun√ß√£o, especialmente de custo, precisamos identificar suas **caracter√≠sticas importantes**:\n\n### üéØ **Propriedades Essenciais:**\n\n1. **Dom√≠nio**: Onde a fun√ß√£o \"vive\"\n2. **M√≠nimos/M√°ximos**: Pontos cr√≠ticos\n3. **Convexidade**: A fun√ß√£o √© uma \"tigela\" ou \"montanha\"?\n4. **Continuidade**: Tem \"saltos\" ou √© lisa?\n5. **Derivabilidade**: Podemos calcular a inclina√ß√£o?\n\n**Por que isso importa na IA?**\n- **Convexidade** ‚Üí Garantia de encontrar o m√≠nimo global\n- **Continuidade** ‚Üí Gradiente Descendente funciona\n- **Derivabilidade** ‚Üí Conseguimos calcular gradientes\n\n**üí° Dica do Pedro:** Fun√ß√µes convexas s√£o o \"santo graal\" da otimiza√ß√£o - sempre t√™m um √∫nico m√≠nimo global!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos analisar diferentes tipos de fun√ß√µes\n",
        "\n",
        "def funcao_convexa(x):\n",
        "    \"\"\"Fun√ß√£o convexa - formato de U\"\"\"\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "def funcao_concava(x):\n",
        "    \"\"\"Fun√ß√£o c√¥ncava - formato de ‚à©\"\"\"\n",
        "    return -(x**2) + 4*x + 1\n",
        "\n",
        "def funcao_nao_convexa(x):\n",
        "    \"\"\"Fun√ß√£o n√£o-convexa - tem v√°rios m√≠nimos locais\"\"\"\n",
        "    return x**4 - 4*x**3 + 4*x**2 + 1\n",
        "\n",
        "def funcao_com_ruido(x):\n",
        "    \"\"\"Fun√ß√£o com muitas oscila√ß√µes\"\"\"\n",
        "    return x**2 + 2*np.sin(5*x)\n",
        "\n",
        "# Criando dados\n",
        "x = np.linspace(-3, 5, 1000)\n",
        "\n",
        "# Calculando as fun√ß√µes\n",
        "y_convexa = funcao_convexa(x)\n",
        "y_concava = funcao_concava(x)\n",
        "y_nao_convexa = funcao_nao_convexa(x)\n",
        "y_ruido = funcao_com_ruido(x)\n",
        "\n",
        "print(\"üîç ANALISANDO DIFERENTES TIPOS DE FUN√á√ïES:\")\n",
        "print(\"\\n1. üìà Convexa: Uma √∫nica 'tigela' - IDEAL para IA\")\n",
        "print(\"2. üìâ C√¥ncava: Uma √∫nica 'montanha' - Para maximiza√ß√£o\")\n",
        "print(\"3. üé¢ N√£o-convexa: V√°rias 'tigelas' - CUIDADO com m√≠nimos locais!\")\n",
        "print(\"4. üåä Com ru√≠do: Oscila√ß√µes - Dif√≠cil de otimizar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando as diferentes propriedades\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Fun√ß√£o Convexa\n",
        "axes[0,0].plot(x, y_convexa, 'b-', linewidth=3)\n",
        "axes[0,0].set_title('üìà Fun√ß√£o CONVEXA\\n(Ideal para otimiza√ß√£o)', fontweight='bold', color='blue')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "axes[0,0].set_ylabel('f(x)')\n",
        "\n",
        "# Marcando o m√≠nimo\n",
        "min_idx = np.argmin(y_convexa)\n",
        "axes[0,0].plot(x[min_idx], y_convexa[min_idx], 'ro', markersize=10, label='M√≠nimo Global')\n",
        "axes[0,0].legend()\n",
        "\n",
        "# Fun√ß√£o C√¥ncava\n",
        "axes[0,1].plot(x, y_concava, 'g-', linewidth=3)\n",
        "axes[0,1].set_title('üìâ Fun√ß√£o C√îNCAVA\\n(Para problemas de maximiza√ß√£o)', fontweight='bold', color='green')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Marcando o m√°ximo\n",
        "max_idx = np.argmax(y_concava)\n",
        "axes[0,1].plot(x[max_idx], y_concava[max_idx], 'ro', markersize=10, label='M√°ximo Global')\n",
        "axes[0,1].legend()\n",
        "\n",
        "# Fun√ß√£o N√£o-Convexa\n",
        "axes[1,0].plot(x, y_nao_convexa, 'r-', linewidth=3)\n",
        "axes[1,0].set_title('üé¢ Fun√ß√£o N√ÉO-CONVEXA\\n(CUIDADO: M√∫ltiplos m√≠nimos!)', fontweight='bold', color='red')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "axes[1,0].set_xlabel('x')\n",
        "axes[1,0].set_ylabel('f(x)')\n",
        "\n",
        "# Marcando m√≠nimos locais\n",
        "from scipy.signal import find_peaks\n",
        "picos_neg, _ = find_peaks(-y_nao_convexa, height=-10)\n",
        "if len(picos_neg) > 0:\n",
        "    axes[1,0].plot(x[picos_neg], y_nao_convexa[picos_neg], 'ro', markersize=8, label='M√≠nimos Locais')\n",
        "    axes[1,0].legend()\n",
        "\n",
        "# Fun√ß√£o com Ru√≠do\n",
        "axes[1,1].plot(x, y_ruido, 'purple', linewidth=2)\n",
        "axes[1,1].set_title('üåä Fun√ß√£o com OSCILA√á√ïES\\n(Dif√≠cil otimiza√ß√£o)', fontweight='bold', color='purple')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "axes[1,1].set_xlabel('x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéØ LI√á√ïES IMPORTANTES:\")\n",
        "print(\"‚úÖ Fun√ß√µes CONVEXAS: Sempre encontramos o m√≠nimo global!\")\n",
        "print(\"‚ö†Ô∏è  Fun√ß√µes N√ÉO-CONVEXAS: Podemos ficar presos em m√≠nimos locais!\")\n",
        "print(\"üåä Fun√ß√µes com RU√çDO: Precisamos de t√©cnicas especiais!\")\n",
        "print(\"\\nüí° Por isso escolher a arquitetura certa √© T√ÉO importante!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Aplica√ß√£o Pr√°tica: Fun√ß√£o de Custo Real\n\n**Bora fazer um exemplo REAL de Machine Learning!**\n\nVamos simular um problema de **predi√ß√£o de pre√ßos de casas** e ver como nossa fun√ß√£o de custo se comporta.\n\n**Cen√°rio:**\n- üè† **Problema**: Prever pre√ßo de casas baseado no tamanho\n- üìä **Dados**: Tamanho da casa (m¬≤) vs Pre√ßo (R$)\n- üéØ **Objetivo**: Encontrar a melhor linha que ajusta os dados\n- üìà **Modelo**: $\\text{Pre√ßo} = \\theta_0 + \\theta_1 \\times \\text{Tamanho}$\n\n**Este √© um exemplo t√≠pico que voc√™ vai encontrar na vida real!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando dataset realista de pre√ßos de casas\n",
        "np.random.seed(42)\n",
        "\n",
        "# Dados de casas (tamanho em m¬≤, pre√ßo base em R$)\n",
        "n_casas = 100\n",
        "tamanho_casa = np.random.normal(120, 40, n_casas)  # Tamanho m√©dio 120m¬≤\n",
        "tamanho_casa = np.clip(tamanho_casa, 50, 300)      # Entre 50 e 300m¬≤\n",
        "\n",
        "# Pre√ßo real: R$ 2000/m¬≤ + ru√≠do + fatores extras\n",
        "preco_base = 2000  # R$ por m¬≤\n",
        "preco_casa = (preco_base * tamanho_casa + \n",
        "              np.random.normal(0, 50000, n_casas) +  # Ru√≠do\n",
        "              tamanho_casa * np.random.normal(500, 200, n_casas))  # Fatores extras\n",
        "\n",
        "# Garantindo pre√ßos positivos e realistas\n",
        "preco_casa = np.clip(preco_casa, 100000, 1000000)\n",
        "\n",
        "print(\"üè† DATASET DE CASAS CRIADO!\")\n",
        "print(f\"   üìä {n_casas} casas no dataset\")\n",
        "print(f\"   üìè Tamanho m√©dio: {np.mean(tamanho_casa):.1f}m¬≤\")\n",
        "print(f\"   üí∞ Pre√ßo m√©dio: R$ {np.mean(preco_casa):,.0f}\")\n",
        "print(f\"   üí∏ Pre√ßo por m¬≤: R$ {np.mean(preco_casa/tamanho_casa):,.0f}\")\n",
        "\n",
        "# Visualizando nossos dados\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(tamanho_casa, preco_casa, alpha=0.6, s=50, c='blue', edgecolors='black')\n",
        "plt.xlabel('Tamanho da Casa (m¬≤)', fontsize=12)\n",
        "plt.ylabel('Pre√ßo da Casa (R$)', fontsize=12)\n",
        "plt.title('üè† Dataset: Tamanho vs Pre√ßo das Casas', fontsize=16, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Formatando eixo Y para mostrar valores em reais\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'R$ {x/1000:.0f}K'))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéØ Agora vamos treinar um modelo para prever o pre√ßo baseado no tamanho!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando nosso modelo de regress√£o linear\n",
        "\n",
        "class ModeloLinearSimples:\n",
        "    def __init__(self):\n",
        "        self.theta0 = 0  # Intercepto\n",
        "        self.theta1 = 0  # Coeficiente angular\n",
        "        self.historico_custo = []\n",
        "    \n",
        "    def prever(self, x):\n",
        "        \"\"\"Faz predi√ß√µes usando o modelo atual\"\"\"\n",
        "        return self.theta0 + self.theta1 * x\n",
        "    \n",
        "    def calcular_custo(self, x, y):\n",
        "        \"\"\"Calcula o custo MSE\"\"\"\n",
        "        m = len(x)\n",
        "        predicoes = self.prever(x)\n",
        "        custo = (1/(2*m)) * np.sum((predicoes - y)**2)\n",
        "        return custo\n",
        "    \n",
        "    def treinar_analitico(self, x, y):\n",
        "        \"\"\"Resolve analiticamente (f√≥rmula fechada)\"\"\"\n",
        "        # F√≥rmulas da regress√£o linear\n",
        "        x_mean = np.mean(x)\n",
        "        y_mean = np.mean(y)\n",
        "        \n",
        "        numerador = np.sum((x - x_mean) * (y - y_mean))\n",
        "        denominador = np.sum((x - x_mean)**2)\n",
        "        \n",
        "        self.theta1 = numerador / denominador\n",
        "        self.theta0 = y_mean - self.theta1 * x_mean\n",
        "        \n",
        "        return self.theta0, self.theta1\n",
        "\n",
        "# Criando e treinando o modelo\n",
        "modelo = ModeloLinearSimples()\n",
        "\n",
        "print(\"ü§ñ TREINANDO O MODELO...\")\n",
        "print(f\"   üìä Dados de entrada: {len(tamanho_casa)} casas\")\n",
        "\n",
        "# Calculando custo inicial (par√¢metros zerados)\n",
        "custo_inicial = modelo.calcular_custo(tamanho_casa, preco_casa)\n",
        "print(f\"   üí∏ Custo inicial (Œ∏‚ÇÄ=0, Œ∏‚ÇÅ=0): {custo_inicial:,.0f}\")\n",
        "\n",
        "# Treinando (solu√ß√£o √≥tima anal√≠tica)\n",
        "theta0_otimo, theta1_otimo = modelo.treinar_analitico(tamanho_casa, preco_casa)\n",
        "custo_final = modelo.calcular_custo(tamanho_casa, preco_casa)\n",
        "\n",
        "print(f\"\\n‚úÖ MODELO TREINADO!\")\n",
        "print(f\"   üéØ Œ∏‚ÇÄ (intercepto): R$ {theta0_otimo:,.0f}\")\n",
        "print(f\"   üìà Œ∏‚ÇÅ (pre√ßo/m¬≤): R$ {theta1_otimo:,.0f} por m¬≤\")\n",
        "print(f\"   üí∞ Custo final: {custo_final:,.0f}\")\n",
        "print(f\"   üìâ Redu√ß√£o do custo: {((custo_inicial-custo_final)/custo_inicial)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nüè† INTERPRETA√á√ÉO DO MODELO:\")\n",
        "print(f\"   üí° Pre√ßo base (casa de 0m¬≤): R$ {theta0_otimo:,.0f}\")\n",
        "print(f\"   üí° Cada m¬≤ adicional custa: R$ {theta1_otimo:,.0f}\")\n",
        "print(f\"   üí° F√≥rmula: Pre√ßo = {theta0_otimo:,.0f} + {theta1_otimo:,.0f} √ó Tamanho\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando o resultado final\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Subplot 1: Dados + Modelo ajustado\n",
        "ax1.scatter(tamanho_casa, preco_casa, alpha=0.6, s=50, c='lightblue', \n",
        "           edgecolors='black', label='Dados Reais')\n",
        "\n",
        "# Linha do modelo\n",
        "x_linha = np.linspace(tamanho_casa.min(), tamanho_casa.max(), 100)\n",
        "y_linha = modelo.prever(x_linha)\n",
        "ax1.plot(x_linha, y_linha, 'r-', linewidth=3, \n",
        "        label=f'Modelo: y = {theta0_otimo:,.0f} + {theta1_otimo:,.0f}x')\n",
        "\n",
        "ax1.set_xlabel('Tamanho da Casa (m¬≤)', fontsize=12)\n",
        "ax1.set_ylabel('Pre√ßo da Casa (R$)', fontsize=12)\n",
        "ax1.set_title('üè† Modelo Treinado vs Dados Reais', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'R$ {x/1000:.0f}K'))\n",
        "\n",
        "# Subplot 2: Res√≠duos (erros)\n",
        "predicoes = modelo.prever(tamanho_casa)\n",
        "residuos = preco_casa - predicoes\n",
        "\n",
        "ax2.scatter(predicoes, residuos, alpha=0.6, s=50, c='orange', edgecolors='black')\n",
        "ax2.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Erro = 0')\n",
        "ax2.set_xlabel('Predi√ß√µes (R$)', fontsize=12)\n",
        "ax2.set_ylabel('Res√≠duos (R$)', fontsize=12)\n",
        "ax2.set_title('üìä An√°lise dos Res√≠duos', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'R$ {x/1000:.0f}K'))\n",
        "ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'R$ {x/1000:.0f}K'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculando m√©tricas de performance\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "r2 = r2_score(preco_casa, predicoes)\n",
        "mae = mean_absolute_error(preco_casa, predicoes)\n",
        "rmse = np.sqrt(np.mean(residuos**2))\n",
        "\n",
        "print(\"üìä M√âTRICAS DE PERFORMANCE:\")\n",
        "print(f\"   üéØ R¬≤ Score: {r2:.3f} ({r2*100:.1f}% da vari√¢ncia explicada)\")\n",
        "print(f\"   üìè MAE: R$ {mae:,.0f} (erro m√©dio absoluto)\")\n",
        "print(f\"   üìê RMSE: R$ {rmse:,.0f} (erro quadr√°tico m√©dio)\")\n",
        "\n",
        "print(f\"\\nüè† TESTE PR√ÅTICO:\")\n",
        "tamanhos_teste = [80, 120, 200]\n",
        "for tamanho in tamanhos_teste:\n",
        "    preco_previsto = modelo.prever(tamanho)\n",
        "    print(f\"   Casa de {tamanho}m¬≤: R$ {preco_previsto:,.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Exerc√≠cio Pr√°tico 1: Explorando Fun√ß√µes\n\n\n\n**Agora √© sua vez de colocar a m√£o na massa!** üöÄ\n\n**Desafio:** Crie e analise diferentes fun√ß√µes de custo para entender seu comportamento.\n\n**Tarefas:**\n1. Implemente uma fun√ß√£o c√∫bica: $f(x) = ax^3 + bx^2 + cx + d$\n2. Teste diferentes valores para os par√¢metros $a$, $b$, $c$, $d$\n3. Visualize como cada par√¢metro afeta o formato da fun√ß√£o\n4. Identifique onde est√£o os m√≠nimos e m√°ximos\n\n**üí° Dica do Pedro:** Use `np.gradient()` para calcular aproximadamente a derivada e encontrar pontos cr√≠ticos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 1: Complete o c√≥digo abaixo\n",
        "\n",
        "def funcao_cubica(x, a, b, c, d):\n",
        "    \"\"\"Fun√ß√£o c√∫bica: f(x) = ax¬≥ + bx¬≤ + cx + d\"\"\"\n",
        "    # TODO: Implemente a fun√ß√£o c√∫bica\n",
        "    return # Sua implementa√ß√£o aqui\n",
        "\n",
        "def encontrar_pontos_criticos(x, y):\n",
        "    \"\"\"Encontra pontos onde a derivada ‚âà 0\"\"\"\n",
        "    # TODO: Use np.gradient para calcular a derivada\n",
        "    derivada = # Sua implementa√ß√£o aqui\n",
        "    \n",
        "    # TODO: Encontre onde a derivada muda de sinal\n",
        "    # Dica: use np.diff(np.sign(derivada)) para detectar mudan√ßas de sinal\n",
        "    \n",
        "    return pontos_criticos\n",
        "\n",
        "# Teste seus par√¢metros aqui\n",
        "x = np.linspace(-3, 3, 1000)\n",
        "\n",
        "# TODO: Teste diferentes valores de par√¢metros\n",
        "a, b, c, d = 1, -3, 2, 1  # Modifique estes valores!\n",
        "\n",
        "y = funcao_cubica(x, a, b, c, d)\n",
        "pontos_criticos = encontrar_pontos_criticos(x, y)\n",
        "\n",
        "# Visualiza√ß√£o\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(x, y, 'b-', linewidth=3, label=f'f(x) = {a}x¬≥ + {b}x¬≤ + {c}x + {d}')\n",
        "\n",
        "# TODO: Marque os pontos cr√≠ticos no gr√°fico\n",
        "# plt.scatter(...)\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.title('üéØ Sua Fun√ß√£o C√∫bica', fontsize=16, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ AN√ÅLISE DA SUA FUN√á√ÉO:\")\n",
        "print(f\"   üìä Par√¢metros: a={a}, b={b}, c={c}, d={d}\")\n",
        "print(f\"   üéØ Pontos cr√≠ticos encontrados: {len(pontos_criticos)}\")\n",
        "print(\"\\nüí° Experimente diferentes valores e veja como a fun√ß√£o muda!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Exerc√≠cio Pr√°tico 2: Criando sua Pr√≥pria Fun√ß√£o de Custo\n\n**Desafio Avan√ßado:** Implemente e compare diferentes fun√ß√µes de custo!\n\n**Cen√°rio:** Voc√™ tem um dataset com alguns outliers e precisa escolher a melhor fun√ß√£o de custo.\n\n**Tarefas:**\n1. Implemente MSE, MAE e Huber Loss\n2. Teste com dados que t√™m outliers\n3. Compare visualmente qual fun√ß√£o √© mais robusta\n4. Analise qual seria melhor para seu problema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 2: Implemente as fun√ß√µes de custo\n",
        "\n",
        "# Dados com outliers para teste\n",
        "y_true = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
        "y_pred_normal = np.array([1.1, 2.1, 2.9, 4.1, 4.9, 6.1, 7.1, 7.9])  # Predi√ß√µes normais\n",
        "y_pred_outlier = np.array([1.1, 2.1, 2.9, 15, 4.9, 6.1, 7.1, 7.9])  # Com outlier\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    \"\"\"TODO: Implemente Mean Squared Error\"\"\"\n",
        "    return # Sua implementa√ß√£o\n",
        "\n",
        "def mae_loss(y_true, y_pred):\n",
        "    \"\"\"TODO: Implemente Mean Absolute Error\"\"\"\n",
        "    return # Sua implementa√ß√£o\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta=1.0):\n",
        "    \"\"\"TODO: Implemente Huber Loss\"\"\"\n",
        "    # Dica: Use np.where para implementar a condi√ß√£o\n",
        "    return # Sua implementa√ß√£o\n",
        "\n",
        "# TODO: Teste suas implementa√ß√µes\n",
        "print(\"üìä COMPARANDO FUN√á√ïES DE CUSTO:\\n\")\n",
        "\n",
        "print(\"Cen√°rio 1: Predi√ß√µes normais\")\n",
        "print(f\"MSE:   {mse_loss(y_true, y_pred_normal):.3f}\")\n",
        "print(f\"MAE:   {mae_loss(y_true, y_pred_normal):.3f}\")\n",
        "print(f\"Huber: {huber_loss(y_true, y_pred_normal):.3f}\")\n",
        "\n",
        "print(\"\\nCen√°rio 2: Com outlier\")\n",
        "print(f\"MSE:   {mse_loss(y_true, y_pred_outlier):.3f}\")\n",
        "print(f\"MAE:   {mae_loss(y_true, y_pred_outlier):.3f}\")\n",
        "print(f\"Huber: {huber_loss(y_true, y_pred_outlier):.3f}\")\n",
        "\n",
        "# TODO: Crie uma visualiza√ß√£o comparando as tr√™s fun√ß√µes\n",
        "# Dica: Use subplots para mostrar cada cen√°rio\n",
        "\n",
        "print(\"\\nü§î PERGUNTA PARA REFLEX√ÉO:\")\n",
        "print(\"Qual fun√ß√£o de custo voc√™ usaria se seus dados tivessem muitos outliers? Por qu√™?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Conectando com os Pr√≥ximos M√≥dulos\n\n```mermaid\ngraph TD\n    A[\"üìä M√≥dulo 2: Fun√ß√µes e Gr√°ficos\"] --> B[\"üéØ M√≥dulo 3: Limites\"]\n    B --> C[\"üìà M√≥dulo 4: Derivadas\"]\n    C --> D[\"‚õìÔ∏è M√≥dulo 6: Regra da Cadeia\"]\n    D --> E[\"üèîÔ∏è M√≥dulo 11: Gradiente Descendente\"]\n    \n    A --> F[\"Visualizar a montanha\"]\n    B --> G[\"Continuidade da fun√ß√£o\"]\n    C --> H[\"Inclina√ß√£o em cada ponto\"]\n    D --> I[\"Backpropagation\"]\n    E --> J[\"Otimiza√ß√£o final\"]\n```\n\n**üéØ Recapitulando nossa jornada:**\n\n**‚úÖ No M√≥dulo 1:** Entendemos que IA √© otimiza√ß√£o e visualizamos a \"montanha do erro\"\n\n**‚úÖ No M√≥dulo 2 (atual):** Aprendemos a:\n- Criar e visualizar fun√ß√µes matem√°ticas\n- Entender fun√ß√µes de custo (MSE, MAE, Huber)\n- Mapear a paisagem completa do erro\n- Identificar propriedades importantes (convexidade, m√≠nimos)\n\n**üöÄ Pr√≥ximos m√≥dulos:**\n- **M√≥dulo 3**: Como definir quando uma fun√ß√£o √© \"suave\" (limites)\n- **M√≥dulo 4**: Como medir a \"inclina√ß√£o\" da montanha (derivadas)\n- **M√≥dulo 6**: Como calcular gradientes em redes neurais (regra da cadeia)\n- **M√≥dulo 11**: Como \"descer\" a montanha eficientemente (gradiente descendente)\n\n**üí° Dica do Pedro:** Tudo que vimos aqui vai ser fundamental para entender como os algoritmos de IA realmente funcionam!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Resumo: O que Aprendemos\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-02_img_08.png)\n\n**Parab√©ns! Voc√™ concluiu o M√≥dulo 2! üéä**\n\n### üß† **Conceitos Fundamentais Dominados:**\n\n1. **üîß Fun√ß√µes Matem√°ticas**\n   - O que s√£o e como funcionam\n   - Entrada ‚Üí Processamento ‚Üí Sa√≠da\n   - Tipos: linear, quadr√°tica, exponencial\n\n2. **üëÅÔ∏è Import√¢ncia da Visualiza√ß√£o**\n   - C√©rebro processa visual 50.000x mais r√°pido\n   - Identifica padr√µes instantaneamente\n   - Detecta problemas facilmente\n\n3. **üí∞ Fun√ß√µes de Custo**\n   - MSE: Para regress√£o, penaliza erros grandes\n   - MAE: Robusta a outliers\n   - Huber: Melhor dos dois mundos\n\n4. **üó∫Ô∏è Mapeamento da Paisagem**\n   - Visualiza√ß√£o 3D da fun√ß√£o de custo\n   - Identifica√ß√£o de m√≠nimos globais\n   - Curvas de n√≠vel e mapas de calor\n\n5. **üîç Propriedades das Fun√ß√µes**\n   - Convexidade (santo graal!)\n   - Continuidade e derivabilidade\n   - M√≠nimos locais vs globais\n\n### üéØ **Skills Pr√°ticas Desenvolvidas:**\n- ‚úÖ Implementar fun√ß√µes de custo do zero\n- ‚úÖ Criar visualiza√ß√µes informativas\n- ‚úÖ Analisar propriedades matem√°ticas\n- ‚úÖ Comparar diferentes abordagens\n- ‚úÖ Resolver problemas reais (previs√£o de pre√ßos)\n\n### üöÄ **Prepara√ß√£o para os Pr√≥ximos M√≥dulos:**\n- üéØ **Limites**: Definir continuidade e suavidade\n- üìà **Derivadas**: Medir inclina√ß√£o e taxa de varia√ß√£o\n- ‚õìÔ∏è **Regra da Cadeia**: Base do backpropagation\n- üèîÔ∏è **Gradiente Descendente**: Otimiza√ß√£o pr√°tica\n\n**üí° Frase do Pedro:** *\"Agora voc√™ n√£o apenas sabe que existe uma montanha do erro - voc√™ sabe como mape√°-la, visualiz√°-la e entender sua geografia! Isso √© fundamental para tudo que vem pela frente!\"*\n\n**üéä Continue firme na jornada! O pr√≥ximo m√≥dulo sobre Limites vai ser LINDO!**"
      ]
    }
  ]
}