{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“Š FunÃ§Ãµes e GrÃ¡ficos: Visualizando o Problema\n\n## *MÃ³dulo 2 - CÃ¡lculo para IA*\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cÃ¡lculo-para-ia-modulo-02_img_01.png)\n\nFala galera! Sou o **Pedro Guth** e hoje vamos mergulhar no mundo das **funÃ§Ãµes e grÃ¡ficos**! ğŸš€\n\nLembra do MÃ³dulo 1 quando falamos sobre a **montanha do erro**? Pois Ã©, hoje vamos aprender a **visualizar** essa montanha de verdade! \n\n**TÃ¡, mas por que isso Ã© importante?**\n\nImagina que vocÃª estÃ¡ dirigindo no Google Maps sem ver o mapa - sÃ³ ouvindo as instruÃ§Ãµes. Meio complicado, nÃ©? Ã‰ exatamente isso que acontece quando tentamos entender IA sem visualizar as funÃ§Ãµes!\n\n**Neste notebook vocÃª vai aprender:**\n- O que sÃ£o funÃ§Ãµes matemÃ¡ticas de verdade\n- Por que visualizar Ã© TÃƒO importante\n- Como criar e interpretar grÃ¡ficos\n- FunÃ§Ãµes de custo na prÃ¡tica\n- E muito mais!\n\n**Bora comeÃ§ar!** ğŸ¯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Importando nossas ferramentas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurando o estilo dos grÃ¡ficos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configurando para aparecer grÃ¡ficos inline\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"ğŸ‰ Bibliotecas carregadas! Vamos visualizar!\")\n",
        "print(\"ğŸ“Š Numpy version:\", np.__version__)\n",
        "print(\"ğŸ“ˆ Matplotlib pronto para os grÃ¡ficos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤” O que sÃ£o FunÃ§Ãµes MatemÃ¡ticas?\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cÃ¡lculo-para-ia-modulo-02_img_02.png)\n\n**TÃ¡, mas o que Ã© uma funÃ§Ã£o afinal?**\n\nPensa numa **mÃ¡quina de salgadinho**: vocÃª coloca uma moeda (entrada), ela processa, e sai um salgadinho (saÃ­da). A funÃ§Ã£o Ã© exatamente isso!\n\n**Matematicamente:**\n$$f(x) = y$$\n\nOnde:\n- $x$ Ã© a **entrada** (input)\n- $f$ Ã© a **funÃ§Ã£o** (processamento) \n- $y$ Ã© a **saÃ­da** (output)\n\n**Exemplos clÃ¡ssicos:**\n- $f(x) = 2x + 1$ (funÃ§Ã£o linear)\n- $f(x) = x^2$ (funÃ§Ã£o quadrÃ¡tica)\n- $f(x) = e^x$ (funÃ§Ã£o exponencial)\n\n**Na IA, as funÃ§Ãµes representam:**\n- **Modelos**: $f(x) = \\text{prediÃ§Ã£o}$\n- **Custos**: $C(\\theta) = \\text{erro do modelo}$\n- **AtivaÃ§Ãµes**: $\\sigma(x) = \\text{neurÃ´nio ativado}$\n\n**ğŸ’¡ Dica do Pedro:** Toda vez que vocÃª vÃª algo do tipo \"entrada â†’ processamento â†’ saÃ­da\", vocÃª estÃ¡ vendo uma funÃ§Ã£o!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar nossas primeiras funÃ§Ãµes!\n",
        "\n",
        "def funcao_linear(x):\n",
        "    \"\"\"FunÃ§Ã£o linear: f(x) = 2x + 1\"\"\"\n",
        "    return 2*x + 1\n",
        "\n",
        "def funcao_quadratica(x):\n",
        "    \"\"\"FunÃ§Ã£o quadrÃ¡tica: f(x) = xÂ²\"\"\"\n",
        "    return x**2\n",
        "\n",
        "def funcao_exponencial(x):\n",
        "    \"\"\"FunÃ§Ã£o exponencial: f(x) = e^x\"\"\"\n",
        "    return np.exp(x)\n",
        "\n",
        "# Testando nossas funÃ§Ãµes\n",
        "print(\"ğŸ§® Testando nossas funÃ§Ãµes:\")\n",
        "print(f\"Linear f(3) = {funcao_linear(3)}\")\n",
        "print(f\"QuadrÃ¡tica f(3) = {funcao_quadratica(3)}\")\n",
        "print(f\"Exponencial f(1) = {funcao_exponencial(1):.2f}\")\n",
        "\n",
        "print(\"\\nğŸ¯ Liiindo! FunÃ§Ãµes criadas com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ˆ Por que Visualizar Ã© Fundamental?\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cÃ¡lculo-para-ia-modulo-02_img_03.png)\n\n**Imagine tentar entender o Brasil sÃ³ olhando coordenadas GPS vs usar o Google Maps...**\n\nÃ‰ exatamente essa a diferenÃ§a entre trabalhar com funÃ§Ãµes sÃ³ numericamente vs visualizÃ¡-las!\n\n**ğŸ§  O cÃ©rebro humano processa informaÃ§Ãµes visuais:**\n- **50.000x mais rÃ¡pido** que texto\n- Identifica **padrÃµes** instantaneamente\n- Detecta **anomalias** facilmente\n\n**Na IA, visualizar funÃ§Ãµes nos ajuda a:**\n\n1. **Entender o comportamento**: Como a funÃ§Ã£o \"se comporta\"?\n2. **Identificar mÃ­nimos/mÃ¡ximos**: Onde estÃ£o os pontos Ã³timos?\n3. **Ver a inclinaÃ§Ã£o**: QuÃ£o Ã­ngreme Ã© a \"montanha\"?\n4. **Detectar problemas**: ExplosÃ£o de gradientes, overfitting...\n5. **Otimizar modelos**: Escolher hiperparÃ¢metros melhores\n\n**ğŸ’¡ Dica do Pedro:** Se vocÃª nÃ£o consegue desenhar sua funÃ§Ã£o de custo, provavelmente nÃ£o entendeu seu modelo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos ver a diferenÃ§a entre nÃºmeros e grÃ¡ficos!\n",
        "\n",
        "# Criando dados\n",
        "x = np.linspace(-3, 3, 100)\n",
        "y_linear = funcao_linear(x)\n",
        "y_quad = funcao_quadratica(x)\n",
        "\n",
        "# Primeiro: sÃ³ nÃºmeros (difÃ­cil de entender)\n",
        "print(\"ğŸ“Š APENAS NÃšMEROS (confuso):\")\n",
        "print(\"x  | Linear | QuadrÃ¡tica\")\n",
        "print(\"-\" * 25)\n",
        "for i in range(0, len(x), 20):\n",
        "    print(f\"{x[i]:4.1f} | {y_linear[i]:6.1f} | {y_quad[i]:8.1f}\")\n",
        "\n",
        "print(\"\\nğŸ¤” Conseguiu entender o padrÃ£o? DifÃ­cil nÃ©...\")\n",
        "print(\"\\nğŸ‘‡ Agora vamos VISUALIZAR!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agora com grÃ¡ficos (muito mais fÃ¡cil!)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# FunÃ§Ã£o Linear\n",
        "ax1.plot(x, y_linear, 'b-', linewidth=3, label='f(x) = 2x + 1')\n",
        "ax1.set_title('ğŸ“ˆ FunÃ§Ã£o Linear', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('f(x)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "# FunÃ§Ã£o QuadrÃ¡tica\n",
        "ax2.plot(x, y_quad, 'r-', linewidth=3, label='f(x) = xÂ²')\n",
        "ax2.set_title('ğŸ“‰ FunÃ§Ã£o QuadrÃ¡tica', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('f(x)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¯ Agora sim! DÃ¡ pra ver claramente:\")\n",
        "print(\"ğŸ“ˆ Linear: sempre cresce na mesma taxa\")\n",
        "print(\"ğŸ“‰ QuadrÃ¡tica: tem formato de U (parÃ¡bola)\")\n",
        "print(\"\\nğŸ’¡ A visualizaÃ§Ã£o revela TUDO instantaneamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ FunÃ§Ãµes de Custo: O CoraÃ§Ã£o da IA\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cÃ¡lculo-para-ia-modulo-02_img_04.png)\n\n**Agora vamos ao que interessa: FUNÃ‡Ã•ES DE CUSTO!**\n\nLembra da **montanha do erro** do MÃ³dulo 1? Pois Ã©, essa montanha Ã© matematicamente uma **funÃ§Ã£o de custo**!\n\n**ğŸ¤” TÃ¡, mas o que Ã© uma funÃ§Ã£o de custo?**\n\nÃ‰ uma funÃ§Ã£o que **mede o quÃ£o errado** seu modelo estÃ¡. Quanto maior o valor, pior o modelo!\n\n**Matematicamente:**\n$$C(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n\nOnde:\n- $C(\\theta)$ = funÃ§Ã£o de custo\n- $\\theta$ = parÃ¢metros do modelo\n- $m$ = nÃºmero de exemplos\n- $h_\\theta(x^{(i)})$ = prediÃ§Ã£o do modelo\n- $y^{(i)}$ = valor real\n\n**Tipos mais comuns:**\n1. **MSE (Mean Squared Error)**: Para regressÃ£o\n2. **Cross-Entropy**: Para classificaÃ§Ã£o\n3. **MAE (Mean Absolute Error)**: Mais robusta\n\n**ğŸ’¡ Dica do Pedro:** O objetivo da IA Ã© encontrar o **fundo do vale** dessa montanha!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar dados sintÃ©ticos para nosso exemplo\n",
        "np.random.seed(42)  # Para reprodutibilidade\n",
        "\n",
        "# Gerando dados de exemplo: y = 2x + 1 + ruÃ­do\n",
        "x_data = np.linspace(0, 10, 50)\n",
        "y_real = 2 * x_data + 1  # FunÃ§Ã£o real (sem ruÃ­do)\n",
        "y_data = y_real + np.random.normal(0, 2, len(x_data))  # Com ruÃ­do\n",
        "\n",
        "print(\"ğŸ“Š Dataset criado!\")\n",
        "print(f\"   ğŸ“ˆ {len(x_data)} pontos de dados\")\n",
        "print(f\"   ğŸ¯ FunÃ§Ã£o real: y = 2x + 1\")\n",
        "print(f\"   ğŸ”€ RuÃ­do adicionado para simular dados reais\")\n",
        "\n",
        "# Visualizando nossos dados\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x_data, y_data, alpha=0.6, s=50, label='Dados com ruÃ­do')\n",
        "plt.plot(x_data, y_real, 'r--', linewidth=2, label='FunÃ§Ã£o real: y = 2x + 1')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('ğŸ“Š Nossos Dados de Treinamento', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ¯ Agora vamos criar um modelo para tentar 'adivinhar' a linha vermelha!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definindo nosso modelo linear simples: h(x) = Î¸â‚€ + Î¸â‚x\n",
        "\n",
        "def modelo_linear(x, theta0, theta1):\n",
        "    \"\"\"Nosso modelo: h(x) = Î¸â‚€ + Î¸â‚x\"\"\"\n",
        "    return theta0 + theta1 * x\n",
        "\n",
        "def funcao_custo_mse(theta0, theta1, x, y):\n",
        "    \"\"\"FunÃ§Ã£o de custo MSE\"\"\"\n",
        "    m = len(x)\n",
        "    predicoes = modelo_linear(x, theta0, theta1)\n",
        "    custo = (1/(2*m)) * np.sum((predicoes - y)**2)\n",
        "    return custo\n",
        "\n",
        "# Testando diferentes parÃ¢metros\n",
        "print(\"ğŸ§® Testando diferentes parÃ¢metros:\")\n",
        "print(\"\\nÎ¸â‚€  | Î¸â‚  | Custo\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "parametros_teste = [\n",
        "    (0, 0),    # Muito ruim\n",
        "    (1, 1),    # RazoÃ¡vel\n",
        "    (1, 2),    # Melhor\n",
        "    (1, 2.1),  # Ainda melhor\n",
        "    (0.8, 2.05) # PrÃ³ximo do Ã³timo\n",
        "]\n",
        "\n",
        "for theta0, theta1 in parametros_teste:\n",
        "    custo = funcao_custo_mse(theta0, theta1, x_data, y_data)\n",
        "    print(f\"{theta0:3.1f} | {theta1:3.1f} | {custo:6.2f}\")\n",
        "\n",
        "print(\"\\nğŸ¯ Repara como o custo vai diminuindo conforme nos aproximamos dos valores reais!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos visualizar como diferentes modelos se ajustam aos dados\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "parametros = [\n",
        "    (0, 0, \"Modelo TerrÃ­vel\"),\n",
        "    (1, 1, \"Modelo Ruim\"), \n",
        "    (1, 2, \"Modelo Bom\"),\n",
        "    (1, 2.1, \"Modelo Muito Bom\"),\n",
        "    (0.8, 2.05, \"Modelo Excelente\"),\n",
        "    (1, 2, \"ComparaÃ§Ã£o Final\")\n",
        "]\n",
        "\n",
        "for i, (theta0, theta1, titulo) in enumerate(parametros):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # Dados\n",
        "    ax.scatter(x_data, y_data, alpha=0.6, s=30, label='Dados')\n",
        "    \n",
        "    if i == 5:  # Ãšltima subplot - comparaÃ§Ã£o\n",
        "        ax.plot(x_data, y_real, 'g--', linewidth=3, label='Real: y = 2x + 1')\n",
        "        ax.plot(x_data, modelo_linear(x_data, 1, 2), 'r-', linewidth=3, label='Modelo: y = 1 + 2x')\n",
        "    else:\n",
        "        # Modelo atual\n",
        "        y_pred = modelo_linear(x_data, theta0, theta1)\n",
        "        ax.plot(x_data, y_pred, 'r-', linewidth=2, label=f'y = {theta0} + {theta1}x')\n",
        "        \n",
        "        # FunÃ§Ã£o real\n",
        "        ax.plot(x_data, y_real, 'g--', alpha=0.7, label='Real')\n",
        "    \n",
        "    custo = funcao_custo_mse(theta0, theta1, x_data, y_data)\n",
        "    ax.set_title(f'{titulo}\\nCusto: {custo:.2f}', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¯ Viu como a visualizaÃ§Ã£o mostra CLARAMENTE qual modelo Ã© melhor?\")\n",
        "print(\"ğŸ“‰ Quanto menor o custo, melhor o ajuste aos dados!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ—ºï¸ Mapeando a Paisagem do Custo\n\nAgora vem a parte mais **LIIIINDA** do negÃ³cio: vamos mapear toda a paisagem da funÃ§Ã£o de custo!\n\n**ğŸ¤” O que isso significa?**\n\nSignifica que vamos testar **TODOS** os possÃ­veis valores de $\\theta_0$ e $\\theta_1$ e ver como fica o custo!\n\n**Matematicamente, vamos criar:**\n$$C(\\theta_0, \\theta_1) = \\text{superfÃ­cie 3D}$$\n\n**Por que isso Ã© importante?**\n- Visualizar onde estÃ¡ o **mÃ­nimo global**\n- Entender a **topografia** do problema\n- Ver se existem **mÃ­nimos locais**\n- Planejar a **estratÃ©gia de otimizaÃ§Ã£o**\n\n**ğŸ’¡ Dica do Pedro:** Isso Ã© exatamente o que o Gradiente Descendente vai \"navegar\" nos prÃ³ximos mÃ³dulos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando a grade de parÃ¢metros para mapear a paisagem\n",
        "theta0_range = np.linspace(-2, 4, 50)\n",
        "theta1_range = np.linspace(0, 4, 50)\n",
        "\n",
        "# Criando a grade 2D\n",
        "Theta0, Theta1 = np.meshgrid(theta0_range, theta1_range)\n",
        "\n",
        "# Calculando o custo para cada combinaÃ§Ã£o de parÃ¢metros\n",
        "Custos = np.zeros_like(Theta0)\n",
        "\n",
        "print(\"ğŸ—ºï¸ Mapeando a paisagem do custo...\")\n",
        "print(f\"   ğŸ“Š Testando {len(theta0_range)} Ã— {len(theta1_range)} = {len(theta0_range) * len(theta1_range)} combinaÃ§Ãµes\")\n",
        "\n",
        "for i in range(len(theta0_range)):\n",
        "    for j in range(len(theta1_range)):\n",
        "        Custos[j, i] = funcao_custo_mse(Theta0[j, i], Theta1[j, i], x_data, y_data)\n",
        "\n",
        "print(\"âœ… Paisagem mapeada!\")\n",
        "print(f\"   ğŸ“ˆ Custo mÃ­nimo: {np.min(Custos):.2f}\")\n",
        "print(f\"   ğŸ“‰ Custo mÃ¡ximo: {np.max(Custos):.2f}\")\n",
        "\n",
        "# Encontrando o ponto de mÃ­nimo\n",
        "min_idx = np.unravel_index(np.argmin(Custos), Custos.shape)\n",
        "theta0_otimo = Theta0[min_idx]\n",
        "theta1_otimo = Theta1[min_idx]\n",
        "custo_minimo = Custos[min_idx]\n",
        "\n",
        "print(f\"\\nğŸ¯ PONTO Ã“TIMO ENCONTRADO:\")\n",
        "print(f\"   Î¸â‚€ Ã³timo: {theta0_otimo:.2f}\")\n",
        "print(f\"   Î¸â‚ Ã³timo: {theta1_otimo:.2f}\")\n",
        "print(f\"   Custo mÃ­nimo: {custo_minimo:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a paisagem em 3D - A MONTANHA DO ERRO!\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Subplot 1: SuperfÃ­cie 3D\n",
        "ax1 = fig.add_subplot(131, projection='3d')\n",
        "surface = ax1.plot_surface(Theta0, Theta1, Custos, \n",
        "                          cmap='viridis', alpha=0.8,\n",
        "                          linewidth=0, antialiased=True)\n",
        "\n",
        "# Marcando o ponto Ã³timo\n",
        "ax1.scatter([theta0_otimo], [theta1_otimo], [custo_minimo], \n",
        "           color='red', s=100, label='MÃ­nimo Global')\n",
        "\n",
        "ax1.set_xlabel('Î¸â‚€')\n",
        "ax1.set_ylabel('Î¸â‚')\n",
        "ax1.set_zlabel('Custo')\n",
        "ax1.set_title('ğŸ”ï¸ A Montanha do Erro!', fontweight='bold')\n",
        "\n",
        "# Subplot 2: Curvas de nÃ­vel\n",
        "ax2 = fig.add_subplot(132)\n",
        "contour = ax2.contour(Theta0, Theta1, Custos, levels=20, cmap='viridis')\n",
        "ax2.clabel(contour, inline=True, fontsize=8)\n",
        "ax2.scatter(theta0_otimo, theta1_otimo, color='red', s=100, marker='x', \n",
        "           linewidth=3, label='MÃ­nimo Global')\n",
        "ax2.set_xlabel('Î¸â‚€')\n",
        "ax2.set_ylabel('Î¸â‚')\n",
        "ax2.set_title('ğŸ—ºï¸ Mapa TopogrÃ¡fico', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: Heatmap\n",
        "ax3 = fig.add_subplot(133)\n",
        "heatmap = ax3.imshow(Custos, extent=[theta0_range[0], theta0_range[-1], \n",
        "                                   theta1_range[0], theta1_range[-1]], \n",
        "                    origin='lower', cmap='viridis', aspect='auto')\n",
        "ax3.scatter(theta0_otimo, theta1_otimo, color='red', s=100, marker='x', \n",
        "           linewidth=3, label='MÃ­nimo Global')\n",
        "ax3.set_xlabel('Î¸â‚€')\n",
        "ax3.set_ylabel('Î¸â‚')\n",
        "ax3.set_title('ğŸ”¥ Mapa de Calor', fontweight='bold')\n",
        "ax3.legend()\n",
        "plt.colorbar(heatmap, ax=ax3, label='Custo')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¯ LIIINDO! Agora vocÃª pode VER a paisagem completa!\")\n",
        "print(\"ğŸ“ O ponto vermelho Ã© onde queremos chegar!\")\n",
        "print(\"ğŸ”ï¸ A montanha azul-escura representa custos altos (ruim)\")\n",
        "print(\"ğŸŒ¿ A regiÃ£o amarela representa custos baixos (bom)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¨ Tipos de FunÃ§Ãµes de Custo\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cÃ¡lculo-para-ia-modulo-02_img_05.png)\n\n**Bora conhecer os principais tipos de funÃ§Ãµes de custo!**\n\nCada problema tem sua funÃ§Ã£o de custo ideal. Ã‰ como escolher a ferramenta certa para cada trabalho!\n\n### 1. **MSE (Mean Squared Error)**\n$$MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2$$\n\n**Quando usar:** RegressÃ£o linear, quando queremos penalizar erros grandes\n\n### 2. **MAE (Mean Absolute Error)**\n$$MAE = \\frac{1}{m} \\sum_{i=1}^{m} |y_i - \\hat{y}_i|$$\n\n**Quando usar:** Quando temos outliers, mais robusta\n\n### 3. **Cross-Entropy**\n$$CE = -\\frac{1}{m} \\sum_{i=1}^{m} y_i \\log(\\hat{y}_i)$$\n\n**Quando usar:** ClassificaÃ§Ã£o, especialmente com probabilidades\n\n**ğŸ’¡ Dica do Pedro:** MSE penaliza erros grandes MUITO mais que erros pequenos (por causa do quadrado)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparando diferentes funÃ§Ãµes de custo\n",
        "\n",
        "def mse_custo(y_real, y_pred):\n",
        "    \"\"\"Mean Squared Error\"\"\"\n",
        "    return np.mean((y_real - y_pred)**2)\n",
        "\n",
        "def mae_custo(y_real, y_pred):\n",
        "    \"\"\"Mean Absolute Error\"\"\"\n",
        "    return np.mean(np.abs(y_real - y_pred))\n",
        "\n",
        "def huber_custo(y_real, y_pred, delta=1.0):\n",
        "    \"\"\"Huber Loss - Combina MSE e MAE\"\"\"\n",
        "    erro = y_real - y_pred\n",
        "    condicao = np.abs(erro) <= delta\n",
        "    quadratico = 0.5 * erro**2\n",
        "    linear = delta * np.abs(erro) - 0.5 * delta**2\n",
        "    return np.mean(np.where(condicao, quadratico, linear))\n",
        "\n",
        "# Criando dados com outliers para demonstrar as diferenÃ§as\n",
        "y_real_exemplo = np.array([1, 2, 3, 4, 5])\n",
        "y_pred_normal = np.array([1.1, 2.2, 2.9, 3.8, 5.1])  # PrediÃ§Ãµes normais\n",
        "y_pred_outlier = np.array([1.1, 2.2, 2.9, 3.8, 10])  # Uma prediÃ§Ã£o muito errada\n",
        "\n",
        "print(\"ğŸ“Š COMPARANDO FUNÃ‡Ã•ES DE CUSTO:\\n\")\n",
        "\n",
        "print(\"CenÃ¡rio 1: PrediÃ§Ãµes normais\")\n",
        "print(f\"Real:     {y_real_exemplo}\")\n",
        "print(f\"Predito:  {y_pred_normal}\")\n",
        "print(f\"MSE:  {mse_custo(y_real_exemplo, y_pred_normal):.3f}\")\n",
        "print(f\"MAE:  {mae_custo(y_real_exemplo, y_pred_normal):.3f}\")\n",
        "print(f\"Huber: {huber_custo(y_real_exemplo, y_pred_normal):.3f}\")\n",
        "\n",
        "print(\"\\nCenÃ¡rio 2: COM OUTLIER (Ãºltimo valor muito errado)\")\n",
        "print(f\"Real:     {y_real_exemplo}\")\n",
        "print(f\"Predito:  {y_pred_outlier}\")\n",
        "print(f\"MSE:  {mse_custo(y_real_exemplo, y_pred_outlier):.3f}  â† EXPLODIU!\")\n",
        "print(f\"MAE:  {mae_custo(y_real_exemplo, y_pred_outlier):.3f}  â† Mais robusta\")\n",
        "print(f\"Huber: {huber_custo(y_real_exemplo, y_pred_outlier):.3f}  â† Meio termo\")\n",
        "\n",
        "print(\"\\nğŸ¯ Viu a diferenÃ§a? MSE penaliza MUITO mais os outliers!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando como cada funÃ§Ã£o de custo se comporta\n",
        "\n",
        "# Criando erros de -5 a 5\n",
        "erros = np.linspace(-5, 5, 100)\n",
        "\n",
        "# Calculando as perdas para cada erro\n",
        "mse_perdas = erros**2\n",
        "mae_perdas = np.abs(erros)\n",
        "huber_perdas = np.where(np.abs(erros) <= 1, \n",
        "                       0.5 * erros**2,\n",
        "                       np.abs(erros) - 0.5)\n",
        "\n",
        "# Plotando\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.plot(erros, mse_perdas, 'r-', linewidth=3, label='MSE (QuadrÃ¡tica)', alpha=0.8)\n",
        "plt.plot(erros, mae_perdas, 'b-', linewidth=3, label='MAE (Linear)', alpha=0.8)\n",
        "plt.plot(erros, huber_perdas, 'g-', linewidth=3, label='Huber (HÃ­brida)', alpha=0.8)\n",
        "\n",
        "plt.xlabel('Erro (y_real - y_pred)', fontsize=12)\n",
        "plt.ylabel('Perda', fontsize=12)\n",
        "plt.title('ğŸ“Š ComparaÃ§Ã£o das FunÃ§Ãµes de Custo', fontsize=16, fontweight='bold')\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Destacando regiÃµes importantes\n",
        "plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
        "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Adicionando anotaÃ§Ãµes\n",
        "plt.annotate('MSE penaliza\\nerros grandes!', \n",
        "            xy=(3, 9), xytext=(1.5, 15),\n",
        "            arrowprops=dict(arrowstyle='->', color='red'),\n",
        "            fontsize=10, ha='center')\n",
        "\n",
        "plt.annotate('MAE Ã© linear\\n(mais robusta)', \n",
        "            xy=(4, 4), xytext=(2.5, 8),\n",
        "            arrowprops=dict(arrowstyle='->', color='blue'),\n",
        "            fontsize=10, ha='center')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¯ Agora fica claro porque cada funÃ§Ã£o Ã© Ãºtil em situaÃ§Ãµes diferentes!\")\n",
        "print(\"ğŸ“ˆ MSE: Boa para erros 'normais', ruim com outliers\")\n",
        "print(\"ğŸ“‰ MAE: Robusta a outliers, mas pode ser lenta para converger\")\n",
        "print(\"ğŸ¨ Huber: O melhor dos dois mundos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ” Anatomia de uma FunÃ§Ã£o: Propriedades Importantes\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cÃ¡lculo-para-ia-modulo-02_img_06.png)\n\n**Agora vamos destrinchar uma funÃ§Ã£o como um detetive!** ğŸ•µï¸â€â™‚ï¸\n\nQuando olhamos para uma funÃ§Ã£o, especialmente de custo, precisamos identificar suas **caracterÃ­sticas importantes**:\n\n### ğŸ¯ **Propriedades Essenciais:**\n\n1. **DomÃ­nio**: Onde a funÃ§Ã£o \"vive\"\n2. **MÃ­nimos/MÃ¡ximos**: Pontos crÃ­ticos\n3. **Convexidade**: A funÃ§Ã£o Ã© uma \"tigela\" ou \"montanha\"?\n4. **Continuidade**: Tem \"saltos\" ou Ã© lisa?\n5. **Derivabilidade**: Podemos calcular a inclinaÃ§Ã£o?\n\n**Por que isso importa na IA?**\n- **Convexidade** â†’ Garantia de encontrar o mÃ­nimo global\n- **Continuidade** â†’ Gradiente Descendente funciona\n- **Derivabilidade** â†’ Conseguimos calcular gradientes\n\n**ğŸ’¡ Dica do Pedro:** FunÃ§Ãµes convexas sÃ£o o \"santo graal\" da otimizaÃ§Ã£o - sempre tÃªm um Ãºnico mÃ­nimo global!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos analisar diferentes tipos de funÃ§Ãµes\n",
        "\n",
        "def funcao_convexa(x):\n",
        "    \"\"\"FunÃ§Ã£o convexa - formato de U\"\"\"\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "def funcao_concava(x):\n",
        "    \"\"\"FunÃ§Ã£o cÃ´ncava - formato de âˆ©\"\"\"\n",
        "    return -(x**2) + 4*x + 1\n",
        "\n",
        "def funcao_nao_convexa(x):\n",
        "    \"\"\"FunÃ§Ã£o nÃ£o-convexa - tem vÃ¡rios mÃ­nimos locais\"\"\"\n",
        "    return x**4 - 4*x**3 + 4*x**2 + 1\n",
        "\n",
        "def funcao_com_ruido(x):\n",
        "    \"\"\"FunÃ§Ã£o com muitas oscilaÃ§Ãµes\"\"\"\n",
        "    return x**2 + 2*np.sin(5*x)\n",
        "\n",
        "# Criando dados\n",
        "x = np.linspace(-3, 5, 1000)\n",
        "\n",
        "# Calculando as funÃ§Ãµes\n",
        "y_convexa = funcao_convexa(x)\n",
        "y_concava = funcao_concava(x)\n",
        "y_nao_convexa = funcao_nao_convexa(x)\n",
        "y_ruido = funcao_com_ruido(x)\n",
        "\n",
        "print(\"ğŸ” ANALISANDO DIFERENTES TIPOS DE FUNÃ‡Ã•ES:\")\n",
        "print(\"\\n1. ğŸ“ˆ Convexa: Uma Ãºnica 'tigela' - IDEAL para IA\")\n",
        "print(\"2. ğŸ“‰ CÃ´ncava: Uma Ãºnica 'montanha' - Para maximizaÃ§Ã£o\")\n",
        "print(\"3. ğŸ¢ NÃ£o-convexa: VÃ¡rias 'tigelas' - CUIDADO com mÃ­nimos locais!\")\n",
        "print(\"4. ğŸŒŠ Com ruÃ­do: OscilaÃ§Ãµes - DifÃ­cil de otimizar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando as diferentes propriedades\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# FunÃ§Ã£o Convexa\n",
        "axes[0,0].plot(x, y_convexa, 'b-', linewidth=3)\n",
        "axes[0,0].set_title('ğŸ“ˆ FunÃ§Ã£o CONVEXA\\n(Ideal para otimizaÃ§Ã£o)', fontweight='bold', color='blue')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "axes[0,0].set_ylabel('f(x)')\n",
        "\n",
        "# Marcando o mÃ­nimo\n",
        "min_idx = np.argmin(y_convexa)\n",
        "axes[0,0].plot(x[min_idx], y_convexa[min_idx], 'ro', markersize=10, label='MÃ­nimo Global')\n",
        "axes[0,0].legend()\n",
        "\n",
        "# FunÃ§Ã£o CÃ´ncava\n",
        "axes[0,1].plot(x, y_concava, 'g-', linewidth=3)\n",
        "axes[0,1].set_title('ğŸ“‰ FunÃ§Ã£o CÃ”NCAVA\\n(Para problemas de maximizaÃ§Ã£o)', fontweight='bold', color='green')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Marcando o mÃ¡ximo\n",
        "max_idx = np.argmax(y_concava)\n",
        "axes[0,1].plot(x[max_idx], y_concava[max_idx], 'ro', markersize=10, label='MÃ¡ximo Global')\n",
        "axes[0,1].legend()\n",
        "\n",
        "# FunÃ§Ã£o NÃ£o-Convexa\n",
        "axes[1,0].plot(x, y_nao_convexa, 'r-', linewidth=3)\n",
        "axes[1,0].set_title('ğŸ¢ FunÃ§Ã£o NÃƒO-CONVEXA\\n(CUIDADO: MÃºltiplos mÃ­nimos!)', fontweight='bold', color='red')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "axes[1,0].set_xlabel('x')\n",
        "axes[1,0].set_ylabel('f(x)')\n",
        "\n",
        "# Marcando mÃ­nimos locais\n",
        "from scipy.signal import find_peaks\n",
        "picos_neg, _ = find_peaks(-y_nao_convexa, height=-10)\n",
        "if len(picos_neg) > 0:\n",
        "    axes[1,0].plot(x[picos_neg], y_nao_convexa[picos_neg], 'ro', markersize=8, label='MÃ­nimos Locais')\n",
        "    axes[1,0].legend()\n",
        "\n",
        "# FunÃ§Ã£o com RuÃ­do\n",
        "axes[1,1].plot(x, y_ruido, 'purple', linewidth=2)\n",
        "axes[1,1].set_title('ğŸŒŠ FunÃ§Ã£o com OSCILAÃ‡Ã•ES\\n(DifÃ­cil otimizaÃ§Ã£o)', fontweight='bold', color='purple')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "axes[1,1].set_xlabel('x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ¯ LIÃ‡Ã•ES IMPORTANTES:\")\n",
        "print(\"âœ… FunÃ§Ãµes CONVEXAS: Sempre encontramos o mÃ­nimo global!\")\n",
        "print(\"âš ï¸  FunÃ§Ãµes NÃƒO-CONVEXAS: Podemos ficar presos em mÃ­nimos locais!\")\n",
        "print(\"ğŸŒŠ FunÃ§Ãµes com RUÃDO: Precisamos de tÃ©cnicas especiais!\")\n",
        "print(\"\\nğŸ’¡ Por isso escolher a arquitetura certa Ã© TÃƒO importante!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ AplicaÃ§Ã£o PrÃ¡tica: FunÃ§Ã£o de Custo Real\n\n**Bora fazer um exemplo REAL de Machine Learning!**\n\nVamos simular um problema de **prediÃ§Ã£o de preÃ§os de casas** e ver como nossa funÃ§Ã£o de custo se comporta.\n\n**CenÃ¡rio:**\n- ğŸ  **Problema**: Prever preÃ§o de casas baseado no tamanho\n- ğŸ“Š **Dados**: Tamanho da casa (mÂ²) vs PreÃ§o (R$)\n- ğŸ¯ **Objetivo**: Encontrar a melhor linha que ajusta os dados\n- ğŸ“ˆ **Modelo**: $\\text{PreÃ§o} = \\theta_0 + \\theta_1 \\times \\text{Tamanho}$\n\n**Este Ã© um exemplo tÃ­pico que vocÃª vai encontrar na vida real!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando dataset realista de preÃ§os de casas\n",
        "np.random.seed(42)\n",
        "\n",
        "# Dados de casas (tamanho em mÂ², preÃ§o base em R$)\n",
        "n_casas = 100\n",
        "tamanho_casa = np.random.normal(120, 40, n_casas)  # Tamanho mÃ©dio 120mÂ²\n",
        "tamanho_casa = np.clip(tamanho_casa, 50, 300)      # Entre 50 e 300mÂ²\n",
        "\n",
        "# PreÃ§o real: R$ 2000/mÂ² + ruÃ­do + fatores extras\n",
        "preco_base = 2000  # R$ por mÂ²\n",
        "preco_casa = (preco_base * tamanho_casa + \n",
        "              np.random.normal(0, 50000, n_casas) +  # RuÃ­do\n",
        "              tamanho_casa * np.random.normal(500, 200, n_casas))  # Fatores extras\n",
        "\n",
        "# Garantindo preÃ§os positivos e realistas\n",
        "preco_casa = np.clip(preco_casa, 100000, 1000000)\n",
        "\n",
        "print(\"ğŸ  DATASET DE CASAS CRIADO!\")\n",
        "print(f\"   ğŸ“Š {n_casas} casas no dataset\")\n",
        "print(f\"   ğŸ“ Tamanho mÃ©dio: {np.mean(tamanho_casa):.1f}mÂ²\")\n",
        "print(f\"   ğŸ’° PreÃ§o mÃ©dio: R$ {np.mean(preco_casa):,.0f}\")\n",
        "print(f\"   ğŸ’¸ PreÃ§o por mÂ²: R$ {np.mean(preco_casa/tamanho_casa):,.0f}\")\n",
        "\n",
        "# Visualizando nossos dados\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(tamanho_casa, preco_casa, alpha=0.6, s=50, c='blue', edgecolors='black')\n",
        "plt.xlabel('Tamanho da Casa (mÂ²)', fontsize=12)\n",
        "plt.ylabel('PreÃ§o da Casa (R$)', fontsize=12)\n",
        "plt.title('ğŸ  Dataset: Tamanho vs PreÃ§o das Casas', fontsize=16, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Formatando eixo Y para mostrar valores em reais\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'R$ {x/1000:.0f}K'))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ¯ Agora vamos treinar um modelo para prever o preÃ§o baseado no tamanho!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementando nosso modelo de regressÃ£o linear\n",
        "\n",
        "class ModeloLinearSimples:\n",
        "    def __init__(self):\n",
        "        self.theta0 = 0  # Intercepto\n",
        "        self.theta1 = 0  # Coeficiente angular\n",
        "        self.historico_custo = []\n",
        "    \n",
        "    def prever(self, x):\n",
        "        \"\"\"Faz prediÃ§Ãµes usando o modelo atual\"\"\"\n",
        "        return self.theta0 + self.theta1 * x\n",
        "    \n",
        "    def calcular_custo(self, x, y):\n",
        "        \"\"\"Calcula o custo MSE\"\"\"\n",
        "        m = len(x)\n",
        "        predicoes = self.prever(x)\n",
        "        custo = (1/(2*m)) * np.sum((predicoes - y)**2)\n",
        "        return custo\n",
        "    \n",
        "    def treinar_analitico(self, x, y):\n",
        "        \"\"\"Resolve analiticamente (fÃ³rmula fechada)\"\"\"\n",
        "        # FÃ³rmulas da regressÃ£o linear\n",
        "        x_mean = np.mean(x)\n",
        "        y_mean = np.mean(y)\n",
        "        \n",
        "        numerador = np.sum((x - x_mean) * (y - y_mean))\n",
        "        denominador = np.sum((x - x_mean)**2)\n",
        "        \n",
        "        self.theta1 = numerador / denominador\n",
        "        self.theta0 = y_mean - self.theta1 * x_mean\n",
        "        \n",
        "        return self.theta0, self.theta1\n",
        "\n",
        "# Criando e treinando o modelo\n",
        "modelo = ModeloLinearSimples()\n",
        "\n",
        "print(\"ğŸ¤– TREINANDO O MODELO...\")\n",
        "print(f\"   ğŸ“Š Dados de entrada: {len(tamanho_casa)} casas\")\n",
        "\n",
        "# Calculando custo inicial (parÃ¢metros zerados)\n",
        "custo_inicial = modelo.calcular_custo(tamanho_casa, preco_casa)\n",
        "print(f\"   ğŸ’¸ Custo inicial (Î¸â‚€=0, Î¸â‚=0): {custo_inicial:,.0f}\")\n",
        "\n",
        "# Treinando (soluÃ§Ã£o Ã³tima analÃ­tica)\n",
        "theta0_otimo, theta1_otimo = modelo.treinar_analitico(tamanho_casa, preco_casa)\n",
        "custo_final = modelo.calcular_custo(tamanho_casa, preco_casa)\n",
        "\n",
        "print(f\"\\nâœ… MODELO TREINADO!\")\n",
        "print(f\"   ğŸ¯ Î¸â‚€ (intercepto): R$ {theta0_otimo:,.0f}\")\n",
        "print(f\"   ğŸ“ˆ Î¸â‚ (preÃ§o/mÂ²): R$ {theta1_otimo:,.0f} por mÂ²\")\n",
        "print(f\"   ğŸ’° Custo final: {custo_final:,.0f}\")\n",
        "print(f\"   ğŸ“‰ ReduÃ§Ã£o do custo: {((custo_inicial-custo_final)/custo_inicial)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nğŸ  INTERPRETAÃ‡ÃƒO DO MODELO:\")\n",
        "print(f\"   ğŸ’¡ PreÃ§o base (casa de 0mÂ²): R$ {theta0_otimo:,.0f}\")\n",
        "print(f\"   ğŸ’¡ Cada mÂ² adicional custa: R$ {theta1_otimo:,.0f}\")\n",
        "print(f\"   ğŸ’¡ FÃ³rmula: PreÃ§o = {theta0_otimo:,.0f} + {theta1_otimo:,.0f} Ã— Tamanho\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando o resultado final\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Subplot 1: Dados + Modelo ajustado\n",
        "ax1.scatter(tamanho_casa, preco_casa, alpha=0.6, s=50, c='lightblue', \n",
        "           edgecolors='black', label='Dados Reais')\n",
        "\n",
        "# Linha do modelo\n",
        "x_linha = np.linspace(tamanho_casa.min(), tamanho_casa.max(), 100)\n",
        "y_linha = modelo.prever(x_linha)\n",
        "ax1.plot(x_linha, y_linha, 'r-', linewidth=3, \n",
        "        label=f'Modelo: y = {theta0_otimo:,.0f} + {theta1_otimo:,.0f}x')\n",
        "\n",
        "ax1.set_xlabel('Tamanho da Casa (mÂ²)', fontsize=12)\n",
        "ax1.set_ylabel('PreÃ§o da Casa (R$)', fontsize=12)\n",
        "ax1.set_title('ğŸ  Modelo Treinado vs Dados Reais', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'R$ {x/1000:.0f}K'))\n",
        "\n",
        "# Subplot 2: ResÃ­duos (erros)\n",
        "predicoes = modelo.prever(tamanho_casa)\n",
        "residuos = preco_casa - predicoes\n",
        "\n",
        "ax2.scatter(predicoes, residuos, alpha=0.6, s=50, c='orange', edgecolors='black')\n",
        "ax2.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Erro = 0')\n",
        "ax2.set_xlabel('PrediÃ§Ãµes (R$)', fontsize=12)\n",
        "ax2.set_ylabel('ResÃ­duos (R$)', fontsize=12)\n",
        "ax2.set_title('ğŸ“Š AnÃ¡lise dos ResÃ­duos', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'R$ {x/1000:.0f}K'))\n",
        "ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'R$ {x/1000:.0f}K'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculando mÃ©tricas de performance\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "r2 = r2_score(preco_casa, predicoes)\n",
        "mae = mean_absolute_error(preco_casa, predicoes)\n",
        "rmse = np.sqrt(np.mean(residuos**2))\n",
        "\n",
        "print(\"ğŸ“Š MÃ‰TRICAS DE PERFORMANCE:\")\n",
        "print(f\"   ğŸ¯ RÂ² Score: {r2:.3f} ({r2*100:.1f}% da variÃ¢ncia explicada)\")\n",
        "print(f\"   ğŸ“ MAE: R$ {mae:,.0f} (erro mÃ©dio absoluto)\")\n",
        "print(f\"   ğŸ“ RMSE: R$ {rmse:,.0f} (erro quadrÃ¡tico mÃ©dio)\")\n",
        "\n",
        "print(f\"\\nğŸ  TESTE PRÃTICO:\")\n",
        "tamanhos_teste = [80, 120, 200]\n",
        "for tamanho in tamanhos_teste:\n",
        "    preco_previsto = modelo.prever(tamanho)\n",
        "    print(f\"   Casa de {tamanho}mÂ²: R$ {preco_previsto:,.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ ExercÃ­cio PrÃ¡tico 1: Explorando FunÃ§Ãµes\n\n\n\n**Agora Ã© sua vez de colocar a mÃ£o na massa!** ğŸš€\n\n**Desafio:** Crie e analise diferentes funÃ§Ãµes de custo para entender seu comportamento.\n\n**Tarefas:**\n1. Implemente uma funÃ§Ã£o cÃºbica: $f(x) = ax^3 + bx^2 + cx + d$\n2. Teste diferentes valores para os parÃ¢metros $a$, $b$, $c$, $d$\n3. Visualize como cada parÃ¢metro afeta o formato da funÃ§Ã£o\n4. Identifique onde estÃ£o os mÃ­nimos e mÃ¡ximos\n\n**ğŸ’¡ Dica do Pedro:** Use `np.gradient()` para calcular aproximadamente a derivada e encontrar pontos crÃ­ticos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÃCIO 1: Complete o cÃ³digo abaixo\n",
        "\n",
        "def funcao_cubica(x, a, b, c, d):\n",
        "    \"\"\"FunÃ§Ã£o cÃºbica: f(x) = axÂ³ + bxÂ² + cx + d\"\"\"\n",
        "    # TODO: Implemente a funÃ§Ã£o cÃºbica\n",
        "    return # Sua implementaÃ§Ã£o aqui\n",
        "\n",
        "def encontrar_pontos_criticos(x, y):\n",
        "    \"\"\"Encontra pontos onde a derivada â‰ˆ 0\"\"\"\n",
        "    # TODO: Use np.gradient para calcular a derivada\n",
        "    derivada = # Sua implementaÃ§Ã£o aqui\n",
        "    \n",
        "    # TODO: Encontre onde a derivada muda de sinal\n",
        "    # Dica: use np.diff(np.sign(derivada)) para detectar mudanÃ§as de sinal\n",
        "    \n",
        "    return pontos_criticos\n",
        "\n",
        "# Teste seus parÃ¢metros aqui\n",
        "x = np.linspace(-3, 3, 1000)\n",
        "\n",
        "# TODO: Teste diferentes valores de parÃ¢metros\n",
        "a, b, c, d = 1, -3, 2, 1  # Modifique estes valores!\n",
        "\n",
        "y = funcao_cubica(x, a, b, c, d)\n",
        "pontos_criticos = encontrar_pontos_criticos(x, y)\n",
        "\n",
        "# VisualizaÃ§Ã£o\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(x, y, 'b-', linewidth=3, label=f'f(x) = {a}xÂ³ + {b}xÂ² + {c}x + {d}')\n",
        "\n",
        "# TODO: Marque os pontos crÃ­ticos no grÃ¡fico\n",
        "# plt.scatter(...)\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.title('ğŸ¯ Sua FunÃ§Ã£o CÃºbica', fontsize=16, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¯ ANÃLISE DA SUA FUNÃ‡ÃƒO:\")\n",
        "print(f\"   ğŸ“Š ParÃ¢metros: a={a}, b={b}, c={c}, d={d}\")\n",
        "print(f\"   ğŸ¯ Pontos crÃ­ticos encontrados: {len(pontos_criticos)}\")\n",
        "print(\"\\nğŸ’¡ Experimente diferentes valores e veja como a funÃ§Ã£o muda!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ ExercÃ­cio PrÃ¡tico 2: Criando sua PrÃ³pria FunÃ§Ã£o de Custo\n\n**Desafio AvanÃ§ado:** Implemente e compare diferentes funÃ§Ãµes de custo!\n\n**CenÃ¡rio:** VocÃª tem um dataset com alguns outliers e precisa escolher a melhor funÃ§Ã£o de custo.\n\n**Tarefas:**\n1. Implemente MSE, MAE e Huber Loss\n2. Teste com dados que tÃªm outliers\n3. Compare visualmente qual funÃ§Ã£o Ã© mais robusta\n4. Analise qual seria melhor para seu problema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÃCIO 2: Implemente as funÃ§Ãµes de custo\n",
        "\n",
        "# Dados com outliers para teste\n",
        "y_true = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
        "y_pred_normal = np.array([1.1, 2.1, 2.9, 4.1, 4.9, 6.1, 7.1, 7.9])  # PrediÃ§Ãµes normais\n",
        "y_pred_outlier = np.array([1.1, 2.1, 2.9, 15, 4.9, 6.1, 7.1, 7.9])  # Com outlier\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    \"\"\"TODO: Implemente Mean Squared Error\"\"\"\n",
        "    return # Sua implementaÃ§Ã£o\n",
        "\n",
        "def mae_loss(y_true, y_pred):\n",
        "    \"\"\"TODO: Implemente Mean Absolute Error\"\"\"\n",
        "    return # Sua implementaÃ§Ã£o\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta=1.0):\n",
        "    \"\"\"TODO: Implemente Huber Loss\"\"\"\n",
        "    # Dica: Use np.where para implementar a condiÃ§Ã£o\n",
        "    return # Sua implementaÃ§Ã£o\n",
        "\n",
        "# TODO: Teste suas implementaÃ§Ãµes\n",
        "print(\"ğŸ“Š COMPARANDO FUNÃ‡Ã•ES DE CUSTO:\\n\")\n",
        "\n",
        "print(\"CenÃ¡rio 1: PrediÃ§Ãµes normais\")\n",
        "print(f\"MSE:   {mse_loss(y_true, y_pred_normal):.3f}\")\n",
        "print(f\"MAE:   {mae_loss(y_true, y_pred_normal):.3f}\")\n",
        "print(f\"Huber: {huber_loss(y_true, y_pred_normal):.3f}\")\n",
        "\n",
        "print(\"\\nCenÃ¡rio 2: Com outlier\")\n",
        "print(f\"MSE:   {mse_loss(y_true, y_pred_outlier):.3f}\")\n",
        "print(f\"MAE:   {mae_loss(y_true, y_pred_outlier):.3f}\")\n",
        "print(f\"Huber: {huber_loss(y_true, y_pred_outlier):.3f}\")\n",
        "\n",
        "# TODO: Crie uma visualizaÃ§Ã£o comparando as trÃªs funÃ§Ãµes\n",
        "# Dica: Use subplots para mostrar cada cenÃ¡rio\n",
        "\n",
        "print(\"\\nğŸ¤” PERGUNTA PARA REFLEXÃƒO:\")\n",
        "print(\"Qual funÃ§Ã£o de custo vocÃª usaria se seus dados tivessem muitos outliers? Por quÃª?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”„ Conectando com os PrÃ³ximos MÃ³dulos\n\n```mermaid\ngraph TD\n    A[\"ğŸ“Š MÃ³dulo 2: FunÃ§Ãµes e GrÃ¡ficos\"] --> B[\"ğŸ¯ MÃ³dulo 3: Limites\"]\n    B --> C[\"ğŸ“ˆ MÃ³dulo 4: Derivadas\"]\n    C --> D[\"â›“ï¸ MÃ³dulo 6: Regra da Cadeia\"]\n    D --> E[\"ğŸ”ï¸ MÃ³dulo 11: Gradiente Descendente\"]\n    \n    A --> F[\"Visualizar a montanha\"]\n    B --> G[\"Continuidade da funÃ§Ã£o\"]\n    C --> H[\"InclinaÃ§Ã£o em cada ponto\"]\n    D --> I[\"Backpropagation\"]\n    E --> J[\"OtimizaÃ§Ã£o final\"]\n```\n\n**ğŸ¯ Recapitulando nossa jornada:**\n\n**âœ… No MÃ³dulo 1:** Entendemos que IA Ã© otimizaÃ§Ã£o e visualizamos a \"montanha do erro\"\n\n**âœ… No MÃ³dulo 2 (atual):** Aprendemos a:\n- Criar e visualizar funÃ§Ãµes matemÃ¡ticas\n- Entender funÃ§Ãµes de custo (MSE, MAE, Huber)\n- Mapear a paisagem completa do erro\n- Identificar propriedades importantes (convexidade, mÃ­nimos)\n\n**ğŸš€ PrÃ³ximos mÃ³dulos:**\n- **MÃ³dulo 3**: Como definir quando uma funÃ§Ã£o Ã© \"suave\" (limites)\n- **MÃ³dulo 4**: Como medir a \"inclinaÃ§Ã£o\" da montanha (derivadas)\n- **MÃ³dulo 6**: Como calcular gradientes em redes neurais (regra da cadeia)\n- **MÃ³dulo 11**: Como \"descer\" a montanha eficientemente (gradiente descendente)\n\n**ğŸ’¡ Dica do Pedro:** Tudo que vimos aqui vai ser fundamental para entender como os algoritmos de IA realmente funcionam!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‰ Resumo: O que Aprendemos\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/cÃ¡lculo-para-ia-modulo-02_img_08.png)\n\n**ParabÃ©ns! VocÃª concluiu o MÃ³dulo 2! ğŸŠ**\n\n### ğŸ§  **Conceitos Fundamentais Dominados:**\n\n1. **ğŸ”§ FunÃ§Ãµes MatemÃ¡ticas**\n   - O que sÃ£o e como funcionam\n   - Entrada â†’ Processamento â†’ SaÃ­da\n   - Tipos: linear, quadrÃ¡tica, exponencial\n\n2. **ğŸ‘ï¸ ImportÃ¢ncia da VisualizaÃ§Ã£o**\n   - CÃ©rebro processa visual 50.000x mais rÃ¡pido\n   - Identifica padrÃµes instantaneamente\n   - Detecta problemas facilmente\n\n3. **ğŸ’° FunÃ§Ãµes de Custo**\n   - MSE: Para regressÃ£o, penaliza erros grandes\n   - MAE: Robusta a outliers\n   - Huber: Melhor dos dois mundos\n\n4. **ğŸ—ºï¸ Mapeamento da Paisagem**\n   - VisualizaÃ§Ã£o 3D da funÃ§Ã£o de custo\n   - IdentificaÃ§Ã£o de mÃ­nimos globais\n   - Curvas de nÃ­vel e mapas de calor\n\n5. **ğŸ” Propriedades das FunÃ§Ãµes**\n   - Convexidade (santo graal!)\n   - Continuidade e derivabilidade\n   - MÃ­nimos locais vs globais\n\n### ğŸ¯ **Skills PrÃ¡ticas Desenvolvidas:**\n- âœ… Implementar funÃ§Ãµes de custo do zero\n- âœ… Criar visualizaÃ§Ãµes informativas\n- âœ… Analisar propriedades matemÃ¡ticas\n- âœ… Comparar diferentes abordagens\n- âœ… Resolver problemas reais (previsÃ£o de preÃ§os)\n\n### ğŸš€ **PreparaÃ§Ã£o para os PrÃ³ximos MÃ³dulos:**\n- ğŸ¯ **Limites**: Definir continuidade e suavidade\n- ğŸ“ˆ **Derivadas**: Medir inclinaÃ§Ã£o e taxa de variaÃ§Ã£o\n- â›“ï¸ **Regra da Cadeia**: Base do backpropagation\n- ğŸ”ï¸ **Gradiente Descendente**: OtimizaÃ§Ã£o prÃ¡tica\n\n**ğŸ’¡ Frase do Pedro:** *\"Agora vocÃª nÃ£o apenas sabe que existe uma montanha do erro - vocÃª sabe como mapeÃ¡-la, visualizÃ¡-la e entender sua geografia! Isso Ã© fundamental para tudo que vem pela frente!\"*\n\n**ğŸŠ Continue firme na jornada! O prÃ³ximo mÃ³dulo sobre Limites vai ser LINDO!**"
      ]
    }
  ]
}