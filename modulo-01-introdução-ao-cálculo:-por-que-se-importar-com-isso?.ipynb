{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèîÔ∏è C√°lculo para IA: Escalando a Montanha do Conhecimento\n\n## M√≥dulo 1: Introdu√ß√£o ao C√°lculo - Por que se importar com isso?\n\n**Por Pedro Nunes Guth**\n\nEa√≠, galera! Bem-vindos ao primeiro m√≥dulo do nosso curso \"C√°lculo para IA\"! üöÄ\n\nT√°, mas antes de come√ßar, uma pergunta honesta: quantas vezes voc√™ j√° ouviu falar que \"n√£o precisa saber matem√°tica pra trabalhar com IA\"? \n\nBom... tecnicamente √© verdade. Voc√™ pode usar bibliotecas prontas, frameworks e APIs sem entender uma v√≠rgula do que t√° rolando por baixo dos panos. Mas a√≠ eu te pergunto: voc√™ quer ser s√≥ um usu√°rio ou quer ser um **mestre da IA**?\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-01_img_01.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Bora preparar nosso ambiente!\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "from IPython.display import HTML, display\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurando o matplotlib para ficar bonitinho\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"üéØ Ambiente configurado! Bora come√ßar nossa jornada!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î T√°, mas o que diabos √© C√°lculo mesmo?\n\nOlha, vou ser direto com voc√™s: **C√°lculo √© a matem√°tica do movimento e da mudan√ßa**.\n\nSabe quando voc√™ t√° dirigindo e quer saber:\n- Qual sua velocidade neste exato momento?\n- Quanto combust√≠vel voc√™ vai gastar na viagem toda?\n- Qual a melhor rota para chegar mais r√°pido?\n\nO C√°lculo responde essas perguntas! E adivinha? IA faz exatamente a mesma coisa:\n\n- **Velocidade** ‚Üí Qu√£o r√°pido o modelo est√° aprendendo?\n- **Combust√≠vel** ‚Üí Quanto \"erro\" total temos?\n- **Melhor rota** ‚Üí Qual dire√ß√£o seguir para melhorar?\n\n### Os Dois Pilares do C√°lculo:\n\n1. **C√°lculo Diferencial** (Derivadas): *\"Qu√£o r√°pido as coisas mudam?\"*\n2. **C√°lculo Integral**: *\"Quanto acumula no total?\"*\n\n**Dica do Pedro**: Pense assim - derivada √© como um veloc√≠metro (mudan√ßa instant√¢nea), integral √© como o hod√¥metro (acumulado total)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma analogia visual simples\n",
        "# Imagine que voc√™ est√° dirigindo e medindo sua posi√ß√£o ao longo do tempo\n",
        "\n",
        "# Dados da viagem\n",
        "tempo = np.linspace(0, 10, 100)  # 10 horas de viagem\n",
        "posicao = 2*tempo**2 + 3*tempo + 1  # Fun√ß√£o da posi√ß√£o (quadr√°tica)\n",
        "\n",
        "# Calculando a velocidade (derivada da posi√ß√£o)\n",
        "velocidade = 4*tempo + 3  # Derivada de 2t¬≤ + 3t + 1 = 4t + 3\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# Gr√°fico da posi√ß√£o\n",
        "ax1.plot(tempo, posicao, 'b-', linewidth=3, label='Posi√ß√£o (km)')\n",
        "ax1.set_xlabel('Tempo (horas)')\n",
        "ax1.set_ylabel('Posi√ß√£o (km)')\n",
        "ax1.set_title('üöó Posi√ß√£o do Carro ao Longo do Tempo')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "# Gr√°fico da velocidade (derivada da posi√ß√£o)\n",
        "ax2.plot(tempo, velocidade, 'r-', linewidth=3, label='Velocidade (km/h)')\n",
        "ax2.set_xlabel('Tempo (horas)')\n",
        "ax2.set_ylabel('Velocidade (km/h)')\n",
        "ax2.set_title('‚ö° Velocidade do Carro (Derivada da Posi√ß√£o)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Olha que lindo! A velocidade √© a 'taxa de mudan√ßa' da posi√ß√£o!\")\n",
        "print(\"üî• Esse √© o conceito de DERIVADA - a base de tudo na IA!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Por que IA precisa de C√°lculo? A Conex√£o Vital\n\nBora ser bem direto: **Todo algoritmo de Machine Learning √©, no fundo, um problema de otimiza√ß√£o**.\n\n### O que significa isso?\n\nSignifica que queremos:\n1. **Minimizar** algo ruim (erro, perda, custo)\n2. **Maximizar** algo bom (acur√°cia, lucro, efici√™ncia)\n\nE como fazemos isso matematicamente? Com **C√°lculo**!\n\n### A Matem√°tica por tr√°s:\n\nSuponha que temos uma fun√ß√£o de erro $E(w)$, onde $w$ s√£o os pesos do modelo.\n\nPara minimizar o erro, precisamos encontrar onde:\n\n$$\\frac{dE}{dw} = 0$$\n\nIsso significa: *\"onde a taxa de mudan√ßa do erro √© zero\"* - ou seja, o ponto m√≠nimo!\n\n**Dica do Pedro**: √â como achar o fundo do vale numa montanha. No ponto mais baixo, se voc√™ der um passinho para qualquer lado, s√≥ sobe!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos visualizar uma fun√ß√£o de erro simples\n",
        "# Imagine que temos um modelo com apenas 1 par√¢metro (w)\n",
        "\n",
        "# Definindo nossa fun√ß√£o de erro\n",
        "w = np.linspace(-3, 5, 1000)\n",
        "erro = (w - 2)**2 + 0.5  # Fun√ß√£o quadr√°tica com m√≠nimo em w=2\n",
        "\n",
        "# Calculando a derivada (taxa de mudan√ßa)\n",
        "derivada_erro = 2 * (w - 2)  # Derivada de (w-2)¬≤ + 0.5\n",
        "\n",
        "# Encontrando o ponto onde a derivada √© zero\n",
        "w_otimo = 2  # Onde derivada = 0\n",
        "erro_minimo = 0.5\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# Gr√°fico da fun√ß√£o de erro\n",
        "ax1.plot(w, erro, 'b-', linewidth=3, label='Fun√ß√£o de Erro E(w)')\n",
        "ax1.plot(w_otimo, erro_minimo, 'ro', markersize=12, label='M√≠nimo Global')\n",
        "ax1.axvline(x=w_otimo, color='red', linestyle='--', alpha=0.7)\n",
        "ax1.set_xlabel('Par√¢metro w')\n",
        "ax1.set_ylabel('Erro E(w)')\n",
        "ax1.set_title('üéØ Fun√ß√£o de Erro - Queremos Minimizar Isso!')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "ax1.text(w_otimo+0.3, erro_minimo+0.1, 'Objetivo!\\n(Menor Erro)', \n",
        "         fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='yellow', alpha=0.7))\n",
        "\n",
        "# Gr√°fico da derivada\n",
        "ax2.plot(w, derivada_erro, 'g-', linewidth=3, label='Derivada dE/dw')\n",
        "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "ax2.axvline(x=w_otimo, color='red', linestyle='--', alpha=0.7)\n",
        "ax2.plot(w_otimo, 0, 'ro', markersize=12, label='Derivada = 0')\n",
        "ax2.set_xlabel('Par√¢metro w')\n",
        "ax2.set_ylabel('dE/dw')\n",
        "ax2.set_title('üìà Derivada da Fun√ß√£o de Erro - Nossa B√∫ssola!')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "ax2.text(w_otimo+0.3, 0.2, 'Aqui a derivada\\n√© zero!', \n",
        "         fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='lightgreen', alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üöÄ Sacou? A derivada nos mostra ONDE est√° o m√≠nimo!\")\n",
        "print(\"üí° √â exatamente isso que o Gradiente Descendente faz!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèîÔ∏è A Analogia da Montanha do Erro - O Cora√ß√£o da IA\n\nAgora vem a analogia mais importante do curso inteiro! Prepara que vai ser **LIIINDO**!\n\n### üé≠ A Hist√≥ria:\n\nImagina que voc√™ est√° perdido numa montanha **gigante**, completamente no escuro, s√≥ com uma lanterna pequena que ilumina alguns metros ao redor.\n\nSeu objetivo? **Chegar no ponto mais baixo poss√≠vel** (o vale).\n\n### üó∫Ô∏è Traduzindo para IA:\n\n- **A montanha**: √â nossa fun√ß√£o de erro/custo\n- **Sua posi√ß√£o**: S√£o os par√¢metros atuais do modelo\n- **A altura**: √â o valor do erro naquele ponto\n- **O vale**: √â o conjunto √≥timo de par√¢metros (menor erro)\n- **A lanterna**: √â o gradiente (derivada)\n- **A descida**: √â o algoritmo de otimiza√ß√£o\n\n### üß≠ Como usar a \"lanterna\" (gradiente)?\n\nA lanterna te mostra:\n1. **Para onde √© mais √≠ngreme** (dire√ß√£o do gradiente)\n2. **Qu√£o √≠ngreme √©** (magnitude do gradiente)\n\nE a estrat√©gia? **Sempre descer na dire√ß√£o mais √≠ngreme!**\n\n**Dica do Pedro**: O gradiente sempre aponta para CIMA (m√°ximo). Por isso descemos na dire√ß√£o OPOSTA (-gradiente) para achar o m√≠nimo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar nossa \"Montanha do Erro\" em 3D!\n",
        "# Imagine um modelo com 2 par√¢metros: w1 e w2\n",
        "\n",
        "# Criando a grade de par√¢metros\n",
        "w1 = np.linspace(-3, 3, 50)\n",
        "w2 = np.linspace(-3, 3, 50)\n",
        "W1, W2 = np.meshgrid(w1, w2)\n",
        "\n",
        "# Fun√ß√£o de erro (uma paraboloide - formato de tigela)\n",
        "# M√≠nimo em (1, 0.5)\n",
        "Z = (W1 - 1)**2 + 2*(W2 - 0.5)**2 + 0.1\n",
        "\n",
        "# Criando a visualiza√ß√£o 3D\n",
        "fig = plt.figure(figsize=(15, 12))\n",
        "\n",
        "# Subplot 1: Vista 3D da montanha\n",
        "ax1 = fig.add_subplot(221, projection='3d')\n",
        "surf = ax1.plot_surface(W1, W2, Z, cmap='terrain', alpha=0.8)\n",
        "ax1.set_xlabel('Par√¢metro w1')\n",
        "ax1.set_ylabel('Par√¢metro w2')\n",
        "ax1.set_zlabel('Erro E(w1,w2)')\n",
        "ax1.set_title('üèîÔ∏è A Montanha do Erro (3D)')\n",
        "\n",
        "# Marcando o ponto √≥timo\n",
        "ax1.scatter([1], [0.5], [0.1], color='red', s=100, label='Vale √ìtimo')\n",
        "\n",
        "# Subplot 2: Vista de cima (curvas de n√≠vel)\n",
        "ax2 = fig.add_subplot(222)\n",
        "contour = ax2.contour(W1, W2, Z, levels=20, cmap='terrain')\n",
        "ax2.clabel(contour, inline=True, fontsize=8)\n",
        "ax2.plot(1, 0.5, 'ro', markersize=12, label='Vale √ìtimo')\n",
        "ax2.set_xlabel('Par√¢metro w1')\n",
        "ax2.set_ylabel('Par√¢metro w2')\n",
        "ax2.set_title('üó∫Ô∏è Mapa Topogr√°fico (Vista de Cima)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: Simulando uma descida\n",
        "ax3 = fig.add_subplot(223)\n",
        "ax3.contour(W1, W2, Z, levels=20, cmap='terrain', alpha=0.6)\n",
        "\n",
        "# Simulando passos de descida do gradiente\n",
        "# Come√ßando de um ponto aleat√≥rio\n",
        "w1_path = [-2.0]\n",
        "w2_path = [2.0]\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Fazendo alguns passos de descida\n",
        "for i in range(15):\n",
        "    # Calculando o gradiente no ponto atual\n",
        "    w1_atual, w2_atual = w1_path[-1], w2_path[-1]\n",
        "    \n",
        "    # Gradiente da fun√ß√£o Z = (w1-1)¬≤ + 2(w2-0.5)¬≤ + 0.1\n",
        "    grad_w1 = 2 * (w1_atual - 1)  # ‚àÇZ/‚àÇw1\n",
        "    grad_w2 = 4 * (w2_atual - 0.5)  # ‚àÇZ/‚àÇw2\n",
        "    \n",
        "    # Dando um passo na dire√ß√£o oposta ao gradiente\n",
        "    w1_novo = w1_atual - learning_rate * grad_w1\n",
        "    w2_novo = w2_atual - learning_rate * grad_w2\n",
        "    \n",
        "    w1_path.append(w1_novo)\n",
        "    w2_path.append(w2_novo)\n",
        "\n",
        "# Plotando o caminho\n",
        "ax3.plot(w1_path, w2_path, 'ro-', linewidth=3, markersize=8, label='Caminho da Descida')\n",
        "ax3.plot(w1_path[0], w2_path[0], 'go', markersize=12, label='In√≠cio')\n",
        "ax3.plot(w1_path[-1], w2_path[-1], 'bo', markersize=12, label='Final')\n",
        "ax3.plot(1, 0.5, 'r*', markersize=20, label='Vale Verdadeiro')\n",
        "ax3.set_xlabel('Par√¢metro w1')\n",
        "ax3.set_ylabel('Par√¢metro w2')\n",
        "ax3.set_title('üö∂‚Äç‚ôÇÔ∏è Descida do Gradiente em A√ß√£o!')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 4: Erro ao longo do tempo\n",
        "ax4 = fig.add_subplot(224)\n",
        "erros = [(w1 - 1)**2 + 2*(w2 - 0.5)**2 + 0.1 for w1, w2 in zip(w1_path, w2_path)]\n",
        "ax4.plot(range(len(erros)), erros, 'b-o', linewidth=3, markersize=6)\n",
        "ax4.set_xlabel('Itera√ß√£o')\n",
        "ax4.set_ylabel('Erro')\n",
        "ax4.set_title('üìâ Erro Diminuindo ao Longo do Tempo')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üéØ Come√ßamos com erro {erros[0]:.3f} e terminamos com {erros[-1]:.3f}\")\n",
        "print(f\"üìç Posi√ß√£o final: w1={w1_path[-1]:.3f}, w2={w2_path[-1]:.3f}\")\n",
        "print(f\"‚≠ê Posi√ß√£o √≥tima: w1=1.000, w2=0.500\")\n",
        "print(\"\\nüî• √â EXATAMENTE assim que funciona o treinamento de IA!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Mermaid: O Fluxo do Aprendizado de M√°quina\n\nBora visualizar como o c√°lculo se encaixa no processo de aprendizado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um diagrama explicativo do processo\n",
        "from IPython.display import HTML\n",
        "\n",
        "mermaid_code = \"\"\"\n",
        "%%html\n",
        "<div class=\"mermaid\">\n",
        "graph TD\n",
        "    A[Dados de Entrada] --> B[Modelo com Par√¢metros w]\n",
        "    B --> C[Predi√ß√µes ≈∑]\n",
        "    C --> D[Fun√ß√£o de Erro E(w)]\n",
        "    D --> E[Calcular Gradiente ‚àáE]\n",
        "    E --> F[Atualizar Par√¢metros w = w - Œ±‚àáE]\n",
        "    F --> G{Erro Pequeno?}\n",
        "    G -->|N√£o| B\n",
        "    G -->|Sim| H[Modelo Treinado! üéâ]\n",
        "    \n",
        "    style D fill:#ffcccc\n",
        "    style E fill:#ccffcc\n",
        "    style F fill:#ccccff\n",
        "    style H fill:#ffffcc\n",
        "</div>\n",
        "\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
        "<script>mermaid.initialize({startOnLoad:true});</script>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<div style=\"text-align: center; font-size: 16px; margin: 20px;\">\n",
        "    <h3>üîÑ O Ciclo do Aprendizado de M√°quina</h3>\n",
        "    <div style=\"background-color: #f0f8ff; padding: 20px; border-radius: 10px; margin: 20px;\">\n",
        "        <p><strong>üî¥ Vermelho:</strong> Fun√ß√£o de Erro (onde mora o C√°lculo Integral)</p>\n",
        "        <p><strong>üü¢ Verde:</strong> C√°lculo do Gradiente (C√°lculo Diferencial)</p>\n",
        "        <p><strong>üîµ Azul:</strong> Atualiza√ß√£o dos Par√¢metros (Gradiente Descendente)</p>\n",
        "        <p><strong>üü° Amarelo:</strong> Sucesso! Modelo Otimizado!</p>\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\"))\n",
        "\n",
        "print(\"üéØ Esse √© o ciclo que TODO algoritmo de ML segue!\")\n",
        "print(\"üìö E o C√°lculo est√° presente em CADA etapa colorida!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Exemplo Pr√°tico: Regress√£o Linear na Unha\n\nT√° na hora de colocar a m√£o na massa! Vamos implementar uma regress√£o linear simples **do zero**, usando apenas os conceitos de c√°lculo.\n\n### O Problema:\nQueremos ajustar uma reta $y = wx + b$ aos nossos dados.\n\n### A Fun√ß√£o de Erro (MSE):\n$$E(w,b) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - (wx_i + b))^2$$\n\n### Os Gradientes:\n$$\\frac{\\partial E}{\\partial w} = \\frac{2}{n}\\sum_{i=1}^{n}(\\hat{y}_i - y_i) \\cdot x_i$$\n\n$$\\frac{\\partial E}{\\partial b} = \\frac{2}{n}\\sum_{i=1}^{n}(\\hat{y}_i - y_i)$$\n\n**Dica do Pedro**: Essas f√≥rmulas v√™m diretamente da regra da cadeia! Vamos estudar isso nos pr√≥ximos m√≥dulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gerando dados sint√©ticos para nossa regress√£o\n",
        "np.random.seed(42)  # Para resultados reproduz√≠veis\n",
        "\n",
        "# Dados verdadeiros: y = 2.5x + 1.3 + ru√≠do\n",
        "n_pontos = 100\n",
        "x_dados = np.random.uniform(-2, 2, n_pontos)\n",
        "w_real, b_real = 2.5, 1.3\n",
        "ruido = np.random.normal(0, 0.5, n_pontos)\n",
        "y_dados = w_real * x_dados + b_real + ruido\n",
        "\n",
        "# Visualizando os dados\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x_dados, y_dados, alpha=0.6, s=50, c='blue', label='Dados Observados')\n",
        "plt.plot(x_dados, w_real * x_dados + b_real, 'r--', linewidth=2, label=f'Linha Verdadeira: y = {w_real}x + {b_real}')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('üéØ Nossos Dados para Regress√£o Linear')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"üìä Gerados {n_pontos} pontos de dados\")\n",
        "print(f\"üìè Par√¢metros reais: w = {w_real}, b = {b_real}\")\n",
        "print(\"üé≤ Agora vamos tentar 'descobrir' esses par√¢metros usando c√°lculo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementa√ß√£o do Gradiente Descendente do ZERO!\n",
        "# Esse √© o cora√ß√£o da IA, galera!\n",
        "\n",
        "class RegressaoLinearNaUnha:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.w = np.random.normal(0, 0.1)  # Peso inicializado aleatoriamente\n",
        "        self.b = np.random.normal(0, 0.1)  # Bias inicializado aleatoriamente\n",
        "        \n",
        "        # Para guardar o hist√≥rico\n",
        "        self.historico_w = []\n",
        "        self.historico_b = []\n",
        "        self.historico_erro = []\n",
        "    \n",
        "    def predizer(self, x):\n",
        "        \"\"\"Faz predi√ß√µes: ≈∑ = wx + b\"\"\"\n",
        "        return self.w * x + self.b\n",
        "    \n",
        "    def calcular_erro(self, x, y):\n",
        "        \"\"\"Calcula o MSE (Mean Squared Error)\"\"\"\n",
        "        y_pred = self.predizer(x)\n",
        "        mse = np.mean((y - y_pred)**2)\n",
        "        return mse\n",
        "    \n",
        "    def calcular_gradientes(self, x, y):\n",
        "        \"\"\"Calcula os gradientes usando C√ÅLCULO! üî•\"\"\"\n",
        "        n = len(x)\n",
        "        y_pred = self.predizer(x)\n",
        "        \n",
        "        # Gradiente em rela√ß√£o ao peso w\n",
        "        # ‚àÇE/‚àÇw = (2/n) * Œ£(≈∑ - y) * x\n",
        "        grad_w = (2/n) * np.sum((y_pred - y) * x)\n",
        "        \n",
        "        # Gradiente em rela√ß√£o ao bias b\n",
        "        # ‚àÇE/‚àÇb = (2/n) * Œ£(≈∑ - y)\n",
        "        grad_b = (2/n) * np.sum(y_pred - y)\n",
        "        \n",
        "        return grad_w, grad_b\n",
        "    \n",
        "    def treinar_um_passo(self, x, y):\n",
        "        \"\"\"Um passo do gradiente descendente\"\"\"\n",
        "        # 1. Calcular gradientes\n",
        "        grad_w, grad_b = self.calcular_gradientes(x, y)\n",
        "        \n",
        "        # 2. Atualizar par√¢metros (descer na montanha!)\n",
        "        self.w -= self.learning_rate * grad_w\n",
        "        self.b -= self.learning_rate * grad_b\n",
        "        \n",
        "        # 3. Guardar hist√≥rico\n",
        "        self.historico_w.append(self.w)\n",
        "        self.historico_b.append(self.b)\n",
        "        self.historico_erro.append(self.calcular_erro(x, y))\n",
        "    \n",
        "    def treinar(self, x, y, epochs=1000, verbose=True):\n",
        "        \"\"\"Treinamento completo\"\"\"\n",
        "        if verbose:\n",
        "            print(f\"üöÄ Iniciando treinamento...\")\n",
        "            print(f\"üìä Par√¢metros iniciais: w = {self.w:.3f}, b = {self.b:.3f}\")\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            self.treinar_um_passo(x, y)\n",
        "            \n",
        "            if verbose and (epoch + 1) % 200 == 0:\n",
        "                erro_atual = self.historico_erro[-1]\n",
        "                print(f\"√âpoca {epoch+1:4d}: Erro = {erro_atual:.4f}, w = {self.w:.3f}, b = {self.b:.3f}\")\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\n‚úÖ Treinamento conclu√≠do!\")\n",
        "            print(f\"üéØ Par√¢metros finais: w = {self.w:.3f}, b = {self.b:.3f}\")\n",
        "            print(f\"üìè Par√¢metros reais:  w = {w_real:.3f}, b = {b_real:.3f}\")\n",
        "\n",
        "# Criando e treinando nosso modelo\n",
        "modelo = RegressaoLinearNaUnha(learning_rate=0.01)\n",
        "modelo.treinar(x_dados, y_dados, epochs=1000)\n",
        "\n",
        "print(\"\\nüî• Liiiindo! Implementamos ML do zero usando s√≥ C√°lculo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos visualizar os resultados do nosso treinamento\n",
        "\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Dados e linha ajustada\n",
        "ax1.scatter(x_dados, y_dados, alpha=0.6, s=30, c='blue', label='Dados')\n",
        "ax1.plot(x_dados, w_real * x_dados + b_real, 'r--', linewidth=2, label=f'Real: y = {w_real}x + {b_real}')\n",
        "ax1.plot(x_dados, modelo.predizer(x_dados), 'g-', linewidth=2, \n",
        "         label=f'Modelo: y = {modelo.w:.2f}x + {modelo.b:.2f}')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_title('üéØ Resultado Final: Dados vs Modelo Treinado')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Evolu√ß√£o do erro\n",
        "ax2.plot(modelo.historico_erro, 'b-', linewidth=2)\n",
        "ax2.set_xlabel('√âpoca')\n",
        "ax2.set_ylabel('Erro (MSE)')\n",
        "ax2.set_title('üìâ Erro Diminuindo Durante o Treinamento')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_yscale('log')  # Escala logar√≠tmica para ver melhor\n",
        "\n",
        "# 3. Evolu√ß√£o do par√¢metro w\n",
        "ax3.plot(modelo.historico_w, 'g-', linewidth=2, label='w aprendido')\n",
        "ax3.axhline(y=w_real, color='red', linestyle='--', linewidth=2, label=f'w real = {w_real}')\n",
        "ax3.set_xlabel('√âpoca')\n",
        "ax3.set_ylabel('Valor de w')\n",
        "ax3.set_title('üéØ Converg√™ncia do Par√¢metro w')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Evolu√ß√£o do par√¢metro b\n",
        "ax4.plot(modelo.historico_b, 'orange', linewidth=2, label='b aprendido')\n",
        "ax4.axhline(y=b_real, color='red', linestyle='--', linewidth=2, label=f'b real = {b_real}')\n",
        "ax4.set_xlabel('√âpoca')\n",
        "ax4.set_ylabel('Valor de b')\n",
        "ax4.set_title('üéØ Converg√™ncia do Par√¢metro b')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculando m√©tricas finais\n",
        "erro_final = modelo.historico_erro[-1]\n",
        "erro_w = abs(modelo.w - w_real)\n",
        "erro_b = abs(modelo.b - b_real)\n",
        "\n",
        "print(f\"üìä RESULTADOS FINAIS:\")\n",
        "print(f\"   Erro MSE final: {erro_final:.4f}\")\n",
        "print(f\"   Erro em w: {erro_w:.4f}\")\n",
        "print(f\"   Erro em b: {erro_b:.4f}\")\n",
        "print(f\"\\nüöÄ Nosso modelo aprendeu os par√¢metros usando apenas C√ÅLCULO!\")\n",
        "print(f\"üî• √â exatamente assim que funciona o ChatGPT, s√≥ que com bilh√µes de par√¢metros!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåü O Que Vem Por A√≠: Preview dos Pr√≥ximos M√≥dulos\n\nAgora que voc√™ entendeu **por que** o c√°lculo √© importante, vamos nos aprofundar nos pr√≥ximos m√≥dulos:\n\n### üìö Roadmap do Curso:\n\n**M√≥dulo 2 - Fun√ß√µes e Gr√°ficos**: Vamos aprender a \"ler\" fun√ß√µes e entender por que visualizar √© crucial para IA.\n\n**M√≥dulo 3 - Limites**: O conceito que permite definir derivadas rigorosamente.\n\n**M√≥dulos 4-5 - Derivadas**: As ferramentas fundamentais para otimiza√ß√£o.\n\n**M√≥dulo 6 - Regra da Cadeia**: O **cora√ß√£o** do backpropagation!\n\n**M√≥dulos 7-8 - Integrais**: Para probabilidade e m√©tricas como AUC.\n\n**M√≥dulos 9-12 - Multivari√°vel e Gradiente Descendente**: O arsenal completo para IA moderna!\n\n### üéØ Conceitos-Chave Que J√° Vimos:\n\n1. ‚úÖ **C√°lculo = Matem√°tica da mudan√ßa**\n2. ‚úÖ **IA = Problema de otimiza√ß√£o**  \n3. ‚úÖ **Derivada = Taxa de mudan√ßa instant√¢nea**\n4. ‚úÖ **Gradiente = Dire√ß√£o de maior crescimento**\n5. ‚úÖ **Gradiente Descendente = Algoritmo de otimiza√ß√£o**\n\n**Dica do Pedro**: Guarde bem esses 5 pontos. Eles s√£o a base de tudo que vem pela frente!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um mapa mental dos pr√≥ximos m√≥dulos\n",
        "from IPython.display import HTML\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
        "            padding: 30px; border-radius: 15px; color: white; text-align: center;\">\n",
        "    <h2>üó∫Ô∏è Jornada do C√°lculo para IA</h2>\n",
        "    <div style=\"display: flex; justify-content: space-around; flex-wrap: wrap; margin-top: 20px;\">\n",
        "        \n",
        "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; margin: 10px; \n",
        "                    border-radius: 10px; min-width: 200px;\">\n",
        "            <h4>üìä Fundamentos</h4>\n",
        "            <p>M√≥dulos 2-3<br>\n",
        "            Fun√ß√µes & Limites</p>\n",
        "        </div>\n",
        "        \n",
        "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; margin: 10px; \n",
        "                    border-radius: 10px; min-width: 200px;\">\n",
        "            <h4>‚ö° Derivadas</h4>\n",
        "            <p>M√≥dulos 4-6<br>\n",
        "            O Core da IA</p>\n",
        "        </div>\n",
        "        \n",
        "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; margin: 10px; \n",
        "                    border-radius: 10px; min-width: 200px;\">\n",
        "            <h4>üìà Integrais</h4>\n",
        "            <p>M√≥dulos 7-8<br>\n",
        "            Probabilidade & AUC</p>\n",
        "        </div>\n",
        "        \n",
        "        <div style=\"background: rgba(255,255,255,0.1); padding: 15px; margin: 10px; \n",
        "                    border-radius: 10px; min-width: 200px;\">\n",
        "            <h4>üöÄ IA Real</h4>\n",
        "            <p>M√≥dulos 9-12<br>\n",
        "            Multivari√°vel & GD</p>\n",
        "        </div>\n",
        "    </div>\n",
        "    \n",
        "    <div style=\"margin-top: 30px; font-size: 18px;\">\n",
        "        <strong>üéØ Objetivo Final:</strong> Entender completamente como funciona o treinamento de IA!\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\"))\n",
        "\n",
        "print(\"\\nüìö Cada m√≥dulo vai construir sobre o anterior\")\n",
        "print(\"üî• No final, voc√™ vai entender IA de uma forma que 99% das pessoas n√£o entende!\")\n",
        "print(\"üí™ Preparado para a jornada? Bora para o M√≥dulo 2!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí™ Exerc√≠cio Pr√°tico 1: Implementando seu Primeiro Otimizador\n\nAgora √© sua vez! Vamos implementar um problema de otimiza√ß√£o simples usando os conceitos que aprendemos.\n\n### üéØ Desafio:\nVoc√™ tem uma fun√ß√£o $f(x) = x^4 - 4x^3 + 6x^2 - 4x + 5$ e quer encontrar seu m√≠nimo usando gradiente descendente.\n\n### üìù Sua Miss√£o:\n1. Calcule a derivada $f'(x)$ na m√£o\n2. Implemente o gradiente descendente\n3. Encontre o m√≠nimo da fun√ß√£o\n4. Visualize o processo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 1: Complete o c√≥digo abaixo!\n",
        "\n",
        "def f(x):\n",
        "    \"\"\"Fun√ß√£o que queremos minimizar: f(x) = x‚Å¥ - 4x¬≥ + 6x¬≤ - 4x + 5\"\"\"\n",
        "    return x**4 - 4*x**3 + 6*x**2 - 4*x + 5\n",
        "\n",
        "def f_derivada(x):\n",
        "    \"\"\"Derivada de f(x). CALCULE VOC√ä!\n",
        "    \n",
        "    Dica: Use as regras de deriva√ß√£o:\n",
        "    - Derivada de x^n = n*x^(n-1)\n",
        "    - Derivada de constante = 0\n",
        "    \"\"\"\n",
        "    # SEU C√ìDIGO AQUI!\n",
        "    # return ???\n",
        "    pass\n",
        "\n",
        "def gradiente_descendente_1d(x_inicial, learning_rate=0.01, max_iter=1000):\n",
        "    \"\"\"Implementa gradiente descendente para 1 vari√°vel\n",
        "    \n",
        "    Args:\n",
        "        x_inicial: ponto inicial\n",
        "        learning_rate: taxa de aprendizado\n",
        "        max_iter: n√∫mero m√°ximo de itera√ß√µes\n",
        "    \n",
        "    Returns:\n",
        "        historico_x: lista com valores de x em cada itera√ß√£o\n",
        "        historico_f: lista com valores de f(x) em cada itera√ß√£o\n",
        "    \"\"\"\n",
        "    x = x_inicial\n",
        "    historico_x = [x]\n",
        "    historico_f = [f(x)]\n",
        "    \n",
        "    for i in range(max_iter):\n",
        "        # COMPLETE AQUI:\n",
        "        # 1. Calcule o gradiente no ponto atual\n",
        "        # 2. Atualize x usando a regra: x_novo = x_atual - learning_rate * gradiente\n",
        "        # 3. Adicione aos hist√≥ricos\n",
        "        \n",
        "        # SEU C√ìDIGO AQUI!\n",
        "        pass\n",
        "    \n",
        "    return historico_x, historico_f\n",
        "\n",
        "# Teste seu c√≥digo (descomente quando implementar)\n",
        "# x_otimo, f_otimo = gradiente_descendente_1d(x_inicial=3.0)\n",
        "# print(f\"M√≠nimo encontrado em x = {x_otimo[-1]:.4f}\")\n",
        "# print(f\"Valor m√≠nimo f(x) = {f_otimo[-1]:.4f}\")\n",
        "\n",
        "print(\"üéØ Implemente as fun√ß√µes acima e teste!\")\n",
        "print(\"üí° Dica: A derivada de x‚Å¥ - 4x¬≥ + 6x¬≤ - 4x + 5 √©...?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß© Exerc√≠cio Pr√°tico 2: An√°lise da Montanha do Erro\n\n### üéØ Desafio Conceitual:\n\nResponda √†s perguntas abaixo baseado no que voc√™ aprendeu:\n\n1. **Por que usamos o negativo do gradiente na atualiza√ß√£o dos par√¢metros?**\n\n2. **O que acontece se a taxa de aprendizado (learning rate) for muito alta?**\n\n3. **O que acontece se a taxa de aprendizado for muito baixa?**\n\n4. **Na analogia da montanha, o que representa:**\n   - A altura?\n   - Sua posi√ß√£o?\n   - A b√∫ssola?\n   - O tamanho dos passos?\n\n5. **Por que √© importante visualizar fun√ß√µes em IA?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 2: Responda escrevendo suas respostas como strings\n",
        "\n",
        "respostas = {\n",
        "    \"pergunta_1\": \"\"\"\n",
        "    Sua resposta aqui...\n",
        "    \"\"\",\n",
        "    \n",
        "    \"pergunta_2\": \"\"\"\n",
        "    Sua resposta aqui...\n",
        "    \"\"\",\n",
        "    \n",
        "    \"pergunta_3\": \"\"\"\n",
        "    Sua resposta aqui...\n",
        "    \"\"\",\n",
        "    \n",
        "    \"pergunta_4\": {\n",
        "        \"altura\": \"Sua resposta...\",\n",
        "        \"posi√ß√£o\": \"Sua resposta...\",\n",
        "        \"b√∫ssola\": \"Sua resposta...\",\n",
        "        \"passos\": \"Sua resposta...\"\n",
        "    },\n",
        "    \n",
        "    \"pergunta_5\": \"\"\"\n",
        "    Sua resposta aqui...\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "# Fun√ß√£o para verificar se voc√™ respondeu\n",
        "def verificar_respostas(respostas):\n",
        "    total = 0\n",
        "    respondidas = 0\n",
        "    \n",
        "    for key, value in respostas.items():\n",
        "        if key == \"pergunta_4\":\n",
        "            for subkey, subvalue in value.items():\n",
        "                total += 1\n",
        "                if len(subvalue.strip()) > 20:  # Resposta com pelo menos 20 chars\n",
        "                    respondidas += 1\n",
        "        else:\n",
        "            total += 1\n",
        "            if len(value.strip()) > 20:\n",
        "                respondidas += 1\n",
        "    \n",
        "    print(f\"üìä Voc√™ respondeu {respondidas}/{total} perguntas\")\n",
        "    if respondidas == total:\n",
        "        print(\"üéâ Parab√©ns! Todas as perguntas respondidas!\")\n",
        "    else:\n",
        "        print(\"üí™ Continue respondendo para fixar o conte√∫do!\")\n",
        "\n",
        "verificar_respostas(respostas)\n",
        "print(\"\\nüéØ Este exerc√≠cio √© crucial para consolidar os conceitos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Resumo do M√≥dulo 1: O Que Aprendemos\n\n### üèÜ Conceitos Fundamentais Dominados:\n\n1. **üßÆ O que √© C√°lculo:**\n   - Matem√°tica do movimento e da mudan√ßa\n   - Duas partes: Diferencial (derivadas) e Integral\n   - Base matem√°tica de toda IA moderna\n\n2. **üèîÔ∏è A Analogia da Montanha do Erro:**\n   - Fun√ß√£o de erro como paisagem montanhosa\n   - Gradiente como b√∫ssola indicando dire√ß√£o\n   - Gradiente descendente como estrat√©gia de descida\n\n3. **‚ö° Por que IA precisa de C√°lculo:**\n   - Todo ML √© problema de otimiza√ß√£o\n   - Precisamos minimizar fun√ß√µes de erro\n   - Derivadas nos mostram a dire√ß√£o √≥tima\n\n4. **üõ†Ô∏è Implementa√ß√£o Pr√°tica:**\n   - Regress√£o linear do zero\n   - Gradiente descendente manual\n   - Visualiza√ß√£o do processo de aprendizado\n\n### üìä F√≥rmulas-Chave Aprendidas:\n\n- **Fun√ß√£o de Erro MSE:** $E = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n- **Atualiza√ß√£o de Par√¢metros:** $w_{new} = w_{old} - \\alpha \\frac{\\partial E}{\\partial w}$\n- **Condi√ß√£o de √ìtimo:** $\\frac{dE}{dw} = 0$\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/c√°lculo-para-ia-modulo-01_img_02.png)\n\n**Dica Final do Pedro**: Voc√™ acabou de aprender os fundamentos que movem TODA a intelig√™ncia artificial moderna. ChatGPT, GPT-4, sistemas de recomenda√ß√£o, carros aut√¥nomos... todos usam exatamente esses conceitos, s√≥ que em escala gigantesca!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√≥digo de encerramento com estat√≠sticas do m√≥dulo\n",
        "import datetime\n",
        "\n",
        "def gerar_certificado_modulo1():\n",
        "    \"\"\"Gera um certificado simb√≥lico de conclus√£o do M√≥dulo 1\"\"\"\n",
        "    \n",
        "    data_atual = datetime.datetime.now().strftime(\"%d/%m/%Y\")\n",
        "    \n",
        "    certificado = f\"\"\"\n",
        "    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "    ‚ïë                    üéì CERTIFICADO DE CONCLUS√ÉO               ‚ïë\n",
        "    ‚ïë                                                              ‚ïë\n",
        "    ‚ïë                      C√ÅLCULO PARA IA                         ‚ïë\n",
        "    ‚ïë                       M√ìDULO 1                               ‚ïë\n",
        "    ‚ïë                                                              ‚ïë\n",
        "    ‚ïë              \"Introdu√ß√£o ao C√°lculo e IA\"                   ‚ïë\n",
        "    ‚ïë                                                              ‚ïë\n",
        "    ‚ïë   üèîÔ∏è  Dominou a analogia da Montanha do Erro               ‚ïë\n",
        "    ‚ïë   ‚ö°  Entendeu o papel do C√°lculo na IA                     ‚ïë\n",
        "    ‚ïë   üõ†Ô∏è  Implementou Gradiente Descendente do zero             ‚ïë\n",
        "    ‚ïë   üìä  Visualizou o processo de otimiza√ß√£o                   ‚ïë\n",
        "    ‚ïë                                                              ‚ïë\n",
        "    ‚ïë              Por: Pedro Nunes Guth                          ‚ïë\n",
        "    ‚ïë              Data: {data_atual}                            ‚ïë\n",
        "    ‚ïë                                                              ‚ïë\n",
        "    ‚ïë        üöÄ Pronto para o M√≥dulo 2: Fun√ß√µes e Gr√°ficos! üöÄ    ‚ïë\n",
        "    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "    \"\"\"\n",
        "    \n",
        "    return certificado\n",
        "\n",
        "# Estat√≠sticas do m√≥dulo\n",
        "estatisticas = {\n",
        "    \"conceitos_aprendidos\": 8,\n",
        "    \"formulas_matematicas\": 5,\n",
        "    \"graficos_criados\": 6,\n",
        "    \"linhas_de_codigo\": 200,\n",
        "    \"exercicios_propostos\": 2,\n",
        "    \"analogias_usadas\": 3\n",
        "}\n",
        "\n",
        "print(gerar_certificado_modulo1())\n",
        "\n",
        "print(\"\\nüìä ESTAT√çSTICAS DO M√ìDULO 1:\")\n",
        "for key, value in estatisticas.items():\n",
        "    nome = key.replace('_', ' ').title()\n",
        "    print(f\"   {nome}: {value}\")\n",
        "\n",
        "print(\"\\nüéØ PR√ìXIMOS PASSOS:\")\n",
        "print(\"   1. Revise os conceitos principais\")\n",
        "print(\"   2. Complete os exerc√≠cios pr√°ticos\")\n",
        "print(\"   3. Parta para o M√≥dulo 2: Fun√ß√µes e Gr√°ficos\")\n",
        "\n",
        "print(\"\\nüî• LEMBRA: Voc√™ agora entende IA de um jeito que pouqu√≠ssimas pessoas entendem!\")\n",
        "print(\"üí™ Continue nessa jornada que vai valer MUITO a pena!\")\n",
        "\n",
        "# Motiva√ß√£o final\n",
        "frases_motivacionais = [\n",
        "    \"üöÄ Cada linha de c√≥digo te aproxima da maestria!\",\n",
        "    \"üß† Conhecimento √© o √∫nico investimento que sempre d√° retorno!\",\n",
        "    \"‚ö° Voc√™ est√° construindo superpoderes matem√°ticos!\",\n",
        "    \"üéØ O futuro pertence a quem entende os fundamentos!\",\n",
        "    \"üèÜ Persist√™ncia + Conhecimento = Sucesso garantido!\"\n",
        "]\n",
        "\n",
        "import random\n",
        "frase_do_dia = random.choice(frases_motivacionais)\n",
        "print(f\"\\nüíé FRASE DO DIA: {frase_do_dia}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ M√ìDULO 1 CONCLU√çDO COM SUCESSO! üéâ\")\n",
        "print(\"=\"*60)"
      ]
    }
  ]
}
